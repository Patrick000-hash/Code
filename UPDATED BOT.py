import os
import sys
import time
import json
import math
import logging
import pickle
import threading
import queue
import warnings
import asyncio
import aiohttp
import websocket
import ssl
import hmac
import hashlib
import base64
import uuid
from datetime import datetime, timedelta, timezone
from collections import defaultdict, deque, OrderedDict
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import wraps, lru_cache
import traceback
import gc
import psutil
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
 
# Data Science & ML
import pandas as pd
import numpy as np
import talib
from scipy import stats
from scipy.optimize import minimize
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import joblib
 
# Binance API
from binance.client import Client
from binance.enums import *
from binance.exceptions import BinanceAPIException, BinanceOrderException
 
# Deep Learning
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers
    HAS_TENSORFLOW = True
except ImportError:
    HAS_TENSORFLOW = False
    print("WARNING: TensorFlow not installed. Neural network features disabled.")
 
# Text Analysis
try:
    from textblob import TextBlob
    import tweepy
    import praw
    HAS_SENTIMENT = True
except ImportError:
    HAS_SENTIMENT = False
    print("WARNING: Sentiment analysis libraries not installed.")
 
# Web Framework for Dashboard
try:
    from flask import Flask, render_template, jsonify
    from flask_socketio import SocketIO, emit
    HAS_FLASK = True
except ImportError:
    HAS_FLASK = False
    print("WARNING: Flask not installed. Dashboard disabled.")
 
# Email notifications
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
 
warnings.filterwarnings('ignore')
pd.set_option('display.max_columns', None)
np.random.seed(42)
 
class MonsterBotConfig:
    """Complete configuration system for the Monster Trading Bot"""
    
    def __init__(self):
        # ===========================
        # BINANCE API CREDENTIALS - REPLACE WITH YOUR ACTUAL KEYS
        # ===========================
        self.API_KEY = "YOUR_BINANCE_API_KEY_HERE"
        self.API_SECRET = "YOUR_BINANCE_SECRET_KEY_HERE"
        
        # ===========================
        # CORE TRADING CONFIGURATION
        # ===========================
        self.STARTING_CAPITAL = 100.0
        self.PAPER_MODE = False  # SET TO FALSE FOR REAL TRADING
        self.MAX_OPEN_POSITIONS = 5
        self.BASE_LEVERAGE = 10
        self.MAX_LEVERAGE = 25
        self.DAILY_LOSS_LIMIT = 0.15  # Stop trading at 15% daily loss
        
        # ===========================
        # PERSONALITY MODES - CAPITAL-BASED AGGRESSION
        # ===========================
        self.PERSONALITY_MODES = {
            'PSYCHO': {
                'capital_range': (0, 1000),
                'max_position_size': 0.80,  # 80% of capital per trade
                'max_leverage': 25,
                'daily_target': 0.30,  # 30% daily target
                'max_trades_per_hour': 15,
                'confidence_threshold': 0.60,
                'recovery_mode': True,  # Martingale on losses
                'stop_loss_pct': 0.025,  # 2.5% stop loss
                'take_profit_levels': [0.03, 0.06, 0.12, 0.20],  # Multiple TPs
                'scaling_percentages': [0.25, 0.35, 0.25, 0.15],  # How much to close at each TP
                'trailing_stop': False,
                'scalp_mode': True,
                'description': 'üî• PSYCHO MODE - Maximum aggression for small accounts'
            },
            'HUNTER': {
                'capital_range': (1000, 10000),
                'max_position_size': 0.60,
                'max_leverage': 20,
                'daily_target': 0.25,  # 25% daily target
                'max_trades_per_hour': 10,
                'confidence_threshold': 0.65,
                'recovery_mode': True,
                'stop_loss_pct': 0.03,
                'take_profit_levels': [0.04, 0.08, 0.15, 0.25],
                'scaling_percentages': [0.20, 0.30, 0.30, 0.20],
                'trailing_stop': True,
                'scalp_mode': True,
                'description': 'üéØ HUNTER MODE - Aggressive profit hunting'
            },
            'ACCUMULATOR': {
                'capital_range': (10000, 100000),
                'max_position_size': 0.40,
                'max_leverage': 15,
                'daily_target': 0.20,  # 20% daily target
                'max_trades_per_hour': 6,
                'confidence_threshold': 0.70,
                'recovery_mode': False,
                'stop_loss_pct': 0.04,
                'take_profit_levels': [0.05, 0.10, 0.18, 0.30],
                'scaling_percentages': [0.15, 0.25, 0.35, 0.25],
                'trailing_stop': True,
                'scalp_mode': False,
                'description': 'üìà ACCUMULATOR MODE - Steady compounding'
            },
            'WHALE': {
                'capital_range': (100000, float('inf')),
                'max_position_size': 0.25,
                'max_leverage': 10,
                'daily_target': 0.15,  # 15% daily target
                'max_trades_per_hour': 4,
                'confidence_threshold': 0.75,
                'recovery_mode': False,
                'stop_loss_pct': 0.05,
                'take_profit_levels': [0.06, 0.12, 0.20, 0.35],
                'scaling_percentages': [0.10, 0.20, 0.40, 0.30],
                'trailing_stop': True,
                'scalp_mode': False,
                'description': 'üêã WHALE MODE - Capital preservation with steady gains'
            }
        }
        
        # ===========================
        # STRATEGY WEIGHTS - EVOLVED BY AI
        # ===========================
        self.STRATEGY_WEIGHTS = {
            'momentum_breakout': 1.0,
            'volume_spike': 1.2,
            'whale_tracker': 1.5,
            'liquidity_grab': 1.4,
            'news_reaction': 1.6,
            'sentiment_extreme': 1.3,
            'pattern_recognition': 1.0,
            'order_flow': 1.2,
            'smart_money': 1.4,
            'accumulation_break': 1.3,
            'volatility_breakout': 1.1,
            'funding_arbitrage': 0.8,
            'correlation_break': 0.9,
            'support_resistance': 1.0,
            'trend_following': 1.1,
            'mean_reversion': 0.7,
            'rsi_divergence': 0.9,
            'bollinger_squeeze': 1.1,
            'market_structure': 1.0,
            'time_based': 0.6,
            'fibonacci_levels': 0.8,
            'scalping': 1.2,
            'gap_trading': 0.9,
            'crossover': 0.8,
            'momentum_divergence': 1.0
        }
        
        # ===========================
        # NEURAL NETWORK CONFIGURATION
        # ===========================
        self.NN_CONFIG = {
            'input_features': 200,
            'hidden_layers': [512, 256, 128, 64],
            'output_classes': 3,  # Buy, Sell, Hold
            'learning_rate': 0.0001,
            'dropout_rate': 0.3,
            'batch_size': 256,
            'epochs': 200,
            'validation_split': 0.2,
            'early_stopping_patience': 20,
            'reduce_lr_patience': 10
        }
        
        # ===========================
        # MARKET DATA CONFIGURATION
        # ===========================
        self.TIMEFRAMES = ['1m', '3m', '5m', '15m', '30m', '1h', '4h', '1d']
        self.PRIMARY_TIMEFRAME = '15m'
        self.LOOKBACK_PERIODS = 1000
        self.TOP_SYMBOLS_COUNT = 100
        self.SCAN_INTERVAL = 30  # seconds
        
        # ===========================
        # RISK MANAGEMENT SETTINGS
        # ===========================
        self.MAX_DAILY_LOSS = 0.15
        self.MAX_DRAWDOWN = 0.25
        self.POSITION_CORRELATION_LIMIT = 0.7
        self.MIN_VOLUME_USDT = 5000000  # $5M minimum volume
        self.MIN_PRICE_CHANGE = 0.005   # 0.5% minimum price change for consideration
        self.MAX_POSITION_HOLD_TIME = 3600  # 1 hour max hold time for scalping
        
        # ===========================
        # WHALE TRACKING SETTINGS
        # ===========================
        self.WHALE_SETTINGS = {
            'large_order_threshold': 500000,  # $500K+ orders
            'whale_volume_multiplier': 4.0,
            'accumulation_period': 50,
            'distribution_threshold': 0.12,
            'order_book_depth': 50,
            'iceberg_detection_sensitivity': 0.15
        }
        
        # ===========================
        # SENTIMENT ANALYSIS SETTINGS
        # ===========================
        self.SENTIMENT_SETTINGS = {
            'twitter_keywords': ['crypto', 'bitcoin', 'ethereum', 'pump', 'moon', 'bullish', 'bearish', 'breakout'],
            'reddit_subreddits': ['cryptocurrency', 'bitcoin', 'ethereum', 'cryptomoonshots', 'satoshistreetbets'],
            'news_sources': ['coindesk', 'cointelegraph', 'decrypt', 'theblock', 'cryptocompare'],
            'sentiment_weight': 0.15,  # 15% weight in final decision
            'extreme_sentiment_threshold': 0.7,
            'sentiment_update_interval': 180  # 3 minutes
        }
        
        # ===========================
        # NOTIFICATION SETTINGS
        # ===========================
        self.TELEGRAM_BOT_TOKEN = "YOUR_TELEGRAM_BOT_TOKEN"
        self.TELEGRAM_CHAT_ID = "YOUR_TELEGRAM_CHAT_ID"
        self.DISCORD_WEBHOOK = "YOUR_DISCORD_WEBHOOK_URL"
        
        self.EMAIL_CONFIG = {
            'smtp_server': 'smtp.gmail.com',
            'smtp_port': 587,
            'email': 'your_email@gmail.com',
            'password': 'your_app_password',
            'recipient': 'recipient@gmail.com'
        }
        
        # ===========================
        # PERFORMANCE SETTINGS
        # ===========================
        self.MAX_WORKERS = min(32, (os.cpu_count() or 1) + 4)
        self.CACHE_SIZE = 2000
        self.MEMORY_LIMIT_MB = 4096
        self.GC_FREQUENCY = 50
        
        # ===========================
        # LOGGING CONFIGURATION
        # ===========================
        self.LOG_LEVEL = logging.INFO
        self.LOG_FORMAT = '[%(asctime)s] %(levelname)s [%(name)s]: %(message)s'
        self.LOG_DATE_FORMAT = '%Y-%m-%d %H:%M:%S'
        
        # ===========================
        # DIRECTORIES
        # ===========================
        self.LOG_DIR = "logs"
        self.DATA_DIR = "data"
        self.MODELS_DIR = "models"
        self.BACKUPS_DIR = "backups"
        self.CACHE_DIR = "cache"
        
        # Create all directories
        for directory in [self.LOG_DIR, self.DATA_DIR, self.MODELS_DIR, self.BACKUPS_DIR, self.CACHE_DIR]:
            os.makedirs(directory, exist_ok=True)
        
        # ===========================
        # SYMBOL FILTERS
        # ===========================
        self.SYMBOL_BLACKLIST = [
            'BTCDOWNUSDT', 'BTCUPUSDT', 'ETHDOWNUSDT', 'ETHUPUSDT',
            'BNBDOWNUSDT', 'BNBUPUSDT', 'ADADOWNUSDT', 'ADAUPUSDT',
            'LINKDOWNUSDT', 'LINKUPUSDT', 'DOTDOWNUSDT', 'DOTUPUSDT'
        ]
        
        self.PREFERRED_SYMBOLS = [
            'BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'ADAUSDT', 'DOTUSDT',
            'LINKUSDT', 'LTCUSDT', 'BCHUSDT', 'XLMUSDT', 'EOSUSDT',
            'TRXUSDT', 'XRPUSDT', 'ATOMUSDT', 'VETUSDT', 'NEOUSDT',
            'IOTAUSDT', 'ALGOUSDT', 'ZILUSDT', 'KNCUSDT', 'ZRXUSDT',
            'SOLUSDT', 'AVAXUSDT', 'MATICUSDT', 'FILUSDT', 'UNIUSDT'
        ]
        
        # ===========================
        # ADVANCED FEATURES FLAGS
        # ===========================
        self.ENABLE_NEURAL_NETWORK = HAS_TENSORFLOW
        self.ENABLE_SENTIMENT_ANALYSIS = HAS_SENTIMENT
        self.ENABLE_WEB_DASHBOARD = HAS_FLASK
        self.ENABLE_BACKTESTING = True
        self.ENABLE_OPTIMIZATION = True
        self.ENABLE_AUTO_COMPOUND = True
        self.ENABLE_CROSS_EXCHANGE = False  # Set True when multiple exchanges added
        
        # ===========================
        # MARKET TIMING SETTINGS
        # ===========================
        self.PEAK_TRADING_HOURS = {
            'start': 13,  # 1 PM UTC (9 AM EDT)
            'end': 21     # 9 PM UTC (5 PM EDT)
        }
        
        self.LOW_LIQUIDITY_HOURS = {
            'start': 2,   # 2 AM UTC
            'end': 6      # 6 AM UTC
        }
        
        # ===========================
        # TECHNICAL INDICATOR SETTINGS
        # ===========================
        self.INDICATOR_SETTINGS = {
            'rsi_period': 14,
            'rsi_overbought': 70,
            'rsi_oversold': 30,
            'macd_fast': 12,
            'macd_slow': 26,
            'macd_signal': 9,
            'bb_period': 20,
            'bb_std': 2.0,
            'ema_fast': 9,
            'ema_slow': 21,
            'sma_period': 50,
            'atr_period': 14,
            'volume_sma_period': 20
        }
 
class AdvancedLogger:
    """Advanced logging system with performance tracking and multiple outputs"""
    
    def __init__(self, config):
        self.config = config
        self.performance_metrics = defaultdict(list)
        self.error_counts = defaultdict(int)
        self.start_time = time.time()
        
        # Initialize logging
        self.setup_logging()
        
        # Performance tracking
        self.operation_times = defaultdict(deque)
        self.memory_usage = deque(maxlen=1000)
        
        # Log files
        self.log_files = {}
        
    def setup_logging(self):
        """Setup comprehensive logging system"""
        # Main logger
        self.logger = logging.getLogger('MonsterBot')
        self.logger.setLevel(self.config.LOG_LEVEL)
        
        # Clear existing handlers
        self.logger.handlers.clear()
        
        # Formatters
        detailed_formatter = logging.Formatter(
            '[%(asctime)s] %(levelname)s [%(name)s.%(funcName)s:%(lineno)d]: %(message)s',
            datefmt=self.config.LOG_DATE_FORMAT
        )
        
        simple_formatter = logging.Formatter(
            self.config.LOG_FORMAT,
            datefmt=self.config.LOG_DATE_FORMAT
        )
        
        # Console handler with colors
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(logging.INFO)
        console_handler.setFormatter(simple_formatter)
        self.logger.addHandler(console_handler)
        
        # Main log file
        main_log_file = os.path.join(
            self.config.LOG_DIR, 
            f'monster_bot_{datetime.now().strftime("%Y%m%d")}.log'
        )
        
        file_handler = logging.FileHandler(main_log_file, encoding='utf-8')
        file_handler.setLevel(logging.DEBUG)
        file_handler.setFormatter(detailed_formatter)
        self.logger.addHandler(file_handler)
        
        # Error log file
        error_log_file = os.path.join(self.config.LOG_DIR, 'errors.log')
        error_handler = logging.FileHandler(error_log_file, encoding='utf-8')
        error_handler.setLevel(logging.ERROR)
        error_handler.setFormatter(detailed_formatter)
        self.logger.addHandler(error_handler)
        
        # Specialized loggers
        self.setup_specialized_loggers()
        
        self.logger.info("üöÄ Monster Bot Advanced Logging System Initialized")
        
    def setup_specialized_loggers(self):
        """Setup specialized loggers for different components"""
        # Trading logger
        self.trading_logger = logging.getLogger('MonsterBot.Trading')
        trading_handler = logging.FileHandler(
            os.path.join(self.config.LOG_DIR, 'trades.csv'),
            encoding='utf-8'
        )
        trading_formatter = logging.Formatter('%(asctime)s,%(message)s')
        trading_handler.setFormatter(trading_formatter)
        self.trading_logger.addHandler(trading_handler)
        
        # Performance logger
        self.perf_logger = logging.getLogger('MonsterBot.Performance')
        perf_handler = logging.FileHandler(
            os.path.join(self.config.LOG_DIR, 'performance.csv'),
            encoding='utf-8'
        )
        perf_formatter = logging.Formatter('%(asctime)s,%(message)s')
        perf_handler.setFormatter(perf_formatter)
        self.perf_logger.addHandler(perf_handler)
        
        # Signals logger
        self.signals_logger = logging.getLogger('MonsterBot.Signals')
        signals_handler = logging.FileHandler(
            os.path.join(self.config.LOG_DIR, 'signals.csv'),
            encoding='utf-8'
        )
        signals_formatter = logging.Formatter('%(asctime)s,%(message)s')
        signals_handler.setFormatter(signals_formatter)
        self.signals_logger.addHandler(signals_handler)
        
    def log_trade(self, trade_data):
        """Log trade execution with structured data"""
        trade_msg = (
            f"{trade_data.get('symbol', 'N/A')},"
            f"{trade_data.get('side', 'N/A')},"
            f"{trade_data.get('size', 0):.6f},"
            f"{trade_data.get('price', 0):.8f},"
            f"{trade_data.get('leverage', 1)},"
            f"{trade_data.get('pnl_pct', 0):.4f},"
            f"{trade_data.get('strategy', 'N/A')},"
            f"{trade_data.get('confidence', 0):.4f},"
            f"{trade_data.get('entry_type', 'N/A')},"
            f"{trade_data.get('exit_reason', 'N/A')}"
        )
        
        self.trading_logger.info(trade_msg)
        
        # Also log to main logger
        main_msg = (
            f"üìä TRADE: {trade_data.get('symbol')} {trade_data.get('side')} "
            f"${trade_data.get('size', 0) * trade_data.get('price', 0):.2f} "
            f"@ {trade_data.get('price', 0):.6f} "
            f"({trade_data.get('leverage')}x) "
            f"PnL: {trade_data.get('pnl_pct', 0):.2%}"
        )
        
        if trade_data.get('pnl_pct', 0) > 0:
            self.logger.info(f"üí∞ {main_msg}")
        else:
            self.logger.info(f"üìâ {main_msg}")
            
    def log_performance(self, perf_data):
        """Log performance metrics"""
        perf_msg = (
            f"{perf_data.get('balance', 0):.2f},"
            f"{perf_data.get('daily_pnl', 0):.4f},"
            f"{perf_data.get('total_pnl', 0):.4f},"
            f"{perf_data.get('trades_today', 0)},"
            f"{perf_data.get('win_rate', 0):.4f},"
            f"{perf_data.get('sharpe', 0):.4f},"
            f"{perf_data.get('max_drawdown', 0):.4f},"
            f"{perf_data.get('open_positions', 0)}"
        )
        
        self.perf_logger.info(perf_msg)
        
    def log_signal(self, signal_data):
        """Log trading signals"""
        signal_msg = (
            f"{signal_data.get('symbol', 'N/A')},"
            f"{signal_data.get('strategy', 'N/A')},"
            f"{signal_data.get('direction', 'N/A')},"
            f"{signal_data.get('confidence', 0):.4f},"
            f"{signal_data.get('entry_type', 'N/A')},"
            f"{json.dumps(signal_data.get('factors', {}))}"
        )
        
        self.signals_logger.info(signal_msg)
        
    def track_performance(self, operation, duration, success=True, details=None):
        """Track operation performance"""
        self.performance_metrics[operation].append({
            'duration': duration,
            'success': success,
            'timestamp': time.time(),
            'details': details
        })
        
        # Keep only recent metrics
        if len(self.performance_metrics[operation]) > 1000:
            self.performance_metrics[operation].pop(0)
            
        if not success:
            self.error_counts[operation] += 1
            
        # Track in deque for quick access
        self.operation_times[operation].append(duration)
        if len(self.operation_times[operation]) > 100:
            self.operation_times[operation].popleft()
            
    def get_performance_stats(self):
        """Get comprehensive performance statistics"""
        stats = {
            'uptime': time.time() - self.start_time,
            'operations': {},
            'memory_usage': self.get_memory_usage(),
            'error_summary': dict(self.error_counts)
        }
        
        for operation, metrics in self.performance_metrics.items():
            if metrics:
                recent_metrics = metrics[-100:]  # Last 100 operations
                durations = [m['duration'] for m in recent_metrics]
                successes = [m['success'] for m in recent_metrics]
                
                stats['operations'][operation] = {
                    'count': len(metrics),
                    'recent_count': len(recent_metrics),
                    'avg_duration': np.mean(durations) if durations else 0,
                    'success_rate': np.mean(successes) if successes else 0,
                    'total_errors': self.error_counts.get(operation, 0),
                    'ops_per_minute': len(recent_metrics) / max((time.time() - recent_metrics[0]['timestamp']) / 60, 1) if recent_metrics else 0
                }
                
        return stats
        
    def get_memory_usage(self):
        """Get current memory usage"""
        process = psutil.Process(os.getpid())
        memory_mb = process.memory_info().rss / 1024 / 1024
        self.memory_usage.append(memory_mb)
        return memory_mb
 
class MemoryManager:
    """Intelligent memory management and caching system"""
    
    def __init__(self, config):
        self.config = config
        self.cache = OrderedDict()
        self.cache_stats = {
            'hits': 0,
            'misses': 0,
            'evictions': 0
        }
        
        self.memory_usage_history = deque(maxlen=1000)
        self.gc_counter = 0
        self.last_cleanup = time.time()
        
        # Cache categories with different TTL
        self.cache_ttl = {
            'market_data': 30,      # 30 seconds
            'indicators': 60,       # 1 minute
            'sentiment': 300,       # 5 minutes
            'whale_data': 120,      # 2 minutes
            'predictions': 60,      # 1 minute
            'symbols': 600,         # 10 minutes
            'balances': 10          # 10 seconds
        }
        
    def get_memory_usage(self):
        """Get current memory usage in MB"""
        process = psutil.Process(os.getpid())
        memory_mb = process.memory_info().rss / 1024 / 1024
        self.memory_usage_history.append({
            'timestamp': time.time(),
            'memory_mb': memory_mb
        })
        return memory_mb
        
    def check_memory_limit(self):
        """Check if memory usage exceeds configured limit"""
        current_usage = self.get_memory_usage()
        
        if current_usage > self.config.MEMORY_LIMIT_MB:
            self.force_cleanup()
            return False
        return True
        
    def force_cleanup(self):
        """Force aggressive memory cleanup"""
        # Clear cache
        cleared_items = len(self.cache)
        self.cache.clear()
        
        # Force garbage collection
        collected = gc.collect()
        
        # Update stats
        self.cache_stats['evictions'] += cleared_items
        
        current_usage = self.get_memory_usage()
        
        logging.getLogger('MonsterBot.Memory').warning(
            f"üßπ Forced cleanup: {cleared_items} cache items, "
            f"{collected} objects collected, "
            f"Memory: {current_usage:.1f}MB"
        )
        
    def cache_get(self, key, category='default'):
        """Get item from cache with TTL checking"""
        if key in self.cache:
            item_data = self.cache[key]
            
            # Check TTL
            ttl = self.cache_ttl.get(category, 300)
            if time.time() - item_data['timestamp'] < ttl:
                # Move to end (LRU)
                self.cache.move_to_end(key)
                self.cache_stats['hits'] += 1
                return item_data['data']
            else:
                # Expired - remove
                del self.cache[key]
                
        self.cache_stats['misses'] += 1
        return None
        
    def cache_set(self, key, value, category='default'):
        """Set item in cache with automatic cleanup"""
        # Check cache size limit
        if len(self.cache) >= self.config.CACHE_SIZE:
            # Remove oldest items (10% of cache size)
            items_to_remove = max(1, self.config.CACHE_SIZE // 10)
            for _ in range(items_to_remove):
                if self.cache:
                    removed_key, removed_value = self.cache.popitem(last=False)
                    self.cache_stats['evictions'] += 1
                    
        # Add new item
        self.cache[key] = {
            'data': value,
            'timestamp': time.time(),
            'category': category
        }
        
    def periodic_cleanup(self):
        """Periodic cleanup based on frequency"""
        self.gc_counter += 1
        
        if self.gc_counter >= self.config.GC_FREQUENCY:
            self.gc_counter = 0
            
            # Clean expired cache items
            current_time = time.time()
            expired_keys = []
            
            for key, item_data in self.cache.items():
                category = item_data.get('category', 'default')
                ttl = self.cache_ttl.get(category, 300)
                
                if current_time - item_data['timestamp'] > ttl:
                    expired_keys.append(key)
                    
            # Remove expired items
            for key in expired_keys:
                del self.cache[key]
                self.cache_stats['evictions'] += 1
                
            # Garbage collection
            collected = gc.collect()
            
            if collected > 0 or expired_keys:
                logging.getLogger('MonsterBot.Memory').debug(
                    f"Periodic cleanup: {len(expired_keys)} expired items, "
                    f"{collected} objects collected"
                )
                
            self.last_cleanup = current_time
            
    def get_cache_stats(self):
        """Get cache performance statistics"""
        total_requests = self.cache_stats['hits'] + self.cache_stats['misses']
        hit_rate = self.cache_stats['hits'] / total_requests if total_requests > 0 else 0
        
        # Memory statistics
        if self.memory_usage_history:
            recent_memory = [m['memory_mb'] for m in list(self.memory_usage_history)[-100:]]
            avg_memory = np.mean(recent_memory)
            max_memory = max(recent_memory)
        else:
            avg_memory = 0
            max_memory = 0
            
        return {
            'hit_rate': hit_rate,
            'total_hits': self.cache_stats['hits'],
            'total_misses': self.cache_stats['misses'],
            'total_evictions': self.cache_stats['evictions'],
            'cache_size': len(self.cache),
            'max_cache_size': self.config.CACHE_SIZE,
            'current_memory_mb': self.get_memory_usage(),
            'avg_memory_mb': avg_memory,
            'max_memory_mb': max_memory,
            'last_cleanup': time.time() - self.last_cleanup
        }
 
def get_personality_mode(capital, config):
    """Get personality mode based on current capital"""
    for mode_name, mode_config in config.PERSONALITY_MODES.items():
        min_cap, max_cap = mode_config['capital_range']
        if min_cap <= capital < max_cap:
            return mode_name, mode_config
            
    # Default to WHALE mode for very large amounts
    return 'WHALE', config.PERSONALITY_MODES['WHALE']
 
def safe_float(value, default=0.0):
    """Safely convert value to float"""
    try:
        return float(value)
    except (ValueError, TypeError):
        return default
 
def safe_divide(numerator, denominator, default=0.0):
    """Safe division with default value"""
    try:
        if denominator == 0:
            return default
        return numerator / denominator
    except:
        return default
 
def calculate_percentage_change(old_value, new_value):
    """Calculate percentage change between two values"""
    if old_value == 0:
        return 0.0
    return (new_value - old_value) / old_value
 
def format_number(number, decimals=2):
    """Format number with appropriate decimals"""
    if number >= 1000000:
        return f"{number/1000000:.1f}M"
    elif number >= 1000:
        return f"{number/1000:.1f}K"
    else:
        return f"{number:.{decimals}f}"
 
def get_timestamp():
    """Get current timestamp in milliseconds"""
    return int(time.time() * 1000)
 
def timestamp_to_datetime(timestamp):
    """Convert timestamp to datetime"""
    return datetime.fromtimestamp(timestamp / 1000)
 
class ConfigValidator:
    """Validates configuration settings"""
    
    @staticmethod
    def validate_config(config):
        """Validate all configuration settings"""
        errors = []
        warnings = []
        
        # Check API credentials
        if config.API_KEY == "YOUR_BINANCE_API_KEY_HERE":
            errors.append("Binance API key not set")
            
        if config.API_SECRET == "YOUR_BINANCE_SECRET_KEY_HERE":
            errors.append("Binance API secret not set")
            
        # Check capital settings
        if config.STARTING_CAPITAL <= 0:
            errors.append("Starting capital must be positive")
            
        # Check personality mode configurations
        for mode_name, mode_config in config.PERSONALITY_MODES.items():
            if mode_config['max_position_size'] > 1.0:
                warnings.append(f"{mode_name} mode position size > 100%")
                
            if mode_config['max_leverage'] > 50:
                warnings.append(f"{mode_name} mode leverage > 50x (very risky)")
                
        # Check neural network settings
        if config.ENABLE_NEURAL_NETWORK and not HAS_TENSORFLOW:
            warnings.append("Neural network enabled but TensorFlow not installed")
            
        # Check notification settings
        if config.TELEGRAM_BOT_TOKEN == "YOUR_TELEGRAM_BOT_TOKEN":
            warnings.append("Telegram notifications not configured")
            
        return errors, warnings
 
class SystemMonitor:
    """Monitors system resources and performance"""
    
    def __init__(self, config):
        self.config = config
        self.start_time = time.time()
        self.metrics = defaultdict(list)
        
    def get_system_metrics(self):
        """Get comprehensive system metrics"""
        process = psutil.Process(os.getpid())
        
        # CPU and memory
        cpu_percent = process.cpu_percent()
        memory_info = process.memory_info()
        memory_mb = memory_info.rss / 1024 / 1024
        
        # Disk usage
        disk_usage = psutil.disk_usage('/')
        
        # Network (if available)
        try:
            network = psutil.net_io_counters()
            network_sent = network.bytes_sent
            network_recv = network.bytes_recv
        except:
            network_sent = 0
            network_recv = 0
            
        metrics = {
            'timestamp': time.time(),
            'uptime': time.time() - self.start_time,
            'cpu_percent': cpu_percent,
            'memory_mb': memory_mb,
            'memory_percent': process.memory_percent(),
            'disk_free_gb': disk_usage.free / 1024 / 1024 / 1024,
            'disk_percent': (disk_usage.total - disk_usage.free) / disk_usage.total * 100,
            'network_sent_mb': network_sent / 1024 / 1024,
            'network_recv_mb': network_recv / 1024 / 1024,
            'threads': process.num_threads(),
            'open_files': len(process.open_files())
        }
        
        # Store metrics
        self.metrics['system'].append(metrics)
        
        # Keep only recent metrics
        if len(self.metrics['system']) > 1000:
            self.metrics['system'].pop(0)
            
        return metrics
        
    def check_system_health(self):
        """Check system health and return warnings"""
        metrics = self.get_system_metrics()
        warnings = []
        
        # Memory warnings
        if metrics['memory_mb'] > self.config.MEMORY_LIMIT_MB * 0.9:
            warnings.append(f"High memory usage: {metrics['memory_mb']:.1f}MB")
            
        # CPU warnings
        if metrics['cpu_percent'] > 90:
            warnings.append(f"High CPU usage: {metrics['cpu_percent']:.1f}%")
            
        # Disk warnings
        if metrics['disk_percent'] > 90:
            warnings.append(f"Low disk space: {metrics['disk_percent']:.1f}% used")
            
        # Thread warnings
        if metrics['threads'] > 100:
            warnings.append(f"High thread count: {metrics['threads']}")
            
        return warnings
 
def initialize_monster_bot():
    """Initialize the Monster Bot with all configurations"""
    print("üöÄ Initializing Monster Trading Bot...")
    
    # Load configuration
    config = MonsterBotConfig()
    
    # Validate configuration
    errors, warnings = ConfigValidator.validate_config(config)
    
    if errors:
        print("‚ùå Configuration errors found:")
        for error in errors:
            print(f"   - {error}")
        print("\nPlease fix configuration errors before starting.")
        return None, None, None
        
    if warnings:
        print("‚ö†Ô∏è  Configuration warnings:")
        for warning in warnings:
            print(f"   - {warning}")
        print()
        
    # Initialize logging
    logger_manager = AdvancedLogger(config)
    logger = logger_manager.logger
    
    # Initialize memory manager
    memory_manager = MemoryManager(config)
    
    # Initialize system monitor
    system_monitor = SystemMonitor(config)
    
    # Log system information
    logger.info(f"üí∞ Starting Capital: ${config.STARTING_CAPITAL}")
    logger.info(f"üéØ Paper Mode: {config.PAPER_MODE}")
    logger.info(f"üß† Neural Networks: {config.ENABLE_NEURAL_NETWORK}")
    logger.info(f"üì± Sentiment Analysis: {config.ENABLE_SENTIMENT_ANALYSIS}")
    logger.info(f"üåê Web Dashboard: {config.ENABLE_WEB_DASHBOARD}")
    
    # Check system resources
    system_metrics = system_monitor.get_system_metrics()
    logger.info(f"üíæ System Memory: {system_metrics['memory_mb']:.1f}MB")
    logger.info(f"üîß CPU Cores: {os.cpu_count()}")
    logger.info(f"‚ö° Max Workers: {config.MAX_WORKERS}")
    
    logger.info("‚úÖ Monster Bot Core Initialization Complete")
    
    return config, logger_manager, memory_manager
 
if __name__ == "__main__":
    # Test initialization
    config, logger_manager, memory_manager = initialize_monster_bot()
    
    if config:
        print("üéâ Monster Bot Core successfully initialized!")
        print(f"üìä Cache size limit: {config.CACHE_SIZE}")
        print(f"üíæ Memory limit: {config.MEMORY_LIMIT_MB}MB")
        print(f"üîÑ GC frequency: {config.GC_FREQUENCY}")
        
        # Test memory manager
        memory_stats = memory_manager.get_cache_stats()
        print(f"üìà Current memory usage: {memory_stats['current_memory_mb']:.1f}MB")
        
        # Display personality modes
        print("\nüé≠ Available Personality Modes:")
        for mode_name, mode_config in config.PERSONALITY_MODES.items():
            capital_range = mode_config['capital_range']
            max_pos = mode_config['max_position_size'] * 100
            max_lev = mode_config['max_leverage']
            daily_target = mode_config['daily_target'] * 100
            
            print(f"   {mode_config['description']}")
            print(f"      Capital: ${capital_range[0]}-${capital_range[1] if capital_range[1] != float('inf') else '‚àû'}")
            print(f"      Max Position: {max_pos}% | Max Leverage: {max_lev}x | Daily Target: {daily_target}%")
            print()
    else:
        print("‚ùå Failed to initialize Monster Bot Core")
        sys.exit(1)
        
class RateLimiter:
    """Advanced rate limiter for Binance API with multiple limit types"""
    
    def __init__(self):
        self.locks = defaultdict(threading.Lock)
        
        # Binance rate limits (requests per interval)
        self.limits = {
            'requests_per_minute': 1200,
            'requests_per_second': 20,
            'orders_per_10sec': 50,
            'orders_per_day': 160000,
            'weight_per_minute': 1200
        }
        
        # Request counters with timestamps
        self.counters = {
            'requests_minute': deque(),
            'requests_second': deque(),
            'orders_10sec': deque(),
            'orders_day': deque(),
            'weight_minute': deque()
        }
        
        # Request weights for different endpoints
        self.endpoint_weights = {
            'ping': 1,
            'time': 1,
            'exchangeInfo': 10,
            'depth': 1,
            'trades': 1,
            'aggTrades': 1,
            'klines': 1,
            'ticker_24hr': 1,
            'ticker_price': 1,
            'account': 10,
            'order': 1,
            'openOrders': 3,
            'allOrders': 10
        }
        
    def can_make_request(self, endpoint='default', is_order=False):
        """Check if we can make a request without hitting rate limits"""
        with self.locks['main']:
            now = time.time()
            self.logger.info("‚úÖ API connectivity test passed")
            
            return {
                'server_time': server_time,
                'account_status': 'OK',
                'market_data': 'OK',
                'permissions': account.get('permissions', []),
                'can_trade': account.get('canTrade', False),
                'can_withdraw': account.get('canWithdraw', False)
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Connectivity test failed: {e}")
            return None
 
class MarketScanner:
    """Advanced market scanning system for opportunity detection"""
    
    def __init__(self, binance_client, config, memory_manager):
        self.binance_client = binance_client
        self.config = config
        self.memory_manager = memory_manager
        self.logger = logging.getLogger('MonsterBot.MarketScanner')
        
        # Scanning state
        self.scan_results = []
        self.last_scan_time = 0
        self.scan_count = 0
        
        # Performance tracking
        self.scan_performance = deque(maxlen=100)
        self.symbol_performance = defaultdict(list)
        
    def scan_for_opportunities(self):
        """Comprehensive market scan for trading opportunities"""
        try:
            start_time = time.time()
            
            # Get top moving symbols
            symbols = self.binance_client.get_top_movers(
                limit=self.config.TOP_SYMBOLS_COUNT,
                min_volume=self.config.MIN_VOLUME_USDT,
                min_change=self.config.MIN_PRICE_CHANGE * 100
            )
            
            if not symbols:
                self.logger.warning("No symbols found in market scan")
                return []
                
            # Scan each symbol
            opportunities = []
            
            with ThreadPoolExecutor(max_workers=self.config.MAX_WORKERS) as executor:
                # Submit all scanning tasks
                future_to_symbol = {
                    executor.submit(self.scan_symbol, symbol): symbol 
                    for symbol in symbols
                }
                
                # Collect results
                for future in as_completed(future_to_symbol):
                    symbol = future_to_symbol[future]
                    try:
                        opportunity = future.result(timeout=30)
                        if opportunity:
                            opportunities.append(opportunity)
                    except Exception as e:
                        self.logger.debug(f"Scan failed for {symbol}: {e}")
                        continue
                        
            # Sort opportunities by score
            opportunities.sort(key=lambda x: x.get('total_score', 0), reverse=True)
            
            # Update scan statistics
            scan_duration = time.time() - start_time
            self.scan_performance.append({
                'timestamp': time.time(),
                'duration': scan_duration,
                'symbols_scanned': len(symbols),
                'opportunities_found': len(opportunities),
                'success_rate': len(opportunities) / len(symbols) if symbols else 0
            })
            
            self.last_scan_time = time.time()
            self.scan_count += 1
            
            self.logger.info(
                f"üîç Market scan complete: {len(opportunities)} opportunities "
                f"from {len(symbols)} symbols in {scan_duration:.2f}s"
            )
            
            self.scan_results = opportunities
            return opportunities
            
        except Exception as e:
            self.logger.error(f"Market scan failed: {e}")
            return []
            
    def scan_symbol(self, symbol):
        """Detailed scan of individual symbol"""
        try:
            scan_start = time.time()
            
            # Get market data
            df = self.binance_client.get_klines(
                symbol, 
                interval=self.config.PRIMARY_TIMEFRAME,
                limit=self.config.LOOKBACK_PERIODS
            )
            
            if df is None or len(df) < 100:
                return None
                
            # Get order book
            order_book = self.binance_client.get_order_book(symbol, limit=50)
            
            # Get recent trades
            recent_trades, trade_summary = self.binance_client.get_recent_trades(symbol, limit=200)
            
            # Get symbol statistics
            symbol_stats = self.binance_client.get_symbol_statistics(symbol)
            
            if not all([order_book, recent_trades, symbol_stats]):
                return None
                
            # Calculate technical indicators
            df = self.add_technical_indicators(df)
            
            # Analyze market structure
            market_structure = self.analyze_market_structure(df, symbol_stats)
            
            # Analyze order flow
            order_flow = self.analyze_order_flow(order_book, recent_trades, trade_summary)
            
            # Analyze volume patterns
            volume_analysis = self.analyze_volume_patterns(df, symbol_stats)
            
            # Calculate momentum indicators
            momentum_score = self.calculate_momentum_score(df, symbol_stats)
            
            # Detect breakout potential
            breakout_score = self.detect_breakout_potential(df, order_book)
            
            # Analyze whale activity
            whale_activity = self.detect_whale_activity(order_book, recent_trades, df)
            
            # Calculate overall opportunity score
            total_score = self.calculate_total_score({
                'market_structure': market_structure,
                'order_flow': order_flow,
                'volume_analysis': volume_analysis,
                'momentum_score': momentum_score,
                'breakout_score': breakout_score,
                'whale_activity': whale_activity
            })
            
            # Only return high-scoring opportunities
            if total_score >= 60:  # Minimum score threshold
                opportunity = {
                    'symbol': symbol,
                    'timestamp': time.time(),
                    'total_score': total_score,
                    'current_price': df['close'].iloc[-1],
                    'market_structure': market_structure,
                    'order_flow': order_flow,
                    'volume_analysis': volume_analysis,
                    'momentum_score': momentum_score,
                    'breakout_score': breakout_score,
                    'whale_activity': whale_activity,
                    'technical_data': {
                        'rsi': df['rsi'].iloc[-1] if 'rsi' in df.columns else None,
                        'macd_hist': df['macd_hist'].iloc[-1] if 'macd_hist' in df.columns else None,
                        'volume_ratio': df['volume_ratio'].iloc[-1] if 'volume_ratio' in df.columns else None,
                        'atr': df['atr'].iloc[-1] if 'atr' in df.columns else None
                    },
                    'scan_duration': time.time() - scan_start
                }
                
                return opportunity
                
            return None
            
        except Exception as e:
            self.logger.debug(f"Symbol scan failed for {symbol}: {e}")
            return None
            
    def add_technical_indicators(self, df):
        """Add comprehensive technical indicators"""
        try:
            # Moving averages
            df['ema_9'] = df['close'].ewm(span=9).mean()
            df['ema_21'] = df['close'].ewm(span=21).mean()
            df['ema_50'] = df['close'].ewm(span=50).mean()
            df['sma_200'] = df['close'].rolling(200).mean()
            
            # RSI
            df['rsi'] = self.calculate_rsi(df['close'], 14)
            df['rsi_fast'] = self.calculate_rsi(df['close'], 7)
            
            # MACD
            exp1 = df['close'].ewm(span=12).mean()
            exp2 = df['close'].ewm(span=26).mean()
            df['macd'] = exp1 - exp2
            df['macd_signal'] = df['macd'].ewm(span=9).mean()
            df['macd_hist'] = df['macd'] - df['macd_signal']
            
            # Bollinger Bands
            df['bb_middle'] = df['close'].rolling(20).mean()
            bb_std = df['close'].rolling(20).std()
            df['bb_upper'] = df['bb_middle'] + (bb_std * 2)
            df['bb_lower'] = df['bb_middle'] - (bb_std * 2)
            df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_middle']
            
            # ATR
            df['atr'] = self.calculate_atr(df, 14)
            
            # Volume indicators
            df['volume_sma'] = df['volume'].rolling(20).mean()
            df['volume_ratio'] = df['volume'] / df['volume_sma']
            
            # Support and resistance
            df['resistance'] = df['high'].rolling(20).max()
            df['support'] = df['low'].rolling(20).min()
            
            # Price position
            df['price_position'] = (df['close'] - df['support']) / (df['resistance'] - df['support'])
            
            return df
            
        except Exception as e:
            self.logger.error(f"Technical indicators calculation failed: {e}")
            return df
            
    def calculate_rsi(self, prices, period=14):
        """Calculate RSI indicator"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi
        
    def calculate_atr(self, df, period=14):
        """Calculate Average True Range"""
        high_low = df['high'] - df['low']
        high_close = np.abs(df['high'] - df['close'].shift())
        low_close = np.abs(df['low'] - df['close'].shift())
        ranges = pd.concat([high_low, high_close, low_close], axis=1)
        true_range = np.max(ranges, axis=1)
        atr = true_range.rolling(period).mean()
        return atr
        
    def analyze_market_structure(self, df, symbol_stats):
        """Analyze market structure for trend and key levels"""
        try:
            score = 0
            signals = {}
            
            # Trend analysis
            if len(df) >= 50:
                # EMA alignment
                current_price = df['close'].iloc[-1]
                ema_9 = df['ema_9'].iloc[-1]
                ema_21 = df['ema_21'].iloc[-1]
                ema_50 = df['ema_50'].iloc[-1]
                
                if current_price > ema_9 > ema_21 > ema_50:
                    score += 20
                    signals['strong_uptrend'] = True
                elif current_price > ema_9 > ema_21:
                    score += 10
                    signals['uptrend'] = True
                    
                # Price position analysis
                if 'price_position' in df.columns:
                    price_pos = df['price_position'].iloc[-1]
                    if price_pos > 0.8:
                        score += 15
                        signals['near_resistance'] = True
                    elif price_pos > 0.6:
                        score += 10
                        signals['upper_range'] = True
                        
                # Volatility analysis
                if symbol_stats and 'volatility_24h' in symbol_stats:
                    vol_24h = symbol_stats['volatility_24h']
                    if 0.05 < vol_24h < 0.20:  # 5-20% volatility
                        score += 10
                        signals['good_volatility'] = True
                        
            return {
                'score': score,
                'signals': signals,
                'max_score': 45
            }
            
        except Exception as e:
            self.logger.error(f"Market structure analysis failed: {e}")
            return {'score': 0, 'signals': {}, 'max_score': 45}
            
    def analyze_order_flow(self, order_book, recent_trades, trade_summary):
        """Analyze order flow for buying/selling pressure"""
        try:
            score = 0
            signals = {}
            
            if order_book and 'volume_imbalance' in order_book:
                imbalance = order_book['volume_imbalance']
                
                # Strong buying pressure
                if imbalance > 0.3:
                    score += 20
                    signals['strong_buying_pressure'] = True
                elif imbalance > 0.1:
                    score += 10
                    signals['buying_pressure'] = True
                    
            # Analyze trade summary
            if trade_summary:
                buy_sell_ratio = trade_summary.get('buy_sell_ratio', 1)
                
                if buy_sell_ratio > 1.5:
                    score += 15
                    signals['trade_flow_bullish'] = True
                elif buy_sell_ratio > 1.2:
                    score += 8
                    signals['trade_flow_positive'] = True
                    
                # Large trades analysis
                large_trades = trade_summary.get('large_trades', 0)
                total_trades = trade_summary.get('total_trades', 1)
                
                if large_trades / total_trades > 0.1:  # >10% large trades
                    score += 10
                    signals['institutional_activity'] = True
                    
            return {
                'score': score,
                'signals': signals,
                'max_score': 45
            }
            
        except Exception as e:
            self.logger.error(f"Order flow analysis failed: {e}")
            return {'score': 0, 'signals': {}, 'max_score': 45}
            
    def analyze_volume_patterns(self, df, symbol_stats):
        """Analyze volume patterns for confirmation"""
        try:
            score = 0
            signals = {}
            
            if 'volume_ratio' in df.columns:
                current_vol_ratio = df['volume_ratio'].iloc[-1]
                
                # Volume spike
                if current_vol_ratio > 3:
                    score += 20
                    signals['volume_spike'] = True
                elif current_vol_ratio > 2:
                    score += 12
                    signals['high_volume'] = True
                elif current_vol_ratio > 1.5:
                    score += 6
                    signals['above_avg_volume'] = True
                    
                # Volume trend
                recent_vol_trend = df['volume_ratio'].iloc[-5:].mean()
                older_vol_trend = df['volume_ratio'].iloc[-15:-5].mean()
                
                if recent_vol_trend > older_vol_trend * 1.3:
                    score += 10
                    signals['increasing_volume'] = True
                    
            return {
                'score': score,
                'signals': signals,
                'max_score': 30
            }
            
        except Exception as e:
            self.logger.error(f"Volume analysis failed: {e}")
            return {'score': 0, 'signals': {}, 'max_score': 30}
            
    def calculate_momentum_score(self, df, symbol_stats):
        """Calculate momentum indicators score"""
        try:
            score = 0
            signals = {}
            
            # RSI momentum
            if 'rsi' in df.columns:
                rsi = df['rsi'].iloc[-1]
                rsi_prev = df['rsi'].iloc[-2]
                
                if 50 < rsi < 75 and rsi > rsi_prev:
                    score += 15
                    signals['rsi_momentum'] = True
                elif 40 < rsi < 80:
                    score += 8
                    signals['rsi_neutral'] = True
                    
            # MACD momentum
            if 'macd_hist' in df.columns:
                macd_hist = df['macd_hist'].iloc[-1]
                macd_hist_prev = df['macd_hist'].iloc[-2]
                
                if macd_hist > 0 and macd_hist > macd_hist_prev:
                    score += 15
                    signals['macd_bullish'] = True
                elif macd_hist > macd_hist_prev:
                    score += 8
                    signals['macd_improving'] = True
                    
            # Price momentum
            if len(df) >= 10:
                price_momentum = (df['close'].iloc[-1] - df['close'].iloc[-10]) / df['close'].iloc[-10]
                
                if price_momentum > 0.03:  # 3%+ move in 10 periods
                    score += 10
                    signals['strong_price_momentum'] = True
                elif price_momentum > 0.01:
                    score += 5
                    signals['price_momentum'] = True
                    
            return {
                'score': score,
                'signals': signals,
                'max_score': 40
            }
            
        except Exception as e:
            self.logger.error(f"Momentum calculation failed: {e}")
            return {'score': 0, 'signals': {}, 'max_score': 40}
            
    def detect_breakout_potential(self, df, order_book):
        """Detect potential breakout setups"""
        try:
            score = 0
            signals = {}
            
            # Bollinger Band squeeze
            if 'bb_width' in df.columns:
                current_width = df['bb_width'].iloc[-1]
                avg_width = df['bb_width'].rolling(20).mean().iloc[-1]
                
                if current_width < avg_width * 0.6:
                    score += 15
                    signals['bb_squeeze'] = True
                    
            # Near resistance breakout
            if 'resistance' in df.columns:
                current_price = df['close'].iloc[-1]
                resistance = df['resistance'].iloc[-1]
                
                distance_to_resistance = (resistance - current_price) / current_price
                
                if 0 < distance_to_resistance < 0.02:  # Within 2% of resistance
                    score += 20
                    signals['near_resistance_breakout'] = True
                elif 0 < distance_to_resistance < 0.05:  # Within 5%
                    score += 10
                    signals['approaching_resistance'] = True
                    
            # Volume at resistance
            if order_book and 'ask_volume_total' in order_book:
                ask_volume = order_book['ask_volume_total']
                bid_volume = order_book['bid_volume_total']
                
                if bid_volume > ask_volume * 1.5:
                    score += 10
                    signals['weak_resistance'] = True
                    
            return {
                'score': score,
                'signals': signals,
                'max_score': 45
            }
            
        except Exception as e:
            self.logger.error(f"Breakout detection failed: {e}")
            return {'score': 0, 'signals': {}, 'max_score': 45}
            
    def detect_whale_activity(self, order_book, recent_trades, df):
        """Detect whale activity indicators"""
        try:
            score = 0
            signals = {}
            
            # Large orders in order book
            if order_book and 'bids' in order_book:
                bid_sizes = [bid[1] for bid in order_book['bids'][:10]]
                ask_sizes = [ask[1] for ask in order_book['asks'][:10]]
                
                avg_bid_size = np.mean(bid_sizes) if bid_sizes else 0
                avg_ask_size = np.mean(ask_sizes) if ask_sizes else 0
                
                # Detect large orders
                large_bids = [size for size in bid_sizes if size > avg_bid_size * 5]
                large_asks = [size for size in ask_sizes if size > avg_ask_size * 5]
                
                if len(large_bids) > len(large_asks):
                    score += 15
                    signals['whale_bid_walls'] = True
                    
            # Large recent trades
            if recent_trades:
                trade_sizes = [trade['quantity'] for trade in recent_trades]
                avg_trade_size = np.mean(trade_sizes) if trade_sizes else 0
                
                large_trades = [t for t in recent_trades 
                               if t['quantity'] > avg_trade_size * 5]
                
                if len(large_trades) > len(recent_trades) * 0.05:  # >5% large trades
                    score += 10
                    signals['whale_trades'] = True
                    
                    # Analyze direction of large trades
                    large_buy_volume = sum(t['quantity'] for t in large_trades 
                                         if not t['is_buyer_maker'])
                    large_sell_volume = sum(t['quantity'] for t in large_trades 
                                          if t['is_buyer_maker'])
                    
                    if large_buy_volume > large_sell_volume * 1.5:
                        score += 10
                        signals['whale_buying'] = True
                        
            return {
                'score': score,
                'signals': signals,
                'max_score': 35
            }
            
        except Exception as e:
            self.logger.error(f"Whale activity detection failed: {e}")
            return {'score': 0, 'signals': {}, 'max_score': 35}
            
    def calculate_total_score(self, analysis_results):
        """Calculate weighted total score from all analyses"""
        try:
            # Weights for different analysis types
            weights = {
                'market_structure': 0.25,
                'order_flow': 0.20,
                'volume_analysis': 0.15,
                'momentum_score': 0.20,
                'breakout_score': 0.15,
                'whale_activity': 0.05
            }
            
            total_score = 0
            max_possible_score = 0
            
            for analysis_type, result in analysis_results.items():
                if isinstance(result, dict) and 'score' in result:
                    weight = weights.get(analysis_type, 0.1)
                    max_score = result.get('max_score', 100)
                    
                    normalized_score = (result['score'] / max_score) * 100
                    weighted_score = normalized_score * weight
                    
                    total_score += weighted_score
                    max_possible_score += 100 * weight
                    
            # Normalize to 0-100 scale
            final_score = (total_score / max_possible_score) * 100 if max_possible_score > 0 else 0
            
            return min(max(final_score, 0), 100)
            
        except Exception as e:
            self.logger.error(f"Total score calculation failed: {e}")
            return 0
            
    def get_scan_statistics(self):
        """Get scanning performance statistics"""
        if not self.scan_performance:
            return {}
            
        recent_scans = list(self.scan_performance)[-10:]  # Last 10 scans
        
        return {
            'total_scans': self.scan_count,
            'avg_scan_duration': np.mean([s['duration'] for s in recent_scans]),
            'avg_symbols_per_scan': np.mean([s['symbols_scanned'] for s in recent_scans]),
            'avg_opportunities_found': np.mean([s['opportunities_found'] for s in recent_scans]),
            'avg_success_rate': np.mean([s['success_rate'] for s in recent_scans]),
            'last_scan_time': self.last_scan_time,
            'opportunities_in_last_scan': len(self.scan_results)
        }
 
if __name__ == "__main__":
    # Test the Binance client and market scanner
    from monster_bot_part1 import initialize_monster_bot
    
    # Initialize core components
    config, logger_manager, memory_manager = initialize_monster_bot()
    
    if config:
        print("üîÑ Testing Binance Client...")
        
        # Initialize Binance client
        try:
            binance_client = AdvancedBinanceClient(config, memory_manager)
            
            # Test connectivity
            connectivity = binance_client.test_connectivity()
            if connectivity:
                print("‚úÖ Binance connectivity test passed")
                print(f"   Server time: {connectivity.get('server_time')}")
                print(f"   Can trade: {connectivity.get('can_trade')}")
                
                # Test market scanner
                print("\nüîç Testing Market Scanner...")
                scanner = MarketScanner(binance_client, config, memory_manager)
                
                # Run a quick scan
                opportunities = scanner.scan_for_opportunities()
                
                if opportunities:
                    print(f"‚úÖ Found {len(opportunities)} opportunities")
                    
                    # Show top opportunity
                    top_opportunity = opportunities[0]
                    print(f"\nüéØ Top Opportunity:")
                    print(f"   Symbol: {top_opportunity['symbol']}")
                    print(f"   Score: {top_opportunity['total_score']:.1f}/100")
                    print(f"   Price: ${top_opportunity['current_price']:.6f}")
                    
                    # Show scan statistics
                    stats = scanner.get_scan_statistics()
                    print(f"\nüìä Scan Statistics:")
                    print(f"   Avg duration: {stats.get('avg_scan_duration', 0):.2f}s")
                    print(f"   Success rate: {stats.get('avg_success_rate', 0):.2%}")
                    
                else:
                    print("‚ö†Ô∏è  No opportunities found in scan")
                    
            else:
                print("‚ùå Binance connectivity test failed")
                
        except Exception as e:
            print(f"‚ùå Binance client test failed: {e}")
            
    print("\nüéâ Part 2 testing complete!")_clean_old_requests(now)
            
            # Get endpoint weight
            weight = self.endpoint_weights.get(endpoint, 1)
            
            # Check per-second limit
            if len(self.counters['requests_second']) >= self.limits['requests_per_second']:
                return False, "Per-second limit exceeded"
                
            # Check per-minute request limit
            if len(self.counters['requests_minute']) >= self.limits['requests_per_minute']:
                return False, "Per-minute request limit exceeded"
                
            # Check per-minute weight limit
            current_weight = sum(w for _, w in self.counters['weight_minute'])
            if current_weight + weight > self.limits['weight_per_minute']:
                return False, "Per-minute weight limit exceeded"
                
            # Check order-specific limits
            if is_order:
                if len(self.counters['orders_10sec']) >= self.limits['orders_per_10sec']:
                    return False, "10-second order limit exceeded"
                    
                if len(self.counters['orders_day']) >= self.limits['orders_per_day']:
                    return False, "Daily order limit exceeded"
                    
            return True, "OK"
            
    def record_request(self, endpoint='default', is_order=False):
        """Record a request in the rate limiter"""
        with self.locks['main']:
            now = time.time()
            weight = self.endpoint_weights.get(endpoint, 1)
            
            # Record request
            self.counters['requests_second'].append(now)
            self.counters['requests_minute'].append(now)
            self.counters['weight_minute'].append((now, weight))
            
            # Record order if applicable
            if is_order:
                self.counters['orders_10sec'].append(now)
                self.counters['orders_day'].append(now)
                
    def _clean_old_requests(self, now):
        """Remove old requests from counters"""
        # Clean per-second counter (1 second)
        while (self.counters['requests_second'] and 
               now - self.counters['requests_second'][0] > 1):
            self.counters['requests_second'].popleft()
            
        # Clean per-minute counter (60 seconds)
        while (self.counters['requests_minute'] and 
               now - self.counters['requests_minute'][0] > 60):
            self.counters['requests_minute'].popleft()
            
        # Clean weight counter (60 seconds)
        while (self.counters['weight_minute'] and 
               now - self.counters['weight_minute'][0][0] > 60):
            self.counters['weight_minute'].popleft()
            
        # Clean order counters
        while (self.counters['orders_10sec'] and 
               now - self.counters['orders_10sec'][0] > 10):
            self.counters['orders_10sec'].popleft()
            
        while (self.counters['orders_day'] and 
               now - self.counters['orders_day'][0] > 86400):
            self.counters['orders_day'].popleft()
            
    def wait_if_needed(self, endpoint='default', is_order=False, max_wait=30):
        """Wait if rate limit would be exceeded"""
        wait_time = 0.1
        total_waited = 0
        
        while total_waited < max_wait:
            can_proceed, reason = self.can_make_request(endpoint, is_order)
            if can_proceed:
                return True
                
            time.sleep(wait_time)
            total_waited += wait_time
            wait_time = min(wait_time * 1.2, 2.0)  # Exponential backoff
            
        return False
        
    def get_rate_limit_status(self):
        """Get current rate limit status"""
        now = time.time()
        self._clean_old_requests(now)
        
        current_weight = sum(w for _, w in self.counters['weight_minute'])
        
        return {
            'requests_per_second': len(self.counters['requests_second']),
            'requests_per_minute': len(self.counters['requests_minute']),
            'weight_per_minute': current_weight,
            'orders_per_10sec': len(self.counters['orders_10sec']),
            'orders_per_day': len(self.counters['orders_day']),
            'limits': self.limits
        }
 
def rate_limited(endpoint='default', is_order=False):
    """Decorator for rate-limited API calls"""
    def decorator(func):
        def wrapper(self, *args, **kwargs):
            if hasattr(self, 'rate_limiter'):
                if not self.rate_limiter.wait_if_needed(endpoint, is_order):
                    raise Exception(f"Rate limit exceeded for {endpoint}")
                    
                start_time = time.time()
                try:
                    result = func(self, *args, **kwargs)
                    self.rate_limiter.record_request(endpoint, is_order)
                    
                    # Track performance
                    if hasattr(self, 'memory_manager'):
                        duration = time.time() - start_time
                        self.memory_manager.track_performance(f"api_{endpoint}", duration, True)
                        
                    return result
                except Exception as e:
                    if hasattr(self, 'memory_manager'):
                        duration = time.time() - start_time
                        self.memory_manager.track_performance(f"api_{endpoint}", duration, False)
                    raise
            else:
                return func(self, *args, **kwargs)
        return wrapper
    return decorator
 
class AdvancedBinanceClient:
    """Enhanced Binance client with intelligent features"""
    
    def __init__(self, config, memory_manager):
        self.config = config
        self.memory_manager = memory_manager
        self.logger = logging.getLogger('MonsterBot.BinanceClient')
        
        # Rate limiting
        self.rate_limiter = RateLimiter()
        
        # Connection management
        self.client = None
        self.testnet_client = None
        self.websocket_manager = None
        
        # Data caches with timestamps
        self.symbol_info_cache = {}
        self.price_cache = {}
        self.volume_cache = {}
        
        # Performance tracking
        self.request_count = 0
        self.error_count = 0
        self.last_request_time = 0
        
        # WebSocket data
        self.ws_data = defaultdict(dict)
        self.ws_connections = {}
        
        self.initialize_clients()
        
    def initialize_clients(self):
        """Initialize Binance clients with proper error handling"""
        try:
            # Test connection first
            self.testnet_client = Client(
                api_key=self.config.API_KEY,
                api_secret=self.config.API_SECRET,
                testnet=True
            )
            
            # Verify testnet connection
            self.testnet_client.ping()
            self.logger.info("‚úÖ Binance testnet connection successful")
            
            # Initialize production client if not in paper mode
            if not self.config.PAPER_MODE:
                self.client = Client(
                    api_key=self.config.API_KEY,
                    api_secret=self.config.API_SECRET,
                    testnet=False
                )
                
                # Test production connection
                account_info = self.client.futures_account()
                balance_count = len([a for a in account_info.get('assets', []) if float(a['walletBalance']) > 0])
                
                self.logger.info(f"‚úÖ Production client initialized. Active balances: {balance_count}")
            else:
                self.client = self.testnet_client
                self.logger.info("üìù Using testnet client (Paper mode)")
                
            # Cache symbol information
            self.cache_symbol_info()
            
            # Initialize WebSocket manager
            self.setup_websockets()
            
        except BinanceAPIException as e:
            self.logger.error(f"‚ùå Binance API Error: {e}")
            raise
        except Exception as e:
            self.logger.error(f"‚ùå Client initialization failed: {e}")
            raise
            
    def cache_symbol_info(self):
        """Cache comprehensive symbol information"""
        try:
            exchange_info = self.client.futures_exchange_info()
            
            for symbol_data in exchange_info['symbols']:
                symbol = symbol_data['symbol']
                
                # Process filters
                filters = {}
                for filter_data in symbol_data['filters']:
                    filter_type = filter_data['filterType']
                    filters[filter_type] = filter_data
                    
                # Cache comprehensive symbol info
                self.symbol_info_cache[symbol] = {
                    'symbol': symbol,
                    'status': symbol_data['status'],
                    'baseAsset': symbol_data['baseAsset'],
                    'quoteAsset': symbol_data['quoteAsset'],
                    'pricePrecision': int(symbol_data['pricePrecision']),
                    'quantityPrecision': int(symbol_data['quantityPrecision']),
                    'baseAssetPrecision': int(symbol_data['baseAssetPrecision']),
                    'quotePrecision': int(symbol_data['quotePrecision']),
                    'filters': filters,
                    'orderTypes': symbol_data['orderTypes'],
                    'timeInForce': symbol_data['timeInForce']
                }
                
            self.logger.info(f"üìã Cached info for {len(self.symbol_info_cache)} symbols")
            
        except Exception as e:
            self.logger.error(f"Failed to cache symbol info: {e}")
            
    def get_symbol_info(self, symbol):
        """Get cached symbol information"""
        return self.symbol_info_cache.get(symbol, {})
        
    def get_precision(self, symbol, precision_type='quantity'):
        """Get symbol precision"""
        symbol_info = self.get_symbol_info(symbol)
        
        precision_map = {
            'quantity': 'quantityPrecision',
            'price': 'pricePrecision',
            'base': 'baseAssetPrecision',
            'quote': 'quotePrecision'
        }
        
        precision_key = precision_map.get(precision_type, 'quantityPrecision')
        return symbol_info.get(precision_key, 8)
        
    def round_to_precision(self, value, symbol, precision_type='quantity'):
        """Round value to symbol precision"""
        precision = self.get_precision(symbol, precision_type)
        return round(float(value), precision)
        
    def get_lot_size_filter(self, symbol):
        """Get LOT_SIZE filter for symbol"""
        symbol_info = self.get_symbol_info(symbol)
        filters = symbol_info.get('filters', {})
        return filters.get('LOT_SIZE', {})
        
    def get_price_filter(self, symbol):
        """Get PRICE_FILTER for symbol"""
        symbol_info = self.get_symbol_info(symbol)
        filters = symbol_info.get('filters', {})
        return filters.get('PRICE_FILTER', {})
        
    def get_min_notional(self, symbol):
        """Get minimum notional value"""
        symbol_info = self.get_symbol_info(symbol)
        filters = symbol_info.get('filters', {})
        min_notional = filters.get('MIN_NOTIONAL', {})
        return float(min_notional.get('minNotional', 10.0))
        
    @rate_limited('account')
    def get_account_balance(self):
        """Get current USDT balance with caching"""
        cache_key = 'account_balance'
        cached_balance = self.memory_manager.cache_get(cache_key, 'balances')
        
        if cached_balance is not None:
            return cached_balance
            
        try:
            if self.config.PAPER_MODE:
                balance = self.config.STARTING_CAPITAL
            else:
                account_data = self.client.futures_account_balance()
                balance = 0.0
                
                for asset in account_data:
                    if asset['asset'] == 'USDT':
                        balance = float(asset['balance'])
                        break
                        
            # Cache balance
            self.memory_manager.cache_set(cache_key, balance, 'balances')
            return balance
            
        except Exception as e:
            self.logger.error(f"Failed to get account balance: {e}")
            return 0.0
            
    @rate_limited('ticker_24hr')
    def get_top_movers(self, limit=100, min_volume=5000000, min_change=0.5):
        """Get top moving symbols with advanced filtering and scoring"""
        cache_key = f'top_movers_{limit}_{min_volume}_{min_change}'
        cached_movers = self.memory_manager.cache_get(cache_key, 'market_data')
        
        if cached_movers is not None:
            return cached_movers
            
        try:
            start_time = time.time()
            
            # Get 24hr ticker statistics
            tickers = self.client.futures_ticker()
            
            valid_symbols = []
            
            for ticker in tickers:
                symbol = ticker['symbol']
                
                # Skip blacklisted symbols
                if symbol in self.config.SYMBOL_BLACKLIST or not symbol.endswith('USDT'):
                    continue
                    
                # Extract metrics
                volume_usdt = float(ticker['quoteVolume'])
                price_change_pct = float(ticker['priceChangePercent'])
                price = float(ticker['lastPrice'])
                volume_24h = float(ticker['volume'])
                high_24h = float(ticker['highPrice'])
                low_24h = float(ticker['lowPrice'])
                count = int(ticker['count'])
                
                # Advanced filtering
                if (volume_usdt >= min_volume and 
                    abs(price_change_pct) >= min_change and
                    price > 0.001 and
                    volume_24h > 1000 and
                    count > 100):
                    
                    # Calculate comprehensive momentum score
                    volatility = (high_24h - low_24h) / price if price > 0 else 0
                    
                    # Momentum score factors:
                    # 1. Volume (higher is better)
                    # 2. Price change magnitude (higher is better)
                    # 3. Price level (prefer mid-range prices)
                    # 4. Trade count (more activity)
                    # 5. Volatility (reasonable volatility preferred)
                    
                    volume_score = min(volume_usdt / 100000000, 10)  # Cap at 10
                    change_score = min(abs(price_change_pct) / 10, 5)  # Cap at 5
                    price_score = 1 / max(price, 0.001) if price < 1 else min(price / 10, 2)
                    activity_score = min(count / 10000, 3)  # Cap at 3
                    volatility_score = min(volatility * 100, 2)  # Cap at 2
                    
                    momentum_score = (
                        volume_score * 0.3 +
                        change_score * 0.3 +
                        price_score * 0.1 +
                        activity_score * 0.1 +
                        volatility_score * 0.2
                    )
                    
                    valid_symbols.append({
                        'symbol': symbol,
                        'price': price,
                        'volume_usdt': volume_usdt,
                        'change_pct': price_change_pct,
                        'volatility': volatility,
                        'momentum_score': momentum_score,
                        'volume_24h': volume_24h,
                        'trade_count': count,
                        'high_24h': high_24h,
                        'low_24h': low_24h
                    })
                    
            # Sort by momentum score and absolute change
            valid_symbols.sort(
                key=lambda x: (abs(x['change_pct']), x['momentum_score']), 
                reverse=True
            )
            
            # Extract top symbols
            top_symbols = [s['symbol'] for s in valid_symbols[:limit]]
            
            # Cache results
            self.memory_manager.cache_set(cache_key, top_symbols, 'market_data')
            
            duration = time.time() - start_time
            self.logger.info(f"üìà Found {len(top_symbols)} top movers in {duration:.2f}s")
            
            return top_symbols
            
        except Exception as e:
            self.logger.error(f"Failed to get top movers: {e}")
            # Return fallback symbols
            return self.config.PREFERRED_SYMBOLS[:limit]
            
    @rate_limited('klines')
    def get_klines(self, symbol, interval='15m', limit=500):
        """Get kline data with intelligent caching"""
        cache_key = f'klines_{symbol}_{interval}_{limit}'
        cached_data = self.memory_manager.cache_get(cache_key, 'market_data')
        
        if cached_data is not None:
            return cached_data
            
        try:
            start_time = time.time()
            
            # Get klines from Binance
            klines = self.client.futures_klines(
                symbol=symbol,
                interval=interval,
                limit=limit
            )
            
            if not klines:
                return None
                
            # Convert to DataFrame with proper column names
            df = pd.DataFrame(klines, columns=[
                'timestamp', 'open', 'high', 'low', 'close', 'volume',
                'close_time', 'quote_volume', 'trades', 'taker_buy_base',
                'taker_buy_quote', 'ignore'
            ])
            
            # Convert to proper data types
            numeric_columns = [
                'open', 'high', 'low', 'close', 'volume', 
                'quote_volume', 'trades', 'taker_buy_base', 'taker_buy_quote'
            ]
            
            for col in numeric_columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
                
            # Add datetime index
            df['datetime'] = pd.to_datetime(df['timestamp'], unit='ms')
            df = df.set_index('datetime')
            
            # Add basic calculated fields
            df['typical_price'] = (df['high'] + df['low'] + df['close']) / 3
            df['hl2'] = (df['high'] + df['low']) / 2
            df['ohlc4'] = (df['open'] + df['high'] + df['low'] + df['close']) / 4
            
            # Calculate returns
            df['returns'] = df['close'].pct_change()
            df['log_returns'] = np.log(df['close'] / df['close'].shift(1))
            
            # Volume metrics
            df['volume_change'] = df['volume'].pct_change()
            df['dollar_volume'] = df['volume'] * df['close']
            
            # Cache the processed data
            cache_duration = self._get_cache_duration(interval)
            self.memory_manager.cache_set(cache_key, df, 'market_data')
            
            duration = time.time() - start_time
            self.logger.debug(f"Fetched {len(df)} klines for {symbol} in {duration:.2f}s")
            
            return df
            
        except Exception as e:
            self.logger.error(f"Failed to get klines for {symbol}: {e}")
            return None
            
    def _get_cache_duration(self, interval):
        """Get appropriate cache duration based on timeframe"""
        duration_map = {
            '1m': 30,    # 30 seconds
            '3m': 60,    # 1 minute
            '5m': 120,   # 2 minutes
            '15m': 300,  # 5 minutes
            '30m': 600,  # 10 minutes
            '1h': 1200,  # 20 minutes
            '4h': 3600,  # 1 hour
            '1d': 7200   # 2 hours
        }
        return duration_map.get(interval, 300)
        
    @rate_limited('depth')
    def get_order_book(self, symbol, limit=100):
        """Get order book with processing and caching"""
        cache_key = f'orderbook_{symbol}_{limit}'
        cached_book = self.memory_manager.cache_get(cache_key, 'market_data')
        
        if cached_book is not None:
            return cached_book
            
        try:
            order_book = self.client.futures_order_book(symbol=symbol, limit=limit)
            
            # Process order book data
            processed_book = {
                'symbol': symbol,
                'bids': [(float(bid[0]), float(bid[1])) for bid in order_book['bids']],
                'asks': [(float(ask[0]), float(ask[1])) for ask in order_book['asks']],
                'timestamp': time.time(),
                'last_update_id': order_book.get('lastUpdateId', 0)
            }
            
            # Calculate order book metrics
            bid_prices = [bid[0] for bid in processed_book['bids']]
            ask_prices = [ask[0] for ask in processed_book['asks']]
            bid_volumes = [bid[1] for bid in processed_book['bids']]
            ask_volumes = [ask[1] for ask in processed_book['asks']]
            
            if bid_prices and ask_prices:
                processed_book['spread'] = ask_prices[0] - bid_prices[0]
                processed_book['spread_pct'] = processed_book['spread'] / bid_prices[0]
                processed_book['mid_price'] = (bid_prices[0] + ask_prices[0]) / 2
                
                # Volume analysis
                processed_book['bid_volume_total'] = sum(bid_volumes[:10])  # Top 10 levels
                processed_book['ask_volume_total'] = sum(ask_volumes[:10])
                processed_book['volume_imbalance'] = (
                    processed_book['bid_volume_total'] - processed_book['ask_volume_total']
                ) / (processed_book['bid_volume_total'] + processed_book['ask_volume_total'])
                
            # Cache for short duration (order books change rapidly)
            self.memory_manager.cache_set(cache_key, processed_book, 'market_data')
            
            return processed_book
            
        except Exception as e:
            self.logger.error(f"Failed to get order book for {symbol}: {e}")
            return None
            
    @rate_limited('aggTrades')
    def get_recent_trades(self, symbol, limit=500):
        """Get recent aggregate trades with analysis"""
        try:
            trades = self.client.futures_aggregate_trades(symbol=symbol, limit=limit)
            
            if not trades:
                return []
                
            processed_trades = []
            
            for trade in trades:
                processed_trades.append({
                    'price': float(trade['p']),
                    'quantity': float(trade['q']),
                    'timestamp': int(trade['T']),
                    'is_buyer_maker': trade['m'],
                    'trade_id': trade['a']
                })
                
            # Sort by timestamp
            processed_trades.sort(key=lambda x: x['timestamp'])
            
            # Add trade analysis
            if len(processed_trades) > 1:
                # Calculate trade metrics
                prices = [t['price'] for t in processed_trades]
                quantities = [t['quantity'] for t in processed_trades]
                
                # Volume-weighted average price
                vwap = sum(p * q for p, q in zip(prices, quantities)) / sum(quantities)
                
                # Buy/sell volume
                buy_volume = sum(t['quantity'] for t in processed_trades if not t['is_buyer_maker'])
                sell_volume = sum(t['quantity'] for t in processed_trades if t['is_buyer_maker'])
                
                # Add summary statistics
                trade_summary = {
                    'vwap': vwap,
                    'buy_volume': buy_volume,
                    'sell_volume': sell_volume,
                    'buy_sell_ratio': buy_volume / (sell_volume + 1e-10),
                    'avg_trade_size': np.mean(quantities),
                    'large_trades': len([q for q in quantities if q > np.mean(quantities) * 3]),
                    'price_range': max(prices) - min(prices),
                    'total_trades': len(processed_trades)
                }
                
                return processed_trades, trade_summary
                
            return processed_trades, {}
            
        except Exception as e:
            self.logger.error(f"Failed to get recent trades for {symbol}: {e}")
            return [], {}
            
    def get_current_price(self, symbol):
        """Get current price with caching"""
        cache_key = f'price_{symbol}'
        cached_price = self.memory_manager.cache_get(cache_key, 'market_data')
        
        if cached_price is not None:
            return cached_price
            
        try:
            ticker = self.client.futures_symbol_ticker(symbol=symbol)
            price = float(ticker['price'])
            
            # Cache for 10 seconds
            self.memory_manager.cache_set(cache_key, price, 'market_data')
            
            return price
            
        except Exception as e:
            self.logger.error(f"Failed to get current price for {symbol}: {e}")
            return None
            
    def setup_websockets(self):
        """Setup WebSocket connections for real-time data"""
        try:
            # This would setup WebSocket streams for real-time data
            # For now, we'll use the REST API approach
            self.logger.info("üì° WebSocket setup completed (using REST API)")
            
        except Exception as e:
            self.logger.error(f"WebSocket setup failed: {e}")
            
    def get_symbol_statistics(self, symbol):
        """Get comprehensive symbol statistics"""
        try:
            ticker = self.client.futures_ticker(symbol=symbol)
            
            stats = {
                'symbol': symbol,
                'price': float(ticker['lastPrice']),
                'price_change': float(ticker['priceChange']),
                'price_change_pct': float(ticker['priceChangePercent']),
                'volume': float(ticker['volume']),
                'quote_volume': float(ticker['quoteVolume']),
                'high_24h': float(ticker['highPrice']),
                'low_24h': float(ticker['lowPrice']),
                'open_price': float(ticker['openPrice']),
                'prev_close': float(ticker['prevClosePrice']),
                'trade_count': int(ticker['count']),
                'weighted_avg_price': float(ticker['weightedAvgPrice'])
            }
            
            # Calculate additional metrics
            if stats['low_24h'] > 0:
                stats['volatility_24h'] = (stats['high_24h'] - stats['low_24h']) / stats['low_24h']
                
            if stats['open_price'] > 0:
                stats['intraday_change'] = (stats['price'] - stats['open_price']) / stats['open_price']
                
            return stats
            
        except Exception as e:
            self.logger.error(f"Failed to get statistics for {symbol}: {e}")
            return None
            
    def get_funding_rate(self, symbol):
        """Get current funding rate"""
        try:
            funding_info = self.client.futures_funding_rate(symbol=symbol, limit=1)
            
            if funding_info:
                latest = funding_info[0]
                return {
                    'symbol': symbol,
                    'funding_rate': float(latest['fundingRate']),
                    'funding_time': int(latest['fundingTime']),
                    'mark_price': float(latest.get('markPrice', 0))
                }
                
            return None
            
        except Exception as e:
            self.logger.error(f"Failed to get funding rate for {symbol}: {e}")
            return None
            
    def get_rate_limit_status(self):
        """Get current rate limit status"""
        return self.rate_limiter.get_rate_limit_status()
        
    def test_connectivity(self):
        """Test API connectivity and permissions"""
        try:
            # Test server time
            server_time = self.client.get_server_time()
            
            # Test account access
            account = self.client.futures_account()
            
            # Test market data access
            self.client.futures_ticker()
            
            self.logger.info("‚úÖ API connectivity test passed")
            
            return {
                'server_time': server_time,
                'account_status': 'OK',
                'market_data': 'OK',
                'permissions': account.get('permissions', []),
                'can_trade': account.get('canTrade', False),
                'can_withdraw': account.get('canWithdraw', False)
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Connectivity test failed: {e}")
            return None
 
class MarketDataProcessor:
    """Advanced market data processing and analysis"""
    
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger('MonsterBot.MarketDataProcessor')
        
    def calculate_volatility_metrics(self, df):
        """Calculate various volatility measures"""
        try:
            # True Range
            high_low = df['high'] - df['low']
            high_close = np.abs(df['high'] - df['close'].shift())
            low_close = np.abs(df['low'] - df['close'].shift())
            true_range = np.maximum(high_low, np.maximum(high_close, low_close))
            
            # Average True Range
            df['atr'] = true_range.rolling(14).mean()
            df['atr_pct'] = df['atr'] / df['close']
            
            # Realized volatility
            df['realized_vol'] = df['returns'].rolling(20).std() * np.sqrt(24)  # 24 periods per day
            
            # Parkinson volatility (using high-low)
            df['parkinson_vol'] = np.sqrt(
                (1 / (4 * np.log(2))) * 
                np.log(df['high'] / df['low']).rolling(20).mean()
            )
            
            # Garman-Klass volatility
            df['gk_vol'] = np.sqrt(
                0.5 * np.log(df['high'] / df['low']) ** 2 - 
                (2 * np.log(2) - 1) * np.log(df['close'] / df['open']) ** 2
            ).rolling(20).mean()
            
            return df
            
        except Exception as e:
            self.logger.error(f"Volatility calculation failed: {e}")
            return df
            
    def calculate_microstructure_features(self, df, order_book, trades):
        """Calculate market microstructure features"""
        try:
            features = {}
            
            # Spread analysis
            if order_book and 'bids' in order_book and 'asks' in order_book:
                best_bid = order_book['bids'][0][0]
                best_ask = order_book['asks'][0][0]
                
                features['spread'] = best_ask - best_bid
                features['spread_pct'] = features['spread'] / best_bid
                features['mid_price'] = (best_bid + best_ask) / 2
                
                # Order book imbalance
                bid_volume = sum(bid[1] for bid in order_book['bids'][:10])
                ask_volume = sum(ask[1] for ask in order_book['asks'][:10])
                
                features['order_imbalance'] = (bid_volume - ask_volume) / (bid_volume + ask_volume)
                
                # Depth analysis
                features['bid_depth'] = bid_volume
                features['ask_depth'] = ask_volume
                features['total_depth'] = bid_volume + ask_volume
                
            # Trade analysis
            if trades:
                # Trade size distribution
                trade_sizes = [t['quantity'] for t in trades]
                features['avg_trade_size'] = np.mean(trade_sizes)
                features['trade_size_std'] = np.std(trade_sizes)
                features['large_trade_ratio'] = len([s for s in trade_sizes if s > np.mean(trade_sizes) * 3]) / len(trade_sizes)
                
                # Order flow toxicity
                buy_trades = [t for t in trades if not t['is_buyer_maker']]
                sell_trades = [t for t in trades if t['is_buyer_maker']]
                
                features['buy_trade_count'] = len(buy_trades)
                features['sell_trade_count'] = len(sell_trades)
                features['trade_imbalance'] = (len(buy_trades) - len(sell_trades)) / len(trades)
                
            return features
            
        except Exception as e:
            self.logger.error(f"Microstructure calculation failed: {e}")
            return {}
            
    def detect_regime_changes(self, df):
        """Detect market regime changes"""
        try:
            # Volatility regime
            vol_ma = df['realized_vol'].rolling(50).mean()
            df['vol_regime'] = np.where(df['realized_vol'] > vol_ma * 1.5, 'high_vol', 
                                       np.where(df['realized_vol'] < vol_ma * 0.7, 'low_vol', 'normal_vol'))
            
            # Trend regime
            sma_20 = df['close'].rolling(20).mean()
            sma_50 = df['close'].rolling(50).mean()
            
            df['trend_regime'] = np.where(sma_20 > sma_50 * 1.02, 'uptrend',
                                        np.where(sma_20 < sma_50 * 0.98, 'downtrend', 'sideways'))
            
            # Volume regime
            vol_ma = df['volume'].rolling(20).mean()
            df['volume_regime'] = np.where(df['volume'] > vol_ma * 1.5, 'high_volume',
                                         np.where(df['volume'] < vol_ma * 0.7, 'low_volume', 'normal_volume'))
            
            return df
            
        except Exception as e:
            self.logger.error(f"Regime detection failed: {e}")
            return df
 
class OrderBookAnalyzer:
    """Advanced order book analysis"""
    
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger('MonsterBot.OrderBookAnalyzer')
        
    def analyze_depth_profile(self, order_book):
        """Analyze order book depth profile"""
        try:
            if not order_book or 'bids' not in order_book:
                return {}
                
            bids = order_book['bids']
            asks = order_book['asks']
            
            analysis = {}
            
            # Calculate cumulative volumes at different depths
            depths = [5, 10, 20, 50]
            
            for depth in depths:
                if len(bids) >= depth and len(asks) >= depth:
                    bid_vol = sum(bid[1] for bid in bids[:depth])
                    ask_vol = sum(ask[1] for ask in asks[:depth])
                    
                    analysis[f'bid_volume_{depth}'] = bid_vol
                    analysis[f'ask_volume_{depth}'] = ask_vol
                    analysis[f'imbalance_{depth}'] = (bid_vol - ask_vol) / (bid_vol + ask_vol)
                    
            # Find large orders (potential walls)
            bid_sizes = [bid[1] for bid in bids[:20]]
            ask_sizes = [ask[1] for ask in asks[:20]]
            
            avg_bid = np.mean(bid_sizes)
            avg_ask = np.mean(ask_sizes)
            
            large_bids = [i for i, size in enumerate(bid_sizes) if size > avg_bid * 3]
            large_asks = [i for i, size in enumerate(ask_sizes) if size > avg_ask * 3]
            
            analysis['large_bid_count'] = len(large_bids)
            analysis['large_ask_count'] = len(large_asks)
            analysis['wall_imbalance'] = len(large_bids) - len(large_asks)
            
            # Price clustering analysis
            bid_prices = [bid[0] for bid in bids[:50]]
            ask_prices = [ask[0] for ask in asks[:50]]
            
            # Round number clustering
            round_bids = len([p for p in bid_prices if self._is_round_number(p)])
            round_asks = len([p for p in ask_prices if self._is_round_number(p)])
            
            analysis['round_number_clustering'] = (round_bids + round_asks) / 100
            
            return analysis
            
        except Exception as e:
            self.logger.error(f"Depth profile analysis failed: {e}")
            return {}
            
    def _is_round_number(self, price):
        """Check if price is a round number"""
        # Check for round numbers (ending in 0, 5, etc.)
        price_str = f"{price:.8f}".rstrip('0').rstrip('.')
        return price_str[-1] in ['0', '5'] if price_str else False
        
    def detect_spoofing(self, order_book_history):
        """Detect potential order book spoofing"""
        try:
            if len(order_book_history) < 3:
                return {}
                
            spoofing_signals = {}
            
            # Analyze order placement and cancellation patterns
            for i in range(1, len(order_book_history)):
                current_book = order_book_history[i]
                prev_book = order_book_history[i-1]
                
                # Check for large orders that disappeared without execution
                prev_large_bids = self._get_large_orders(prev_book.get('bids', []))
                current_large_bids = self._get_large_orders(current_book.get('bids', []))
                
                disappeared_bids = []
                for price, size in prev_large_bids:
                    if not any(abs(p - price) < price * 0.001 for p, s in current_large_bids):
                        disappeared_bids.append((price, size))
                        
                if disappeared_bids:
                    spoofing_signals['disappeared_large_bids'] = len(disappeared_bids)
                    
            return spoofing_signals
            
        except Exception as e:
            self.logger.error(f"Spoofing detection failed: {e}")
            return {}
            
    def _get_large_orders(self, orders, threshold_multiplier=3):
        """Get large orders from order list"""
        if not orders:
            return []
            
        sizes = [order[1] for order in orders[:20]]
        avg_size = np.mean(sizes)
        
        return [(order[0], order[1]) for order in orders[:20] 
                if order[1] > avg_size * threshold_multiplier]
 
class LiquidityAnalyzer:
    """Analyze market liquidity"""
    
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger('MonsterBot.LiquidityAnalyzer')
        
    def calculate_liquidity_metrics(self, order_book, trades, symbol_stats):
        """Calculate comprehensive liquidity metrics"""
        try:
            metrics = {}
            
            if order_book:
                # Spread-based metrics
                if 'spread_pct' in order_book:
                    metrics['spread_pct'] = order_book['spread_pct']
                    
                    # Liquidity cost (price impact)
                    bid_depth = order_book.get('bid_volume_total', 0)
                    ask_depth = order_book.get('ask_volume_total', 0)
                    
                    # Estimate price impact for $10k trade
                    trade_size_usd = 10000
                    current_price = order_book.get('mid_price', 0)
                    
                    if current_price > 0:
                        trade_size_base = trade_size_usd / current_price
                        
                        # Calculate price impact
                        cumulative_volume = 0
                        price_impact = 0
                        
                        for price, volume in order_book.get('asks', []):
                            cumulative_volume += volume
                            if cumulative_volume >= trade_size_base:
                                price_impact = (price - current_price) / current_price
                                break
                                
                        metrics['price_impact_10k'] = price_impact
                        
            # Trade-based liquidity
            if trades:
                # Market impact from recent trades
                trade_prices = [t['price'] for t in trades[-20:]]
                trade_volumes = [t['quantity'] for t in trades[-20:]]
                
                if trade_prices:
                    price_volatility = np.std(trade_prices) / np.mean(trade_prices)
                    metrics['trade_price_volatility'] = price_volatility
                    
                    # Volume-weighted variance
                    total_volume = sum(trade_volumes)
                    if total_volume > 0:
                        vw_price = sum(p * v for p, v in zip(trade_prices, trade_volumes)) / total_volume
                        vw_variance = sum(v * (p - vw_price) ** 2 for p, v in zip(trade_prices, trade_volumes)) / total_volume
                        metrics['volume_weighted_variance'] = vw_variance
                        
            # Market depth resilience
            if symbol_stats:
                volume_24h = symbol_stats.get('volume', 0)
                trade_count = symbol_stats.get('trade_count', 1)
                
                metrics['avg_trade_size'] = volume_24h / trade_count if trade_count > 0 else 0
                metrics['market_depth_ratio'] = (bid_depth + ask_depth) / (volume_24h / 24) if volume_24h > 0 else 0
                
            return metrics
            
        except Exception as e:
            self.logger.error(f"Liquidity metrics calculation failed: {e}")
            return {}
 
class MarketEfficiencyAnalyzer:
    """Analyze market efficiency and anomalies"""
    
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger('MonsterBot.MarketEfficiencyAnalyzer')
        
    def test_market_efficiency(self, df):
        """Test various market efficiency hypotheses"""
        try:
            efficiency_tests = {}
            
            if len(df) < 100:
                return efficiency_tests
                
            returns = df['returns'].dropna()
            
            # Autocorrelation test
            lag_1_corr = returns.corr(returns.shift(1))
            efficiency_tests['autocorr_lag1'] = lag_1_corr
            
            # Variance ratio test
            variance_1 = returns.var()
            variance_5 = returns.rolling(5).sum().var() / 5
            
            if variance_1 > 0:
                efficiency_tests['variance_ratio'] = variance_5 / variance_1
                
            # Runs test for randomness
            runs_test_result = self._runs_test(returns > 0)
            efficiency_tests['runs_test_pvalue'] = runs_test_result
            
            # Hurst exponent
            hurst = self._calculate_hurst_exponent(df['close'])
            efficiency_tests['hurst_exponent'] = hurst
            
            return efficiency_tests
            
        except Exception as e:
            self.logger.error(f"Market efficiency test failed: {e}")
            return {}
            
    def _runs_test(self, binary_series):
        """Perform runs test for randomness"""
        try:
            runs = 1
            for i in range(1, len(binary_series)):
                if binary_series.iloc[i] != binary_series.iloc[i-1]:
                    runs += 1
                    
            n1 = sum(binary_series)
            n2 = len(binary_series) - n1
            
            if n1 == 0 or n2 == 0:
                return 0.5
                
            expected_runs = (2 * n1 * n2) / (n1 + n2) + 1
            variance_runs = (2 * n1 * n2 * (2 * n1 * n2 - n1 - n2)) / ((n1 + n2) ** 2 * (n1 + n2 - 1))
            
            if variance_runs > 0:
                z_stat = (runs - expected_runs) / np.sqrt(variance_runs)
                # Approximate p-value (simplified)
                p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))
                return p_value
                
            return 0.5
            
        except:
            return 0.5
            
    def _calculate_hurst_exponent(self, prices):
        """Calculate Hurst exponent for trend analysis"""
        try:
            lags = range(2, min(100, len(prices) // 4))
            tau = [np.sqrt(np.std(np.subtract(prices[lag:], prices[:-lag]))) for lag in lags]
            
            if len(tau) < 2:
                return 0.5
                
            poly = np.polyfit(np.log(lags), np.log(tau), 1)
            return poly[0] * 2.0
            
        except:
            return 0.5
 
def initialize_market_data_system(config, memory_manager):
    """Initialize the complete market data system"""
    try:
        # Initialize Binance client
        binance_client = AdvancedBinanceClient(config, memory_manager)
        
        # Initialize processors
        data_processor = MarketDataProcessor(config)
        orderbook_analyzer = OrderBookAnalyzer(config)
        liquidity_analyzer = LiquidityAnalyzer(config)
        efficiency_analyzer = MarketEfficiencyAnalyzer(config)
        
        # Initialize market scanner
        market_scanner = MarketScanner(binance_client, config, memory_manager)
        
        # Test connectivity
        connectivity = binance_client.test_connectivity()
        
        if not connectivity:
            raise Exception("Failed to connect to Binance API")
            
        logger = logging.getLogger('MonsterBot.MarketData')
        logger.info("‚úÖ Market data system initialized successfully")
        
        return {
            'binance_client': binance_client,
            'data_processor': data_processor,
            'orderbook_analyzer': orderbook_analyzer,
            'liquidity_analyzer': liquidity_analyzer,
            'efficiency_analyzer': efficiency_analyzer,
            'market_scanner': market_scanner,
            'connectivity': connectivity
        }
        
    except Exception as e:
        logger = logging.getLogger('MonsterBot.MarketData')
        logger.error(f"Failed to initialize market data system: {e}")
        raise
 
class FeatureEngineer:
    """Advanced feature engineering for neural network training"""
    
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger('MonsterBot.FeatureEngineer')
        self.feature_columns = []
        self.feature_scalers = {}
        
    def engineer_features(self, df, order_book=None, trades=None, sentiment=None):
        """Create comprehensive feature set for neural network"""
        try:
            if len(df) < 100:
                return None
                
            features = pd.DataFrame(index=df.index)
            
            # Price-based features
            features = self._add_price_features(features, df)
            
            # Technical indicator features
            features = self._add_technical_features(features, df)
            
            # Volume features
            features = self._add_volume_features(features, df)
            
            # Volatility features
            features = self._add_volatility_features(features, df)
            
            # Market microstructure features
            if order_book:
                features = self._add_microstructure_features(features, order_book)
                
            # Order flow features
            if trades:
                features = self._add_orderflow_features(features, trades)
                
            # Sentiment features
            if sentiment:
                features = self._add_sentiment_features(features, sentiment)
                
            # Time-based features
            features = self._add_time_features(features, df)
            
            # Statistical features
            features = self._add_statistical_features(features, df)
            
            # Cross-correlation features
            features = self._add_correlation_features(features, df)
            
            # Clean and prepare features
            features = self._clean_features(features)
            
            self.feature_columns = features.columns.tolist()
            
            return features
            
        except Exception as e:
            self.logger.error(f"Feature engineering failed: {e}")
            return None
            
    def _add_price_features(self, features, df):
        """Add price-based features"""
        # Returns at different periods
        for period in [1, 2, 3, 5, 10, 15, 20, 30]:
            features[f'return_{period}'] = df['close'].pct_change(period)
            features[f'log_return_{period}'] = np.log(df['close'] / df['close'].shift(period))
            
        # Price ratios
        features['high_low_ratio'] = df['high'] / df['low']
        features['open_close_ratio'] = df['open'] / df['close']
        features['close_high_ratio'] = df['close'] / df['high']
        features['close_low_ratio'] = df['close'] / df['low']
        
        # Price position in range
        features['price_position_daily'] = (df['close'] - df['low']) / (df['high'] - df['low'])
        
        for window in [5, 10, 20, 50]:
            high_window = df['high'].rolling(window).max()
            low_window = df['low'].rolling(window).min()
            features[f'price_position_{window}'] = (df['close'] - low_window) / (high_window - low_window)
            
        # Gap analysis
        features['gap'] = (df['open'] - df['close'].shift(1)) / df['close'].shift(1)
        features['gap_filled'] = np.where(
            (features['gap'] > 0) & (df['low'] <= df['close'].shift(1)), 1,
            np.where((features['gap'] < 0) & (df['high'] >= df['close'].shift(1)), 1, 0)
        )
        
        return features
        
    def _add_technical_features(self, features, df):
        """Add technical indicator features"""
        # Multiple RSI periods
        for period in [7, 14, 21, 50]:
            features[f'rsi_{period}'] = self._calculate_rsi(df['close'], period)
            
        # RSI derivatives
        features['rsi_14_change'] = features['rsi_14'].diff()
        features['rsi_14_velocity'] = features['rsi_14'].diff().diff()
        features['rsi_divergence'] = features['rsi_14'] - features['rsi_14'].rolling(20).mean()
        
        # MACD variations
        for fast, slow in [(8, 21), (12, 26), (5, 35)]:
            exp1 = df['close'].ewm(span=fast).mean()
            exp2 = df['close'].ewm(span=slow).mean()
            macd = exp1 - exp2
            signal = macd.ewm(span=9).mean()
            
            features[f'macd_{fast}_{slow}'] = macd
            features[f'macd_signal_{fast}_{slow}'] = signal
            features[f'macd_hist_{fast}_{slow}'] = macd - signal
            
        # Bollinger Bands variations
        for period, std_mult in [(10, 1.5), (20, 2), (50, 2.5)]:
            bb_middle = df['close'].rolling(period).mean()
            bb_std = df['close'].rolling(period).std()
            bb_upper = bb_middle + (bb_std * std_mult)
            bb_lower = bb_middle - (bb_std * std_mult)
            
            features[f'bb_upper_{period}'] = bb_upper
            features[f'bb_lower_{period}'] = bb_lower
            features[f'bb_width_{period}'] = (bb_upper - bb_lower) / bb_middle
            features[f'bb_position_{period}'] = (df['close'] - bb_lower) / (bb_upper - bb_lower)
            features[f'bb_squeeze_{period}'] = features[f'bb_width_{period}'] < features[f'bb_width_{period}'].rolling(20).mean() * 0.8
            
        # Moving averages
        for period in [5, 10, 20, 50, 100, 200]:
            features[f'sma_{period}'] = df['close'].rolling(period).mean()
            features[f'ema_{period}'] = df['close'].ewm(span=period).mean()
            features[f'price_sma_{period}_ratio'] = df['close'] / features[f'sma_{period}']
            features[f'price_ema_{period}_ratio'] = df['close'] / features[f'ema_{period}']
            
        # Moving average crossovers
        features['sma_5_20_cross'] = features['sma_5'] > features['sma_20']
        features['ema_9_21_cross'] = features['ema_9'] > features['ema_21']
        features['price_sma50_cross'] = df['close'] > features['sma_50']
        
        # Stochastic oscillator
        for period in [14, 21]:
            lowest_low = df['low'].rolling(period).min()
            highest_high = df['high'].rolling(period).max()
            features[f'stoch_k_{period}'] = ((df['close'] - lowest_low) / (highest_high - lowest_low)) * 100
            features[f'stoch_d_{period}'] = features[f'stoch_k_{period}'].rolling(3).mean()
            
        # Williams %R
        for period in [14, 21]:
            features[f'williams_r_{period}'] = ((df['high'].rolling(period).max() - df['close']) / 
                                              (df['high'].rolling(period).max() - df['low'].rolling(period).min())) * -100
            
        # Commodity Channel Index
        for period in [14, 20]:
            typical_price = (df['high'] + df['low'] + df['close']) / 3
            sma_tp = typical_price.rolling(period).mean()
            mad = typical_price.rolling(period).apply(lambda x: np.mean(np.abs(x - x.mean())))
            features[f'cci_{period}'] = (typical_price - sma_tp) / (0.015 * mad)
            
        return features
        
    def _add_volume_features(self, features, df):
        """Add volume-based features"""
        # Volume ratios
        for period in [5, 10, 20, 50]:
            features[f'volume_sma_{period}'] = df['volume'].rolling(period).mean()
            features[f'volume_ratio_{period}'] = df['volume'] / features[f'volume_sma_{period}']
            
        # Volume changes
        features['volume_change'] = df['volume'].pct_change()
        features['volume_acceleration'] = features['volume_change'].diff()
        
        # On-Balance Volume
        features['obv'] = (np.sign(df['close'].diff()) * df['volume']).cumsum()
        features['obv_ema'] = features['obv'].ewm(span=20).mean()
        features['obv_signal'] = features['obv'] > features['obv_ema']
        
        # Volume-Price Trend
        features['vpt'] = (df['volume'] * df['close'].pct_change()).cumsum()
        
        # Accumulation/Distribution Line
        clv = ((df['close'] - df['low']) - (df['high'] - df['close'])) / (df['high'] - df['low'])
        features['ad_line'] = (clv * df['volume']).cumsum()
        
        # Volume oscillators
        features['volume_oscillator'] = features['volume_sma_5'] - features['volume_sma_20']
        
        # Money Flow Index
        for period in [14, 21]:
            typical_price = (df['high'] + df['low'] + df['close']) / 3
            money_flow = typical_price * df['volume']
            
            positive_flow = money_flow.where(typical_price > typical_price.shift(1), 0).rolling(period).sum()
            negative_flow = money_flow.where(typical_price < typical_price.shift(1), 0).rolling(period).sum()
            
            features[f'mfi_{period}'] = 100 - (100 / (1 + positive_flow / negative_flow))
            
        # Volume at price levels
        features['volume_at_close'] = df['volume']
        features['volume_weighted_price'] = (df['volume'] * df['close']).rolling(20).sum() / df['volume'].rolling(20).sum()
        
        return features
        
    def _add_volatility_features(self, features, df):
        """Add volatility-based features"""
        # True Range and ATR
        high_low = df['high'] - df['low']
        high_close = np.abs(df['high'] - df['close'].shift())
        low_close = np.abs(df['low'] - df['close'].shift())
        true_range = np.maximum(high_low, np.maximum(high_close, low_close))
        
        for period in [7, 14, 21, 50]:
            features[f'atr_{period}'] = true_range.rolling(period).mean()
            features[f'atr_ratio_{period}'] = features[f'atr_{period}'] / df['close']
            
        # Realized volatility
        for period in [5, 10, 20, 50]:
            features[f'realized_vol_{period}'] = df['close'].pct_change().rolling(period).std() * np.sqrt(period)
            
        # GARCH-style volatility
        returns = df['close'].pct_change()
        for window in [10, 20]:
            features[f'garch_vol_{window}'] = returns.rolling(window).std() * np.sqrt(252)
            
        # Volatility ratios
        features['vol_ratio_5_20'] = features['realized_vol_5'] / features['realized_vol_20']
        features['vol_ratio_10_50'] = features['realized_vol_10'] / features['realized_vol_50']
        
        # High-Low volatility estimators
        for period in [10, 20]:
            features[f'parkinson_vol_{period}'] = np.sqrt(
                (1 / (4 * np.log(2))) * 
                (np.log(df['high'] / df['low']) ** 2).rolling(period).mean()
            )
            
        # Volatility regime indicators
        features['vol_regime'] = features['realized_vol_20'] > features['realized_vol_20'].rolling(50).mean()
        
        return features
        
    def _add_microstructure_features(self, features, order_book):
        """Add market microstructure features"""
        if not order_book:
            return features
            
        # Spread features
        if 'spread_pct' in order_book:
            features.loc[features.index[-1], 'spread_pct'] = order_book['spread_pct']
            
        # Order book imbalance
        if 'volume_imbalance' in order_book:
            features.loc[features.index[-1], 'order_imbalance'] = order_book['volume_imbalance']
            
        # Depth features
        for depth in [5, 10, 20]:
            if f'imbalance_{depth}' in order_book:
                features.loc[features.index[-1], f'depth_imbalance_{depth}'] = order_book[f'imbalance_{depth}']
                
        # Wall detection
        if 'large_bid_count' in order_book and 'large_ask_count' in order_book:
            features.loc[features.index[-1], 'wall_imbalance'] = (
                order_book['large_bid_count'] - order_book['large_ask_count']
            )
            
        return features
        
    def _add_orderflow_features(self, features, trades):
        """Add order flow features"""
        if not trades:
            return features
            
        # Buy/sell ratios
        buy_volume = sum(t['quantity'] for t in trades if not t['is_buyer_maker'])
        sell_volume = sum(t['quantity'] for t in trades if t['is_buyer_maker'])
        
        if sell_volume > 0:
            features.loc[features.index[-1], 'buy_sell_ratio'] = buy_volume / sell_volume
        else:
            features.loc[features.index[-1], 'buy_sell_ratio'] = 10  # Very high buy pressure
            
        # Trade size analysis
        trade_sizes = [t['quantity'] for t in trades]
        if trade_sizes:
            features.loc[features.index[-1], 'avg_trade_size'] = np.mean(trade_sizes)
            features.loc[features.index[-1], 'trade_size_std'] = np.std(trade_sizes)
            
            # Large trade ratio
            large_threshold = np.mean(trade_sizes) * 3
            large_trades = [t for t in trades if t['quantity'] > large_threshold]
            features.loc[features.index[-1], 'large_trade_ratio'] = len(large_trades) / len(trades)
            
        return features
        
    def _add_sentiment_features(self, features, sentiment):
        """Add sentiment features"""
        if not sentiment:
            return features
            
        # Basic sentiment scores
        features.loc[features.index[-1], 'sentiment_score'] = sentiment.get('score', 0)
        features.loc[features.index[-1], 'sentiment_confidence'] = sentiment.get('confidence', 0)
        features.loc[features.index[-1], 'sentiment_extreme'] = sentiment.get('is_extreme', False)
        
        # Source-specific sentiment
        raw_scores = sentiment.get('raw_scores', {})
        for source, score in raw_scores.items():
            features.loc[features.index[-1], f'sentiment_{source}'] = score
            
        return features
        
    def _add_time_features(self, features, df):
        """Add time-based features"""
        # Hour of day
        features['hour'] = df.index.hour
        features['hour_sin'] = np.sin(2 * np.pi * features['hour'] / 24)
        features['hour_cos'] = np.cos(2 * np.pi * features['hour'] / 24)
        
        # Day of week
        features['day_of_week'] = df.index.dayofweek
        features['dow_sin'] = np.sin(2 * np.pi * features['day_of_week'] / 7)
        features['dow_cos'] = np.cos(2 * np.pi * features['day_of_week'] / 7)
        
        # Market session indicators
        features['asian_session'] = ((features['hour'] >= 0) & (features['hour'] < 8)).astype(int)
        features['london_session'] = ((features['hour'] >= 8) & (features['hour'] < 16)).astype(int)
        features['ny_session'] = ((features['hour'] >= 16) & (features['hour'] < 24)).astype(int)
        
        # Weekend indicator
        features['is_weekend'] = (features['day_of_week'] >= 5).astype(int)
        
        return features
        
    def _add_statistical_features(self, features, df):
        """Add statistical features"""
        # Skewness and kurtosis of returns
        for window in [10, 20, 50]:
            returns = df['close'].pct_change()
            features[f'skewness_{window}'] = returns.rolling(window).skew()
            features[f'kurtosis_{window}'] = returns.rolling(window).kurt()
            
        # Z-scores
        for col in ['close', 'volume']:
            for window in [20, 50]:
                rolling_mean = df[col].rolling(window).mean()
                rolling_std = df[col].rolling(window).std()
                features[f'{col}_zscore_{window}'] = (df[col] - rolling_mean) / rolling_std
                
        # Percentile ranks
        for window in [20, 50]:
            features[f'price_percentile_{window}'] = df['close'].rolling(window).rank() / window
            features[f'volume_percentile_{window}'] = df['volume'].rolling(window).rank() / window
            
        return features
        
    def _add_correlation_features(self, features, df):
        """Add cross-correlation features"""
        # Price-volume correlation
        for window in [10, 20]:
            features[f'price_volume_corr_{window}'] = df['close'].rolling(window).corr(df['volume'])
            
        # High-low correlation
        for window in [10, 20]:
            features[f'high_low_corr_{window}'] = df['high'].rolling(window).corr(df['low'])
            
        # Return autocorrelations
        returns = df['close'].pct_change()
        for lag in [1, 2, 3, 5]:
            features[f'return_autocorr_lag_{lag}'] = returns.rolling(20).corr(returns.shift(lag))
            
        return features
        
    def _clean_features(self, features):
        """Clean and prepare features"""
        # Replace infinities with NaN
        features = features.replace([np.inf, -np.inf], np.nan)
        
        # Forward fill then backward fill
        features = features.fillna(method='ffill').fillna(method='bfill')
        
        # Fill remaining NaN with 0
        features = features.fillna(0)
        
        return features
        
    def _calculate_rsi(self, prices, period=14):
        """Calculate RSI"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi
 
class NeuralNetworkPredictor:
    """Advanced neural network for price prediction"""
    
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger('MonsterBot.NeuralNetwork')
        
        # Model architecture
        self.models = {}
        self.scalers = {}
        self.feature_engineer = FeatureEngineer(config)
        
        # Training data
        self.training_data = []
        self.validation_data = []
        
        # Performance tracking
        self.model_performance = defaultdict(list)
        self.prediction_cache = {}
        
        # Model configurations
        self.model_configs = {
            'direction': {
                'layers': [512, 256, 128, 64, 32],
                'dropout': 0.3,
                'activation': 'relu',
                'output_activation': 'sigmoid',
                'loss': 'binary_crossentropy',
                'metrics': ['accuracy']
            },
            'magnitude': {
                'layers': [256, 128, 64, 32],
                'dropout': 0.2,
                'activation': 'relu',
                'output_activation': 'linear',
                'loss': 'mse',
                'metrics': ['mae']
            },
            'volatility': {
                'layers': [128, 64, 32],
                'dropout': 0.2,
                'activation': 'relu', 
                'output_activation': 'linear',
                'loss': 'mse',
                'metrics': ['mae']
            }
        }
        
        self.initialize_models()
        
    def initialize_models(self):
        """Initialize neural network models"""
        try:
            if HAS_TENSORFLOW:
                self._build_tensorflow_models()
            else:
                self._build_sklearn_models()
                
            self.logger.info("üß† Neural network models initialized")
            
        except Exception as e:
            self.logger.error(f"Model initialization failed: {e}")
            
    def _build_tensorflow_models(self):
        """Build TensorFlow models"""
        for model_name, config in self.model_configs.items():
            try:
                model = keras.Sequential()
                
                # Input layer
                model.add(layers.Dense(
                    config['layers'][0],
                    activation=config['activation'],
                    input_shape=(self.config.NN_CONFIG['input_features'],)
                ))
                model.add(layers.Dropout(config['dropout']))
                
                # Hidden layers
                for layer_size in config['layers'][1:]:
                    model.add(layers.Dense(layer_size, activation=config['activation']))
                    model.add(layers.Dropout(config['dropout']))
                    
                # Output layer
                output_size = 1 if model_name != 'direction' else 1
                model.add(layers.Dense(output_size, activation=config['output_activation']))
                
                # Compile model
                model.compile(
                    optimizer=keras.optimizers.Adam(learning_rate=self.config.NN_CONFIG['learning_rate']),
                    loss=config['loss'],
                    metrics=config['metrics']
                )
                
                self.models[model_name] = model
                
            except Exception as e:
                self.logger.error(f"Failed to build {model_name} model: {e}")
                
    def _build_sklearn_models(self):
        """Build scikit-learn models as fallback"""
        from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
        
        self.models['direction'] = RandomForestClassifier(
            n_estimators=500,
            max_depth=20,
            random_state=42,
            n_jobs=-1
        )
        
        self.models['magnitude'] = RandomForestRegressor(
            n_estimators=300,
            max_depth=15,
            random_state=42,
            n_jobs=-1
        )
        
        self.models['volatility'] = RandomForestRegressor(
            n_estimators=200,
            max_depth=12,
            random_state=42,
            n_jobs=-1
        )
        
    def prepare_training_data(self, symbol_data_dict, lookback_periods=50):
        """Prepare comprehensive training dataset"""
        try:
            all_features = []
            all_labels = []
            
            for symbol, data_list in symbol_data_dict.items():
                for data_entry in data_list:
                    df = data_entry['df']
                    order_book = data_entry.get('order_book')
                    trades = data_entry.get('trades')
                    sentiment = data_entry.get('sentiment')
                    
                    # Engineer features
                    features = self.feature_engineer.engineer_features(df, order_book, trades, sentiment)
                    
                    if features is None or len(features) < lookback_periods + 20:
                        continue
                        
                    # Create labels
                    labels = self._create_labels(df, lookback_periods)
                    
                    if labels is None:
                        continue
                        
                    # Create sequences
                    sequences, sequence_labels = self._create_sequences(features, labels, lookback_periods)
                    
                    if len(sequences) > 0:
                        all_features.extend(sequences)
                        all_labels.extend(sequence_labels)
                        
            if len(all_features) == 0:
                self.logger.warning("No training data generated")
                return False
                
            # Convert to arrays
            X = np.array(all_features)
            y = np.array(all_labels)
            
            # Split into train/validation
            split_idx = int(len(X) * 0.8)
            
            self.training_data = (X[:split_idx], y[:split_idx])
            self.validation_data = (X[split_idx:], y[split_idx:])
            
            self.logger.info(f"Training data prepared: {len(X)} samples, {X.shape[1]} features")
            
            return True
            
        except Exception as e:
            self.logger.error(f"Training data preparation failed: {e}")
            return False
            
    def _create_labels(self, df, future_periods=5):
        """Create prediction labels"""
        try:
            labels = {}
            
            # Price direction (1 = up, 0 = down)
            future_returns = df['close'].shift(-future_periods) / df['close'] - 1
            labels['direction'] = (future_returns > 0.002).astype(int)  # 0.2% threshold
            
            # Price magnitude (absolute future return)
            labels['magnitude'] = np.abs(future_returns)
            
            # Future volatility
            future_vol = df['close'].pct_change().shift(-future_periods).rolling(future_periods).std()
            labels['volatility'] = future_vol
            
            # Convert to DataFrame
            labels_df = pd.DataFrame(labels, index=df.index)
            
            return labels_df
            
        except Exception as e:
            self.logger.error(f"Label creation failed: {e}")
            return None
            
    def _create_sequences(self, features, labels, sequence_length):
        """Create sequences for time series prediction"""
        try:
            sequences = []
            sequence_labels = []
            
            # Align features and labels
            min_length = min(len(features), len(labels))
            features = features.iloc[:min_length]
            labels = labels.iloc[:min_length]
            
            # Remove rows with NaN
            valid_mask = ~(features.isna().any(axis=1) | labels.isna().any(axis=1))
            features = features[valid_mask]
            labels = labels[valid_mask]
            
            if len(features) < sequence_length + 10:
                return [], []
                
            # Create sequences
            for i in range(sequence_length, len(features) - 5):  # Leave buffer
                # Feature sequence
                feature_seq = features.iloc[i-sequence_length:i].values
                
                # Flatten sequence (alternative: keep 3D for LSTM)
                feature_seq_flat = feature_seq.flatten()
                
                # Labels at prediction point
                label_point = {
                    'direction': labels['direction'].iloc[i],
                    'magnitude': labels['magnitude'].iloc[i],
                    'volatility': labels['volatility'].iloc[i]
                }
                
                sequences.append(feature_seq_flat)
                sequence_labels.append(label_point)
                
            return sequences, sequence_labels
            
        except Exception as e:
            self.logger.error(f"Sequence creation failed: {e}")
            return [], []
            
    def train_models(self, epochs=100, batch_size=256):
        """Train all neural network models"""
        try:
            if not self.training_data or len(self.training_data[0]) == 0:
                self.logger.error("No training data available")
                return False
                
            X_train, y_train = self.training_data
            X_val, y_val = self.validation_data
            
            # Prepare data for each model
            training_results = {}
            
            for model_name in ['direction', 'magnitude', 'volatility']:
                self.logger.info(f"Training {model_name} model...")
                
                try:
                    # Extract labels for this model
                    y_train_model = np.array([y[model_name] for y in y_train])
                    y_val_model = np.array([y[model_name] for y in y_val])
                    
                    # Remove NaN values
                    train_mask = ~np.isnan(y_train_model)
                    val_mask = ~np.isnan(y_val_model)
                    
                    X_train_clean = X_train[train_mask]
                    y_train_clean = y_train_model[train_mask]
                    X_val_clean = X_val[val_mask]
                    y_val_clean = y_val_model[val_mask]
                    
                    if len(X_train_clean) == 0:
                        continue
                        
                    # Scale features
                    if model_name not in self.scalers:
                        self.scalers[model_name] = StandardScaler()
                        
                    X_train_scaled = self.scalers[model_name].fit_transform(X_train_clean)
                    X_val_scaled = self.scalers[model_name].transform(X_val_clean)
                    
                    # Train model
                    if HAS_TENSORFLOW and model_name in self.models:
                        # TensorFlow training
                        history = self.models[model_name].fit(
                            X_train_scaled, y_train_clean,
                            validation_data=(X_val_scaled, y_val_clean),
                            epochs=epochs,
                            batch_size=batch_size,
                            verbose=0,
                            callbacks=[
                                keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True),
                                keras.callbacks.ReduceLROnPlateau(patience=10, factor=0.5)
                            ]
                        )
                        
                        # Evaluate
                        train_loss = history.history['loss'][-1]
                        val_loss = history.history['val_loss'][-1]
                        
                        training_results[model_name] = {
                            'train_loss': train_loss,
                            'val_loss': val_loss,
                            'epochs_trained': len(history.history['loss'])
                        }
                        
                    else:
                        # Scikit-learn training
                        self.models[model_name].fit(X_train_scaled, y_train_clean)
                        
                        train_score = self.models[model_name].score(X_train_scaled, y_train_clean)
                        val_score = self.models[model_name].score(X_val_scaled, y_val_clean)
                        
                        training_results[model_name] = {
                            'train_score': train_score,
                            'val_score': val_score
                        }
                        
                    self.logger.info(f"‚úÖ {model_name} model trained successfully")
                    
                except Exception as e:
                    self.logger.error(f"Failed to train {model_name} model: {e}")
                    continue
                    
            # Save models
            self.save_models()
            
            self.logger.info(f"üéØ Neural network training completed")
            return True
            
        except Exception as e:
            self.logger.error(f"Model training failed: {e}")
            return False
            
    def predict(self, symbol, df, order_book=None, trades=None, sentiment=None):
        """Make real-time predictions"""
        try:
            # Check cache first
            cache_key = f"prediction_{symbol}_{len(df)}"
            if cache_key in self.prediction_cache:
                cached_pred = self.prediction_cache[cache_key]
                if time.time() - cached_pred['timestamp'] < 60:  # 1 minute cache
                    return cached_pred['prediction']
                    
            # Engineer features
            features = self.feature_engineer.engineer_features(df, order_book, trades, sentiment)
            
            if features is None or len(features) < 50:
                return None
                
            # Get latest feature vector
            latest_features = features.iloc[-1:].values
            
            predictions = {}
            
            # Make predictions with each model
            for model_name, model in self.models.items():
                if model is None or model_name not in self.scalers:
                    continue
                    
                try:
                    # Scale features
                    features_scaled = self.scalers[model_name].transform(latest_features)
                    
                    # Make prediction
                    if HAS_TENSORFLOW and hasattr(model, 'predict'):
                        pred = model.predict(features_scaled, verbose=0)[0]
                        if model_name == 'direction':
                            predictions[model_name] = float(pred[0])
                        else:
                            predictions[model_name] = float(pred)
                    else:
                        # Scikit-learn prediction
                        if model_name == 'direction' and hasattr(model, 'predict_proba'):
                            pred_proba = model.predict_proba(features_scaled)[0]
                            predictions[model_name] = pred_proba[1] if len(pred_proba) > 1 else pred_proba[0]
                        else:
                            pred = model.predict(features_scaled)[0]
                            predictions[model_name] = float(pred)
                            
                except Exception as e:
                    self.logger.error(f"Prediction failed for {model_name}: {e}")
                    continue
                    
            # Calculate final prediction
            final_prediction = self._calculate_final_prediction(predictions, symbol, df)
            
            # Cache prediction
            self.prediction_cache[cache_key] = {
                'prediction': final_prediction,
                'timestamp': time.time()
            }
            
            return final_prediction
            
        except Exception as e:
            self.logger.error(f"Prediction failed for {symbol}: {e}")
            return None
            
    def _calculate_final_prediction(self, predictions, symbol, df):
        """Calculate final prediction with confidence scoring"""
        try:
            if not predictions:
                return None
                
            # Base prediction components
            direction_prob = predictions.get('direction', 0.5)
            magnitude_pred = predictions.get('magnitude', 0.0)
            volatility_pred = predictions.get('volatility', 0.02)
            
            # Calculate directional confidence
            direction_confidence = abs(direction_prob - 0.5) * 2
            
            # Calculate magnitude confidence (higher magnitude = higher confidence)
            magnitude_confidence = min(magnitude_pred * 20, 1.0)
            
            # Calculate volatility adjustment
            vol_adjustment = 1.0
            if volatility_pred > 0.05:  # High volatility
                vol_adjustment = 0.8
            elif volatility_pred < 0.01:  # Low volatility
                vol_adjustment = 1.2
                
            # Market condition adjustments
            market_adjustment = self._get_market_condition_adjustment(df)
            
            # Calculate overall confidence
            base_confidence = (direction_confidence * 0.6 + magnitude_confidence * 0.4)
            adjusted_confidence = base_confidence * vol_adjustment * market_adjustment
            
            # Final confidence (0-1 scale)
            final_confidence = min(max(adjusted_confidence, 0.0), 1.0)
            
            # Determine trade direction
            if direction_prob > 0.55:
                trade_direction = 'long'
            elif direction_prob < 0.45:
                trade_direction = 'short'
            else:
                trade_direction = 'neutral'
                
            # Calculate expected move size
            expected_move = magnitude_pred * (1 if trade_direction == 'long' else -1)
            
            # Risk-adjusted position sizing recommendation
            risk_score = self._calculate_risk_score(volatility_pred, final_confidence)
            position_size_rec = self._recommend_position_size(final_confidence, risk_score)
            
            return {
                'symbol': symbol,
                'timestamp': time.time(),
                'direction': trade_direction,
                'direction_probability': direction_prob,
                'expected_move': expected_move,
                'magnitude_prediction': magnitude_pred,
                'volatility_prediction': volatility_pred,
                'confidence': final_confidence,
                'risk_score': risk_score,
                'position_size_recommendation': position_size_rec,
                'raw_predictions': predictions,
                'market_regime': self._identify_market_regime(df),
                'trade_signal': final_confidence > 0.65 and trade_direction != 'neutral'
            }
            
        except Exception as e:
            self.logger.error(f"Final prediction calculation failed: {e}")
            return None
            
    def _get_market_condition_adjustment(self, df):
        """Adjust confidence based on market conditions"""
        try:
            # Trend strength
            if len(df) >= 50:
                sma_20 = df['close'].rolling(20).mean().iloc[-1]
                sma_50 = df['close'].rolling(50).mean().iloc[-1]
                current_price = df['close'].iloc[-1]
                
                # Strong trend = higher confidence
                if current_price > sma_20 > sma_50:
                    trend_adjustment = 1.1
                elif current_price < sma_20 < sma_50:
                    trend_adjustment = 1.1
                else:
                    trend_adjustment = 0.9
            else:
                trend_adjustment = 1.0
                
            # Volume confirmation
            if 'volume' in df.columns and len(df) >= 20:
                current_volume = df['volume'].iloc[-1]
                avg_volume = df['volume'].rolling(20).mean().iloc[-1]
                
                if current_volume > avg_volume * 1.5:
                    volume_adjustment = 1.1
                elif current_volume < avg_volume * 0.7:
                    volume_adjustment = 0.9
                else:
                    volume_adjustment = 1.0
            else:
                volume_adjustment = 1.0
                
            return min(trend_adjustment * volume_adjustment, 1.3)
            
        except Exception as e:
            self.logger.error(f"Market condition adjustment failed: {e}")
            return 1.0
            
    def _calculate_risk_score(self, volatility_pred, confidence):
        """Calculate risk score for the prediction"""
        try:
            # Base risk from volatility
            vol_risk = min(volatility_pred * 10, 1.0)
            
            # Confidence risk (lower confidence = higher risk)
            conf_risk = 1.0 - confidence
            
            # Combined risk score
            risk_score = (vol_risk * 0.6 + conf_risk * 0.4)
            
            return min(max(risk_score, 0.0), 1.0)
            
        except:
            return 0.5
            
    def _recommend_position_size(self, confidence, risk_score):
        """Recommend position size based on confidence and risk"""
        try:
            # Base size from confidence
            base_size = confidence * 0.5  # Max 50% from confidence alone
            
            # Risk adjustment
            risk_adjustment = 1.0 - (risk_score * 0.5)
            
            # Final position size recommendation
            recommended_size = base_size * risk_adjustment
            
            return min(max(recommended_size, 0.05), 0.8)  # 5% to 80% range
            
        except:
            return 0.2
            
    def _identify_market_regime(self, df):
        """Identify current market regime"""
        try:
            if len(df) < 50:
                return 'unknown'
                
            # Volatility regime
            returns = df['close'].pct_change()
            current_vol = returns.rolling(20).std().iloc[-1]
            long_vol = returns.rolling(50).std().iloc[-1]
            
            if current_vol > long_vol * 1.5:
                vol_regime = 'high_vol'
            elif current_vol < long_vol * 0.7:
                vol_regime = 'low_vol'
            else:
                vol_regime = 'normal_vol'
                
            # Trend regime
            sma_20 = df['close'].rolling(20).mean().iloc[-1]
            sma_50 = df['close'].rolling(50).mean().iloc[-1]
            
            if sma_20 > sma_50 * 1.02:
                trend_regime = 'uptrend'
            elif sma_20 < sma_50 * 0.98:
                trend_regime = 'downtrend'
            else:
                trend_regime = 'sideways'
                
            return f"{trend_regime}_{vol_regime}"
            
        except:
            return 'unknown'
            
    def save_models(self):
        """Save trained models to disk"""
        try:
            models_dir = self.config.MODELS_DIR
            
            # Save TensorFlow models
            if HAS_TENSORFLOW:
                for model_name, model in self.models.items():
                    if model is not None and hasattr(model, 'save'):
                        model_path = os.path.join(models_dir, f'nn_{model_name}.h5')
                        model.save(model_path)
                        
            # Save scikit-learn models
            else:
                for model_name, model in self.models.items():
                    if model is not None:
                        model_path = os.path.join(models_dir, f'nn_{model_name}.pkl')
                        joblib.dump(model, model_path)
                        
            # Save scalers
            for scaler_name, scaler in self.scalers.items():
                scaler_path = os.path.join(models_dir, f'scaler_{scaler_name}.pkl')
                joblib.dump(scaler, scaler_path)
                
            # Save feature columns
            features_path = os.path.join(models_dir, 'feature_columns.json')
            with open(features_path, 'w') as f:
                json.dump(self.feature_engineer.feature_columns, f)
                
            self.logger.info("üíæ Neural network models saved")
            
        except Exception as e:
            self.logger.error(f"Model saving failed: {e}")
            
    def load_models(self):
        """Load trained models from disk"""
        try:
            models_dir = self.config.MODELS_DIR
            
            # Load TensorFlow models
            if HAS_TENSORFLOW:
                for model_name in ['direction', 'magnitude', 'volatility']:
                    model_path = os.path.join(models_dir, f'nn_{model_name}.h5')
                    if os.path.exists(model_path):
                        self.models[model_name] = keras.models.load_model(model_path)
                        
            # Load scikit-learn models
            else:
                for model_name in ['direction', 'magnitude', 'volatility']:
                    model_path = os.path.join(models_dir, f'nn_{model_name}.pkl')
                    if os.path.exists(model_path):
                        self.models[model_name] = joblib.load(model_path)
                        
            # Load scalers
            for scaler_name in ['direction', 'magnitude', 'volatility']:
                scaler_path = os.path.join(models_dir, f'scaler_{scaler_name}.pkl')
                if os.path.exists(scaler_path):
                    self.scalers[scaler_name] = joblib.load(scaler_path)
                    
            # Load feature columns
            features_path = os.path.join(models_dir, 'feature_columns.json')
            if os.path.exists(features_path):
                with open(features_path, 'r') as f:
                    self.feature_engineer.feature_columns = json.load(f)
                    
            self.logger.info("üìÇ Neural network models loaded")
            return True
            
        except Exception as e:
            self.logger.error(f"Model loading failed: {e}")
            return False
 
class PredictionValidator:
    """Validates and tracks prediction accuracy"""
    
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger('MonsterBot.PredictionValidator')
        
        # Prediction tracking
        self.active_predictions = {}
        self.prediction_history = []
        self.accuracy_metrics = defaultdict(list)
        
    def register_prediction(self, prediction):
        """Register a prediction for future validation"""
        try:
            if not prediction or 'symbol' not in prediction:
                return
                
            pred_id = f"{prediction['symbol']}_{prediction['timestamp']}"
            
            self.active_predictions[pred_id] = {
                'prediction': prediction,
                'start_price': prediction.get('current_price', 0),
                'start_time': prediction['timestamp'],
                'validation_time': prediction['timestamp'] + 300,  # 5 minutes
                'validated': False
            }
            
        except Exception as e:
            self.logger.error(f"Prediction registration failed: {e}")
            
    def validate_predictions(self, current_prices):
        """Validate active predictions"""
        try:
            current_time = time.time()
            validated_count = 0
            
            for pred_id, pred_data in list(self.active_predictions.items()):
                if pred_data['validated'] or current_time < pred_data['validation_time']:
                    continue
                    
                prediction = pred_data['prediction']
                symbol = prediction['symbol']
                
                if symbol not in current_prices:
                    continue
                    
                # Calculate actual outcome
                start_price = pred_data['start_price']
                current_price = current_prices[symbol]
                actual_return = (current_price - start_price) / start_price
                
                # Validate direction
                predicted_direction = prediction['direction']
                actual_direction = 'long' if actual_return > 0.002 else 'short' if actual_return < -0.002 else 'neutral'
                
                direction_correct = predicted_direction == actual_direction
                
                # Validate magnitude
                predicted_magnitude = prediction.get('expected_move', 0)
                magnitude_error = abs(abs(actual_return) - abs(predicted_magnitude))
                
                # Calculate accuracy scores
                direction_accuracy = 1.0 if direction_correct else 0.0
                magnitude_accuracy = max(0.0, 1.0 - magnitude_error * 10)  # Scale error
                
                # Overall prediction accuracy
                confidence = prediction.get('confidence', 0.5)
                overall_accuracy = (direction_accuracy * 0.7 + magnitude_accuracy * 0.3) * confidence
                
                # Record results
                validation_result = {
                    'symbol': symbol,
                    'prediction_time': pred_data['start_time'],
                    'validation_time': current_time,
                    'predicted_direction': predicted_direction,
                    'actual_direction': actual_direction,
                    'predicted_magnitude': predicted_magnitude,
                    'actual_return': actual_return,
                    'direction_correct': direction_correct,
                    'direction_accuracy': direction_accuracy,
                    'magnitude_accuracy': magnitude_accuracy,
                    'overall_accuracy': overall_accuracy,
                    'confidence': confidence
                }
                
                self.prediction_history.append(validation_result)
                
                # Update accuracy metrics
                self.accuracy_metrics['direction'].append(direction_accuracy)
                self.accuracy_metrics['magnitude'].append(magnitude_accuracy)
                self.accuracy_metrics['overall'].append(overall_accuracy)
                
                # Mark as validated
                pred_data['validated'] = True
                validated_count += 1
                
                # Keep history manageable
                if len(self.prediction_history) > 1000:
                    self.prediction_history.pop(0)
                    
            # Clean up old predictions
            self._cleanup_old_predictions(current_time)
            
            if validated_count > 0:
                self.logger.info(f"‚úÖ Validated {validated_count} predictions")
                
        except Exception as e:
            self.logger.error(f"Prediction validation failed: {e}")
            
    def _cleanup_old_predictions(self, current_time):
        """Remove old predictions that are no longer relevant"""
        cutoff_time = current_time - 3600  # 1 hour
        
        to_remove = [
            pred_id for pred_id, pred_data in self.active_predictions.items()
            if pred_data['start_time'] < cutoff_time
        ]
        
        for pred_id in to_remove:
            del self.active_predictions[pred_id]
            
    def get_accuracy_statistics(self):
        """Get comprehensive accuracy statistics"""
        try:
            if not self.prediction_history:
                return {}
                
            recent_predictions = self.prediction_history[-100:]  # Last 100
            
            stats = {
                'total_predictions': len(self.prediction_history),
                'recent_predictions': len(recent_predictions),
                'direction_accuracy': np.mean([p['direction_accuracy'] for p in recent_predictions]),
                'magnitude_accuracy': np.mean([p['magnitude_accuracy'] for p in recent_predictions]),
                'overall_accuracy': np.mean([p['overall_accuracy'] for p in recent_predictions]),
                'confidence_calibration': self._calculate_confidence_calibration(recent_predictions)
            }
            
            # Accuracy by confidence level
            high_conf_preds = [p for p in recent_predictions if p['confidence'] > 0.7]
            if high_conf_preds:
                stats['high_confidence_accuracy'] = np.mean([p['overall_accuracy'] for p in high_conf_preds])
                
            # Accuracy by symbol
            symbol_accuracy = defaultdict(list)
            for pred in recent_predictions:
                symbol_accuracy[pred['symbol']].append(pred['overall_accuracy'])
                
            stats['symbol_accuracy'] = {
                symbol: np.mean(accuracies) 
                for symbol, accuracies in symbol_accuracy.items()
                if len(accuracies) >= 5
            }
            
            return stats
            
        except Exception as e:
            self.logger.error(f"Accuracy statistics calculation failed: {e}")
            return {}
            
    def _calculate_confidence_calibration(self, predictions):
        """Calculate how well confidence scores match actual accuracy"""
        try:
            if len(predictions) < 20:
                return 0.5
                
            # Group predictions by confidence bins
            confidence_bins = np.linspace(0, 1, 11)  # 10 bins
            bin_calibration = []
            
            for i in range(len(confidence_bins) - 1):
                bin_start, bin_end = confidence_bins[i], confidence_bins[i + 1]
                
                bin_predictions = [
                    p for p in predictions 
                    if bin_start <= p['confidence'] < bin_end
                ]
                
                if len(bin_predictions) >= 5:
                    avg_confidence = np.mean([p['confidence'] for p in bin_predictions])
                    avg_accuracy = np.mean([p['overall_accuracy'] for p in bin_predictions])
                    
                    calibration_error = abs(avg_confidence - avg_accuracy)
                    bin_calibration.append(calibration_error)
                    
            if bin_calibration:
                # Lower error = better calibration
                return 1.0 - np.mean(bin_calibration)
            else:
                return 0.5
                
        except:
            return 0.5
 
def initialize_neural_network_system(config, memory_manager):
    """Initialize the complete neural network prediction system"""
    try:
        # Initialize feature engineer
        feature_engineer = FeatureEngineer(config)
        
        # Initialize neural network predictor
        nn_predictor = NeuralNetworkPredictor(config)
        
        # Initialize prediction validator
        prediction_validator = PredictionValidator(config)
        
        # Try to load existing models
        models_loaded = nn_predictor.load_models()
        
        logger = logging.getLogger('MonsterBot.NeuralNetwork')
        
        if models_loaded:
            logger.info("‚úÖ Pre-trained models loaded successfully")
        else:
            logger.info("‚ö†Ô∏è No pre-trained models found - will need training")
            
        return {
            'feature_engineer': feature_engineer,
            'nn_predictor': nn_predictor,
            'prediction_validator': prediction_validator,
            'models_loaded': models_loaded
        }
        
    except Exception as e:
        logger = logging.getLogger('MonsterBot.NeuralNetwork')
        logger.error(f"Neural network system initialization failed: {e}")
        raise
        
class BaseStrategy:
    """Base class for all trading strategies"""
    
    def __init__(self, config, name):
        self.config = config
        self.name = name
        self.logger = logging.getLogger(f'MonsterBot.Strategy.{name}')
        
        # Strategy performance tracking
        self.trade_history = []
        self.signal_history = []
        self.performance_metrics = defaultdict(list)
        
        # Strategy state
        self.last_signal_time = 0
        self.cooldown_period = 60  # seconds between signals
        self.is_active = True
        
        # Weight and confidence
        self.base_weight = config.STRATEGY_WEIGHTS.get(name.lower(), 1.0)
        self.current_weight = self.base_weight
        self.confidence_threshold = 0.6
        
    def analyze(self, symbol, market_data, order_book, trades, sentiment, nn_prediction):
        """Override this method in strategy implementations"""
        raise NotImplementedError
        
    def calculate_confidence(self, factors):
        """Calculate confidence based on multiple factors"""
        if not factors:
            return 0.0
            
        weighted_sum = 0
        total_weight = 0
        
        for factor_name, factor_data in factors.items():
            if isinstance(factor_data, dict):
                value = factor_data.get('value', 0)
                weight = factor_data.get('weight', 1.0)
            else:
                value = factor_data
                weight = 1.0
                
            weighted_sum += value * weight
            total_weight += weight
            
        confidence = weighted_sum / total_weight if total_weight > 0 else 0
        return min(max(confidence, 0.0), 1.0)
        
    def update_performance(self, trade_result):
        """Update strategy performance metrics"""
        self.trade_history.append(trade_result)
        
        # Keep only recent trades
        if len(self.trade_history) > 100:
            self.trade_history.pop(0)
            
        # Update weight based on performance
        self._update_weight()
        
    def _update_weight(self):
        """Update strategy weight based on recent performance"""
        if len(self.trade_history) < 10:
            return
            
        recent_trades = self.trade_history[-20:]
        win_rate = sum(1 for t in recent_trades if t.get('pnl', 0) > 0) / len(recent_trades)
        avg_return = np.mean([t.get('pnl', 0) for t in recent_trades])
        
        # Adjust weight
        if win_rate > 0.65 and avg_return > 0.02:
            self.current_weight = min(self.base_weight * 1.2, 2.0)
        elif win_rate < 0.35 or avg_return < -0.01:
            self.current_weight = max(self.base_weight * 0.8, 0.3)
        else:
            self.current_weight = self.base_weight
            
    def can_generate_signal(self):
        """Check if strategy can generate a new signal"""
        return (time.time() - self.last_signal_time > self.cooldown_period and 
                self.is_active)
 
class MomentumBreakoutStrategy(BaseStrategy):
    """Catches momentum breakouts early - primary profit strategy"""
    
    def __init__(self, config):
        super().__init__(config, 'MomentumBreakout')
        self.confidence_threshold = 0.65
        
    def analyze(self, symbol, market_data, order_book, trades, sentiment, nn_prediction):
        try:
            if market_data is None or len(market_data) < 50:
                return None
                
            if not self.can_generate_signal():
                return None
                
            df = market_data
            factors = {}
            
            # Neural network boost
            if nn_prediction and nn_prediction.get('trade_signal'):
                nn_confidence = nn_prediction.get('confidence', 0)
                if nn_confidence > 0.7:
                    factors['nn_prediction'] = {'value': nn_confidence, 'weight': 2.0}
                    
            # Price breakout detection
            resistance_20 = df['high'].rolling(20).max().iloc[-1]
            current_price = df['close'].iloc[-1]
            breakout_distance = (current_price - resistance_20) / resistance_20
            
            if breakout_distance > -0.02:  # Within 2% of breakout
                factors['breakout_proximity'] = {
                    'value': 1 - abs(breakout_distance) * 25, 
                    'weight': 1.8
                }
                
            # Volume confirmation
            current_volume = df['volume'].iloc[-1]
            avg_volume = df['volume'].rolling(20).mean().iloc[-1]
            volume_ratio = current_volume / avg_volume
            
            if volume_ratio > 1.5:
                factors['volume_breakout'] = {
                    'value': min(volume_ratio / 3, 1.0), 
                    'weight': 2.0
                }
                
            # RSI momentum (not overbought)
            if len(df) >= 14:
                rsi = self._calculate_rsi(df['close'], 14).iloc[-1]
                if 50 < rsi < 75:
                    factors['rsi_momentum'] = {
                        'value': (rsi - 50) / 25, 
                        'weight': 1.2
                    }
                    
            # MACD confirmation
            exp1 = df['close'].ewm(span=12).mean()
            exp2 = df['close'].ewm(span=26).mean()
            macd = exp1 - exp2
            signal_line = macd.ewm(span=9).mean()
            macd_hist = macd - signal_line
            
            if macd_hist.iloc[-1] > 0 and macd_hist.iloc[-1] > macd_hist.iloc[-2]:
                factors['macd_bullish'] = {'value': 0.8, 'weight': 1.3}
                
            # Moving average alignment
            ema9 = df['close'].ewm(span=9).mean().iloc[-1]
            ema21 = df['close'].ewm(span=21).mean().iloc[-1]
            
            if current_price > ema9 > ema21:
                factors['ma_alignment'] = {'value': 0.9, 'weight': 1.5}
                
            # Order book support
            if order_book and order_book.get('volume_imbalance', 0) > 0.2:
                factors['orderbook_support'] = {'value': 0.7, 'weight': 1.0}
                
            # Recent price acceleration
            price_5 = df['close'].iloc[-5]
            price_momentum = (current_price - price_5) / price_5
            
            if price_momentum > 0.01:
                factors['price_acceleration'] = {
                    'value': min(price_momentum * 50, 1.0), 
                    'weight': 1.4
                }
                
            confidence = self.calculate_confidence(factors)
            
            if confidence > self.confidence_threshold:
                self.last_signal_time = time.time()
                
                # Calculate target based on volatility and momentum
                atr = self._calculate_atr(df, 14).iloc[-1]
                volatility_target = atr * 2
                momentum_target = price_momentum * 5 if price_momentum > 0 else 0.03
                
                target = max(volatility_target, momentum_target, 0.025)  # Min 2.5%
                
                return {
                    'strategy': self.name,
                    'direction': 'long',
                    'confidence': confidence,
                    'target': min(target, 0.15),  # Cap at 15%
                    'stop_loss': atr * 1.5,
                    'factors': factors,
                    'entry_type': 'market',
                    'urgency': 'high' if confidence > 0.8 else 'medium'
                }
                
            return None
            
        except Exception as e:
            self.logger.error(f"Momentum breakout analysis failed: {e}")
            return None
            
    def _calculate_rsi(self, prices, period=14):
        """Calculate RSI"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        return 100 - (100 / (1 + rs))
        
    def _calculate_atr(self, df, period=14):
        """Calculate ATR"""
        high_low = df['high'] - df['low']
        high_close = np.abs(df['high'] - df['close'].shift())
        low_close = np.abs(df['low'] - df['close'].shift())
        ranges = pd.concat([high_low, high_close, low_close], axis=1)
        true_range = np.max(ranges, axis=1)
        return true_range.rolling(period).mean()
 
class VolumeSpikeMomentumStrategy(BaseStrategy):
    """Catches moves based on unusual volume spikes"""
    
    def __init__(self, config):
        super().__init__(config, 'VolumeSpikeMomentum')
        self.confidence_threshold = 0.7
        
    def analyze(self, symbol, market_data, order_book, trades, sentiment, nn_prediction):
        try:
            if market_data is None or len(market_data) < 30:
                return None
                
            if not self.can_generate_signal():
                return None
                
            df = market_data
            factors = {}
            
            # Volume spike detection
            current_volume = df['volume'].iloc[-1]
            volume_ma = df['volume'].rolling(20).mean().iloc[-1]
            volume_ratio = current_volume / volume_ma
            
            if volume_ratio < 2.0:  # Need significant volume spike
                return None
                
            factors['volume_spike'] = {
                'value': min(volume_ratio / 5, 1.0), 
                'weight': 2.5
            }
            
            # Price confirmation
            current_price = df['close'].iloc[-1]
            open_price = df['open'].iloc[-1]
            price_change = (current_price - open_price) / open_price
            
            if abs(price_change) < 0.005:  # Need some price movement
                return None
                
            factors['price_movement'] = {
                'value': min(abs(price_change) * 100, 1.0), 
                'weight': 2.0
            }
            
            # Direction determination
            direction = 'long' if price_change > 0 else 'short'
            
            # Volume trend confirmation
            volume_trend = df['volume'].iloc[-3:].mean() / df['volume'].iloc[-10:-3].mean()
            
            if volume_trend > 1.3:
                factors['volume_trend'] = {
                    'value': min((volume_trend - 1) * 2, 1.0), 
                    'weight': 1.5
                }
                
            # Order book confirmation
            if order_book:
                imbalance = order_book.get('volume_imbalance', 0)
                if (direction == 'long' and imbalance > 0.1) or (direction == 'short' and imbalance < -0.1):
                    factors['orderbook_confirm'] = {'value': 0.8, 'weight': 1.3}
                    
            # Recent trades analysis
            if trades:
                buy_volume = sum(t['quantity'] for t in trades[-50:] if not t['is_buyer_maker'])
                sell_volume = sum(t['quantity'] for t in trades[-50:] if t['is_buyer_maker'])
                
                if buy_volume + sell_volume > 0:
                    trade_ratio = buy_volume / (buy_volume + sell_volume)
                    if (direction == 'long' and trade_ratio > 0.6) or (direction == 'short' and trade_ratio < 0.4):
                        factors['trade_flow'] = {'value': 0.75, 'weight': 1.4}
                        
            # Neural network alignment
            if nn_prediction and nn_prediction.get('direction') == direction:
                factors['nn_alignment'] = {
                    'value': nn_prediction.get('confidence', 0.5), 
                    'weight': 1.5
                }
                
            confidence = self.calculate_confidence(factors)
            
            if confidence > self.confidence_threshold:
                self.last_signal_time = time.time()
                
                # Calculate targets based on volume intensity
                base_target = 0.03
                volume_multiplier = min(volume_ratio / 3, 3.0)
                target = base_target * volume_multiplier
                
                return {
                    'strategy': self.name,
                    'direction': direction,
                    'confidence': confidence,
                    'target': min(target, 0.20),  # Cap at 20%
                    'stop_loss': 0.025,
                    'factors': factors,
                    'entry_type': 'market',
                    'urgency': 'high'
                }
                
            return None
            
        except Exception as e:
            self.logger.error(f"Volume spike analysis failed: {e}")
            return None
 
class WhaleActivityStrategy(BaseStrategy):
    """Follows whale movements and large order activity"""
    
    def __init__(self, config):
        super().__init__(config, 'WhaleActivity')
        self.confidence_threshold = 0.75
        self.large_order_threshold = 500000  # $500k
        
    def analyze(self, symbol, market_data, order_book, trades, sentiment, nn_prediction):
        try:
            if not trades or not order_book:
                return None
                
            if not self.can_generate_signal():
                return None
                
            factors = {}
            
            # Large order detection in trades
            trade_sizes = [t['quantity'] * t['price'] for t in trades]
            large_trades = [t for t, size in zip(trades, trade_sizes) if size > 100000]  # $100k+
            
            if len(large_trades) < 3:
                return None
                
            # Analyze large trade direction
            large_buy_volume = sum(t['quantity'] for t in large_trades if not t['is_buyer_maker'])
            large_sell_volume = sum(t['quantity'] for t in large_trades if t['is_buyer_maker'])
            
            if large_buy_volume > large_sell_volume * 1.5:
                direction = 'long'
                factors['whale_buying'] = {'value': 0.9, 'weight': 2.5}
            elif large_sell_volume > large_buy_volume * 1.5:
                direction = 'short'
                factors['whale_selling'] = {'value': 0.9, 'weight': 2.5}
            else:
                return None
                
            # Order book whale walls
            bid_sizes = [bid[1] for bid in order_book['bids'][:10]]
            ask_sizes = [ask[1] for ask in order_book['asks'][:10]]
            
            avg_bid = np.mean(bid_sizes)
            avg_ask = np.mean(ask_sizes)
            
            large_bids = sum(1 for size in bid_sizes if size > avg_bid * 4)
            large_asks = sum(1 for size in ask_sizes if size > avg_ask * 4)
            
            if direction == 'long' and large_bids > large_asks:
                factors['bid_walls'] = {'value': 0.8, 'weight': 1.8}
            elif direction == 'short' and large_asks > large_bids:
                factors['ask_walls'] = {'value': 0.8, 'weight': 1.8}
                
            # Accumulation/distribution pattern
            if market_data is not None and len(market_data) >= 20:
                df = market_data
                
                # Check for volume without price movement (accumulation)
                recent_volume = df['volume'].iloc[-5:].mean()
                older_volume = df['volume'].iloc[-15:-5].mean()
                
                price_change = abs(df['close'].iloc[-1] - df['close'].iloc[-5]) / df['close'].iloc[-5]
                
                if recent_volume > older_volume * 1.3 and price_change < 0.02:
                    factors['accumulation_pattern'] = {'value': 0.85, 'weight': 2.0}
                    
            # Order flow momentum
            recent_trades = trades[-20:]
            if len(recent_trades) >= 10:
                trade_momentum = 0
                for i in range(1, len(recent_trades)):
                    if not recent_trades[i]['is_buyer_maker']:  # Buy order
                        trade_momentum += 1
                    else:  # Sell order
                        trade_momentum -= 1
                        
                momentum_score = abs(trade_momentum) / len(recent_trades)
                if momentum_score > 0.3:
                    factors['trade_momentum'] = {'value': momentum_score, 'weight': 1.5}
                    
            confidence = self.calculate_confidence(factors)
            
            if confidence > self.confidence_threshold:
                self.last_signal_time = time.time()
                
                # Whale moves tend to be larger
                target = 0.06 + (confidence - 0.75) * 0.2  # 6-10% targets
                
                return {
                    'strategy': self.name,
                    'direction': direction,
                    'confidence': confidence,
                    'target': min(target, 0.25),  # Cap at 25%
                    'stop_loss': 0.035,
                    'factors': factors,
                    'entry_type': 'market',
                    'urgency': 'high'
                }
                
            return None
            
        except Exception as e:
            self.logger.error(f"Whale activity analysis failed: {e}")
            return None
 
class LiquidityGrabStrategy(BaseStrategy):
    """Catches liquidity grabs and stop hunts"""
    
    def __init__(self, config):
        super().__init__(config, 'LiquidityGrab')
        self.confidence_threshold = 0.8
        
    def analyze(self, symbol, market_data, order_book, trades, sentiment, nn_prediction):
        try:
            if market_data is None or len(market_data) < 30:
                return None
                
            if not self.can_generate_signal():
                return None
                
            df = market_data
            factors = {}
            
            # Find key support/resistance levels
            support_20 = df['low'].rolling(20).min().iloc[-1]
            resistance_20 = df['high'].rolling(20).max().iloc[-1]
            current_price = df['close'].iloc[-1]
            
            # Check for liquidity sweep (brief break of key level)
            swept_support = False
            swept_resistance = False
            
            # Look for wicks below support in last 5 candles
            for i in range(-5, 0):
                low = df['low'].iloc[i]
                close = df['close'].iloc[i]
                
                if low < support_20 * 0.995 and close > support_20:
                    swept_support = True
                    factors['support_sweep'] = {'value': 0.9, 'weight': 2.5}
                    break
                    
            # Look for wicks above resistance
            for i in range(-5, 0):
                high = df['high'].iloc[i]
                close = df['close'].iloc[i]
                
                if high > resistance_20 * 1.005 and close < resistance_20:
                    swept_resistance = True
                    factors['resistance_sweep'] = {'value': 0.9, 'weight': 2.5}
                    break
                    
            # Need either support or resistance sweep
            if not (swept_support or swept_resistance):
                return None
                
            # Determine direction based on sweep
            if swept_support and current_price > support_20:
                direction = 'long'
            elif swept_resistance and current_price < resistance_20:
                direction = 'short'
            else:
                return None
                
            # Volume confirmation on the sweep
            recent_volume = df['volume'].iloc[-3:].max()
            avg_volume = df['volume'].rolling(20).mean().iloc[-1]
            
            if recent_volume > avg_volume * 1.5:
                factors['sweep_volume'] = {'value': 0.8, 'weight': 2.0}
                
            # Quick recovery after sweep
            if direction == 'long':
                recovery_strength = (current_price - support_20) / support_20
                if recovery_strength > 0.005:
                    factors['recovery_strength'] = {
                        'value': min(recovery_strength * 100, 1.0), 
                        'weight': 2.2
                    }
            else:
                recovery_strength = (resistance_20 - current_price) / current_price
                if recovery_strength > 0.005:
                    factors['recovery_strength'] = {
                        'value': min(recovery_strength * 100, 1.0), 
                        'weight': 2.2
                    }
                    
            # Order book confirmation
            if order_book:
                imbalance = order_book.get('volume_imbalance', 0)
                if (direction == 'long' and imbalance > 0.3) or (direction == 'short' and imbalance < -0.3):
                    factors['orderbook_confirm'] = {'value': 0.85, 'weight': 1.8}
                    
            # Neural network confirmation
            if nn_prediction and nn_prediction.get('direction') == direction:
                nn_conf = nn_prediction.get('confidence', 0)
                if nn_conf > 0.6:
                    factors['nn_confirm'] = {'value': nn_conf, 'weight': 1.5}
                    
            confidence = self.calculate_confidence(factors)
            
            if confidence > self.confidence_threshold:
                self.last_signal_time = time.time()
                
                # Liquidity grabs often lead to strong moves
                target = 0.05 + confidence * 0.15  # 5-20% targets
                
                return {
                    'strategy': self.name,
                    'direction': direction,
                    'confidence': confidence,
                    'target': min(target, 0.30),  # Cap at 30%
                    'stop_loss': 0.025,
                    'factors': factors,
                    'entry_type': 'market',
                    'urgency': 'very_high'
                }
                
            return None
            
        except Exception as e:
            self.logger.error(f"Liquidity grab analysis failed: {e}")
            return None
 
class NewsReactionStrategy(BaseStrategy):
    """Reacts to news and events in milliseconds"""
    
    def __init__(self, config):
        super().__init__(config, 'NewsReaction')
        self.confidence_threshold = 0.7
        self.news_keywords = {
            'bullish': ['partnership', 'adoption', 'upgrade', 'launch', 'integration', 'positive', 'growth'],
            'bearish': ['hack', 'ban', 'regulation', 'negative', 'lawsuit', 'problem', 'crash']
        }
        
    def analyze(self, symbol, market_data, order_book, trades, sentiment, nn_prediction):
        try:
            if not sentiment:
                return None
                
            if not self.can_generate_signal():
                return None
                
            factors = {}
            
            # Extreme sentiment detection
            if not sentiment.get('is_extreme', False):
                return None
                
            sentiment_score = sentiment.get('score', 0)
            sentiment_confidence = sentiment.get('confidence', 0)
            
            if abs(sentiment_score) < 0.6 or sentiment_confidence < 0.5:
                return None
                
            # Direction from sentiment
            direction = 'long' if sentiment_score > 0 else 'short'
            
            factors['sentiment_extreme'] = {
                'value': min(abs(sentiment_score), 1.0), 
                'weight': 2.5
            }
            
            # Volume spike confirmation (news should drive volume)
            if market_data is not None and len(market_data) >= 5:
                df = market_data
                current_volume = df['volume'].iloc[-1]
                avg_volume = df['volume'].rolling(10).mean().iloc[-1]
                volume_ratio = current_volume / avg_volume
                
                if volume_ratio > 2.0:
                    factors['news_volume'] = {
                        'value': min(volume_ratio / 5, 1.0), 
                        'weight': 2.0
                    }
                    
            # Price movement confirmation
            if market_data is not None:
                df = market_data
                recent_change = (df['close'].iloc[-1] - df['close'].iloc[-3]) / df['close'].iloc[-3]
                
                if (direction == 'long' and recent_change > 0.01) or (direction == 'short' and recent_change < -0.01):
                    factors['price_confirmation'] = {
                        'value': min(abs(recent_change) * 50, 1.0), 
                        'weight': 1.8
                    }
                    
            # Multiple news sources
            sources = sentiment.get('sources', [])
            if len(sources) >= 2:
                factors['multiple_sources'] = {'value': 0.8, 'weight': 1.5}
                
            # Time sensitivity (fresher news = better)
            # This would check news timestamp vs current time
            factors['news_freshness'] = {'value': 0.9, 'weight': 1.3}
            
            confidence = self.calculate_confidence(factors)
            
            if confidence > self.confidence_threshold:
                self.last_signal_time = time.time()
                
                # News reactions can be very strong
                base_target = 0.04
                sentiment_multiplier = min(abs(sentiment_score) * 2, 3.0)
                target = base_target * sentiment_multiplier
                
                return {
                    'strategy': self.name,
                    'direction': direction,
                    'confidence': confidence,
                    'target': min(target, 0.25),  # Cap at 25%
                    'stop_loss': 0.03,
                    'factors': factors,
                    'entry_type': 'market',
                    'urgency': 'very_high'
                }
                
            return None
            
        except Exception as e:
            self.logger.error(f"News reaction analysis failed: {e}")
            return None
 
class AccumulationBreakoutStrategy(BaseStrategy):
    """Detects accumulation phases before major breakouts"""
    
    def __init__(self, config):
        super().__init__(config, 'AccumulationBreakout')
        self.confidence_threshold = 0.75
        
    def analyze(self, symbol, market_data, order_book, trades, sentiment, nn_prediction):
        try:
            if market_data is None or len(market_data) < 50:
                return None
                
            if not self.can_generate_signal():
                return None
                
            df = market_data
            factors = {}
            
            # Detect accumulation pattern
            # 1. Decreasing volatility
            recent_vol = df['close'].pct_change().iloc[-10:].std()
            older_vol = df['close'].pct_change().iloc[-30:-10].std()
            
            if recent_vol >= older_vol * 0.8:  # Need decreasing volatility
                return None
                
            factors['volatility_compression'] = {
                'value': 1 - (recent_vol / older_vol), 
                'weight': 2.0
            }
            
            # 2. Increasing volume trend
            recent_volume = df['volume'].iloc[-10:].mean()
            older_volume = df['volume'].iloc[-30:-10].mean()
            
            if recent_volume <= older_volume * 1.1:
                return None
                
            factors['volume_accumulation'] = {
                'value': min((recent_volume / older_volume - 1) * 5, 1.0), 
                'weight': 2.2
            }
            
            # 3. Price stability (trading in range)
            price_high = df['high'].iloc[-20:].max()
            price_low = df['low'].iloc[-20:].min()
            price_range = (price_high - price_low) / df['close'].iloc[-1]
            
            if price_range > 0.08:  # Range too wide
                return None
                
            factors['price_stability'] = {
                'value': 1 - (price_range * 10), 
                'weight': 1.8
            }
            
            # 4. Higher lows formation
            recent_lows = []
            for i in range(-20, -1):
                if (df['low'].iloc[i] < df['low'].iloc[i-1] and 
                    df['low'].iloc[i] < df['low'].iloc[i+1]):
                    recent_lows.append(df['low'].iloc[i])
                    
            if len(recent_lows) >= 2:
                higher_lows = all(recent_lows[i] > recent_lows[i-1] for i in range(1, len(recent_lows)))
                if higher_lows:
                    factors['higher_lows'] = {'value': 0.9, 'weight': 1.8}
                    
            # 5. Approaching resistance
            resistance = df['high'].rolling(30).max().iloc[-1]
            current_price = df['close'].iloc[-1]
            resistance_distance = (resistance - current_price) / current_price
            
            if resistance_distance < 0.05:  # Within 5% of resistance
                factors['resistance_approach'] = {
                    'value': 1 - (resistance_distance * 20), 
                    'weight': 2.0
                }
                
            # 6. Order book support for breakout
            if order_book and order_book.get('volume_imbalance', 0) > 0.2:
                factors['orderbook_support'] = {'value': 0.8, 'weight': 1.5}
                
            # 7. Neural network prediction alignment
            if nn_prediction and nn_prediction.get('direction') == 'long':
                nn_conf = nn_prediction.get('confidence', 0)
                factors['nn_alignment'] = {'value': nn_conf, 'weight': 1.5}
                
            confidence = self.calculate_confidence(factors)
            
            if confidence > self.confidence_threshold:
                self.last_signal_time = time.time()
                
                # Accumulation breakouts can be explosive
                target = 0.08 + confidence * 0.15  # 8-23% targets
                
                return {
                    'strategy': self.name,
                    'direction': 'long',
                    'confidence': confidence,
                    'target': min(target, 0.35),  # Cap at 35%
                    'stop_loss': 0.04,
                    'factors': factors,
                    'entry_type': 'market',
                    'urgency': 'high'
                }
                
            return None
            
        except Exception as e:
            self.logger.error(f"Accumulation breakout analysis failed: {e}")
            return None
 
class BollingerSqueezeStrategy(BaseStrategy):
    """Catches explosive moves after Bollinger Band squeezes"""
    
    def __init__(self, config):
        super().__init__(config, 'BollingerSqueeze')
        self.confidence_threshold = 0.7
        
    def analyze(self, symbol, market_data, order_book, trades, sentiment, nn_prediction):
        try:
            if market_data is None or len(market_data) < 50:
                return None
                
            if not self.can_generate_signal():
                return None
                
            df = market_data
            factors = {}
            
            # Calculate Bollinger Bands
            bb_period = 20
            bb_std = 2
            bb_middle = df['close'].rolling(bb_period).mean()
            bb_std_dev = df['close'].rolling(bb_period).std()
            bb_upper = bb_middle + (bb_std_dev * bb_std)
            bb_lower = bb_middle - (bb_std_dev * bb_std)
            
            # Calculate band width
            bb_width = (bb_upper - bb_lower) / bb_middle
            current_width = bb_width.iloc[-1]
            avg_width = bb_width.rolling(50).mean().iloc[-1]
            
            # Squeeze detection (bands narrowing)
            if current_width >= avg_width * 0.6:  # Need significant squeeze
                return None
                
            factors['bb_squeeze'] = {
                'value': 1 - (current_width / avg_width), 
                'weight': 2.5
            }
            
            # Price near middle band (coiling)
            current_price = df['close'].iloc[-1]
            middle_band = bb_middle.iloc[-1]
            distance_from_middle = abs(current_price - middle_band) / middle_band
            
            if distance_from_middle < 0.01:  # Within 1% of middle
                factors['middle_coiling'] = {'value': 0.9, 'weight': 1.8}
                
            # Volume building during squeeze
            squeeze_volume = df['volume'].iloc[-10:].mean()
            normal_volume = df['volume'].iloc[-30:-10].mean()
            
            if squeeze_volume > normal_volume * 1.2:
                factors['volume_building'] = {
                    'value': min((squeeze_volume / normal_volume - 1) * 3, 1.0), 
                    'weight': 2.0
                }
                
            # Direction hints
            direction_score = 0
            
            # RSI momentum
            if len(df) >= 14:
                rsi = self._calculate_rsi(df['close'], 14).iloc[-1]
                if rsi > 55:
                    direction_score += 1
                    direction = 'long'
                elif rsi < 45:
                    direction_score += 1
                    direction = 'short'
                else:
                    direction = 'long'  # Default bias
                    
            # MACD direction
            exp1 = df['close'].ewm(span=12).mean()
            exp2 = df['close'].ewm(span=26).mean()
            macd = exp1 - exp2
            signal_line = macd.ewm(span=9).mean()
            
            if macd.iloc[-1] > signal_line.iloc[-1]:
                direction_score += 1
                direction = 'long'
            else:
                direction_score += 1
                direction = 'short'
                
            # Price action bias
            recent_high = df['high'].iloc[-5:].max()
            recent_low = df['low'].iloc[-5:].min()
            
            if current_price > (recent_high + recent_low) / 2:
                direction_score += 1
                direction = 'long'
            else:
                direction_score += 1
                direction = 'short'
                
            # Need clear directional bias
            if direction_score < 2:
                return None
                
            factors['direction_clarity'] = {
                'value': direction_score / 3, 
                'weight': 1.5
            }
            
            # Neural network confirmation
            if nn_prediction and nn_prediction.get('direction') == direction:
                factors['nn_confirm'] = {
                    'value': nn_prediction.get('confidence', 0.5), 
                    'weight': 1.8
                }
                
            # Order book bias
            if order_book:
                imbalance = order_book.get('volume_imbalance', 0)
                if (direction == 'long' and imbalance > 0.1) or (direction == 'short' and imbalance < -0.1):
                    factors['orderbook_bias'] = {'value': 0.7, 'weight': 1.3}
                    
            confidence = self.calculate_confidence(factors)
            
            if confidence > self.confidence_threshold:
                self.last_signal_time = time.time()
                
                # Squeeze breakouts are often explosive
                squeeze_intensity = 1 - (current_width / avg_width)
                target = 0.06 + squeeze_intensity * 0.2  # 6-26% targets
                
                return {
                    'strategy': self.name,
                    'direction': direction,
                    'confidence': confidence,
                    'target': min(target, 0.30),  # Cap at 30%
                    'stop_loss': 0.03,
                    'factors': factors,
                    'entry_type': 'market',
                    'urgency': 'high'
                }
                
            return None
            
        except Exception as e:
            self.logger.error(f"Bollinger squeeze analysis failed: {e}")
            return None
            
    def _calculate_rsi(self, prices, period=14):
        """Calculate RSI"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        return 100 - (100 / (1 + rs))
 
class SmartMoneyDivergenceStrategy(BaseStrategy):
    """Detects smart money vs retail divergences"""
    
    def __init__(self, config):
        super().__init__(config, 'SmartMoneyDivergence')
        self.confidence_threshold = 0.8
        
    def analyze(self, symbol, market_data, order_book, trades, sentiment, nn_prediction):
        try:
            if not trades or market_data is None or len(market_data) < 20:
                return None
                
            if not self.can_generate_signal():
                return None
                
            df = market_data
            factors = {}
            
            # Analyze trade sizes to separate smart money from retail
            trade_sizes = [t['quantity'] * t['price'] for t in trades]
            avg_trade_size = np.mean(trade_sizes)
            
            # Classify trades
            smart_money_trades = [t for t, size in zip(trades, trade_sizes) if size > avg_trade_size * 3]
            retail_trades = [t for t, size in zip(trades, trade_sizes) if size <= avg_trade_size]
            
            if len(smart_money_trades) < 5 or len(retail_trades) < 10:
                return None
                
            # Smart money flow
            smart_buy_volume = sum(t['quantity'] for t in smart_money_trades if not t['is_buyer_maker'])
            smart_sell_volume = sum(t['quantity'] for t in smart_money_trades if t['is_buyer_maker'])
            
            # Retail flow
            retail_buy_volume = sum(t['quantity'] for t in retail_trades if not t['is_buyer_maker'])
            retail_sell_volume = sum(t['quantity'] for t in retail_trades if t['is_buyer_maker'])
            
            # Calculate flow ratios
            smart_ratio = smart_buy_volume / (smart_buy_volume + smart_sell_volume) if (smart_buy_volume + smart_sell_volume) > 0 else 0.5
            retail_ratio = retail_buy_volume / (retail_buy_volume + retail_sell_volume) if (retail_buy_volume + retail_sell_volume) > 0 else 0.5
            
            # Divergence detection
            divergence_strength = abs(smart_ratio - retail_ratio)
            
            if divergence_strength < 0.3:  # Need significant divergence
                return None
                
            factors['divergence_strength'] = {
                'value': min(divergence_strength * 2, 1.0), 
                'weight': 2.5
            }
            
            # Direction from smart money
            if smart_ratio > 0.6:
                direction = 'long'
                factors['smart_money_buying'] = {'value': smart_ratio, 'weight': 2.2}
            elif smart_ratio < 0.4:
                direction = 'short'
                factors['smart_money_selling'] = {'value': 1 - smart_ratio, 'weight': 2.2}
            else:
                return None
                
            # Retail sentiment opposite
            if (direction == 'long' and retail_ratio < 0.4) or (direction == 'short' and retail_ratio > 0.6):
                factors['retail_opposite'] = {'value': 0.9, 'weight': 2.0}
                
            # Price action confirmation
            recent_price_change = (df['close'].iloc[-1] - df['close'].iloc[-5]) / df['close'].iloc[-5]
            
            if (direction == 'long' and recent_price_change > 0.005) or (direction == 'short' and recent_price_change < -0.005):
                factors['price_confirmation'] = {
                    'value': min(abs(recent_price_change) * 100, 1.0), 
                    'weight': 1.8
                }
                
            # Volume intensity
            total_smart_volume = smart_buy_volume + smart_sell_volume
            total_retail_volume = retail_buy_volume + retail_sell_volume
            
            smart_dominance = total_smart_volume / (total_smart_volume + total_retail_volume)
            
            if smart_dominance > 0.3:  # Smart money makes up significant portion
                factors['smart_dominance'] = {'value': smart_dominance, 'weight': 1.5}
                
            # Order book alignment
            if order_book:
                imbalance = order_book.get('volume_imbalance', 0)
                if (direction == 'long' and imbalance > 0.2) or (direction == 'short' and imbalance < -0.2):
                    factors['orderbook_alignment'] = {'value': 0.8, 'weight': 1.4}
                    
            confidence = self.calculate_confidence(factors)
            
            if confidence > self.confidence_threshold:
                self.last_signal_time = time.time()
                
                # Smart money moves can be significant
                target = 0.05 + divergence_strength * 0.25  # 5-30% targets
                
                return {
                    'strategy': self.name,
                    'direction': direction,
                    'confidence': confidence,
                    'target': min(target, 0.35),  # Cap at 35%
                    'stop_loss': 0.035,
                    'factors': factors,
                    'entry_type': 'market',
                    'urgency': 'high'
                }
                
            return None
            
        except Exception as e:
            self.logger.error(f"Smart money divergence analysis failed: {e}")
            return None
 
class MultiStrategyOrchestrator:
    """Orchestrates all trading strategies and combines signals"""
    
    def __init__(self, config, neural_network_system):
        self.config = config
        self.nn_system = neural_network_system
        self.logger = logging.getLogger('MonsterBot.MultiStrategy')
        
        # Initialize all strategies
        self.strategies = {
            'momentum_breakout': MomentumBreakoutStrategy(config),
            'volume_spike': VolumeSpikeMomentumStrategy(config),
            'whale_activity': WhaleActivityStrategy(config),
            'liquidity_grab': LiquidityGrabStrategy(config),
            'news_reaction': NewsReactionStrategy(config),
            'accumulation_breakout': AccumulationBreakoutStrategy(config),
            'bollinger_squeeze': BollingerSqueezeStrategy(config),
            'smart_money': SmartMoneyDivergenceStrategy(config)
        }
        
        # Strategy performance tracking
        self.strategy_performance = defaultdict(list)
        self.signal_history = []
        self.execution_stats = defaultdict(int)
        
        # Signal combination weights
        self.combination_weights = {
            'momentum_breakout': 1.2,
            'volume_spike': 1.1,
            'whale_activity': 1.5,
            'liquidity_grab': 1.4,
            'news_reaction': 1.6,
            'accumulation_breakout': 1.3,
            'bollinger_squeeze': 1.2,
            'smart_money': 1.4
        }
        
    def analyze_symbol(self, symbol, market_data, order_book, trades, sentiment):
        """Run all strategies on a symbol and combine signals"""
        try:
            start_time = time.time()
            
            # Get neural network prediction
            nn_prediction = None
            if self.nn_system['nn_predictor']:
                nn_prediction = self.nn_system['nn_predictor'].predict(
                    symbol, market_data, order_book, trades, sentiment
                )
                
            # Collect signals from all strategies
            all_signals = {}
            
            for strategy_name, strategy in self.strategies.items():
                if not strategy.is_active:
                    continue
                    
                try:
                    signal = strategy.analyze(
                        symbol, market_data, order_book, trades, sentiment, nn_prediction
                    )
                    
                    if signal and signal.get('confidence', 0) > strategy.confidence_threshold:
                        all_signals[strategy_name] = signal
                        self.execution_stats[f'{strategy_name}_signals'] += 1
                        
                except Exception as e:
                    self.logger.error(f"Strategy {strategy_name} failed for {symbol}: {e}")
                    continue
                    
            # Combine signals if we have any
            combined_signal = None
            if all_signals:
                combined_signal = self._combine_signals(symbol, all_signals, nn_prediction)
                
            analysis_time = time.time() - start_time
            
            if combined_signal:
                self.logger.info(
                    f"üéØ {symbol}: Combined signal from {len(all_signals)} strategies "
                    f"(confidence: {combined_signal['confidence']:.2f}) in {analysis_time:.3f}s"
                )
                
                # Store signal history
                self.signal_history.append({
                    'symbol': symbol,
                    'timestamp': time.time(),
                    'strategies': list(all_signals.keys()),
                    'combined_signal': combined_signal,
                    'analysis_time': analysis_time
                })
                
                # Keep history manageable
                if len(self.signal_history) > 500:
                    self.signal_history.pop(0)
                    
            return combined_signal
            
        except Exception as e:
            self.logger.error(f"Multi-strategy analysis failed for {symbol}: {e}")
            return None
            
    def _combine_signals(self, symbol, all_signals, nn_prediction):
        """Combine multiple strategy signals into final trading decision"""
        try:
            # Separate by direction
            long_signals = {k: v for k, v in all_signals.items() if v['direction'] == 'long'}
            short_signals = {k: v for k, v in all_signals.items() if v['direction'] == 'short'}
            
            # Calculate weighted scores for each direction
            long_score = self._calculate_direction_score(long_signals)
            short_score = self._calculate_direction_score(short_signals)
            
            # Determine final direction
            if long_score > short_score * 1.2:  # Bias towards long
                final_direction = 'long'
                active_signals = long_signals
                direction_score = long_score
            elif short_score > long_score:
                final_direction = 'short'
                active_signals = short_signals
                direction_score = short_score
            else:
                return None  # No clear direction
                
            # Calculate combined confidence
            base_confidence = direction_score / len(active_signals) if active_signals else 0
            
            # Neural network boost
            nn_boost = 0
            if nn_prediction and nn_prediction.get('direction') == final_direction:
                nn_confidence = nn_prediction.get('confidence', 0)
                nn_boost = nn_confidence * 0.15  # Up to 15% boost
                
            # Strategy consensus boost
            consensus_boost = 0
            if len(active_signals) >= 3:
                consensus_boost = 0.1  # 10% boost for multiple strategies
            elif len(active_signals) >= 2:
                consensus_boost = 0.05  # 5% boost for two strategies
                
            # Final confidence
            final_confidence = min(base_confidence + nn_boost + consensus_boost, 0.95)
            
            # Calculate position sizing
            position_size = self._calculate_position_size(final_confidence, len(active_signals))
            
            # Calculate targets and stops
            targets = self._calculate_combined_targets(active_signals, final_confidence)
            
            # Determine urgency
            urgency_scores = [s.get('urgency', 'medium') for s in active_signals.values()]
            if 'very_high' in urgency_scores or len([u for u in urgency_scores if u == 'high']) >= 2:
                urgency = 'very_high'
            elif 'high' in urgency_scores:
                urgency = 'high'
            else:
                urgency = 'medium'
                
            return {
                'symbol': symbol,
                'direction': final_direction,
                'confidence': final_confidence,
                'position_size': position_size,
                'targets': targets,
                'strategies': list(active_signals.keys()),
                'strategy_count': len(active_signals),
                'urgency': urgency,
                'entry_type': 'market',
                'nn_prediction': nn_prediction,
                'timestamp': time.time()
            }
            
        except Exception as e:
            self.logger.error(f"Signal combination failed: {e}")
            return None
            
    def _calculate_direction_score(self, signals):
        """Calculate weighted score for a direction"""
        total_score = 0
        
        for strategy_name, signal in signals.items():
            strategy_weight = self.combination_weights.get(strategy_name, 1.0)
            strategy_confidence = signal.get('confidence', 0)
            current_weight = self.strategies[strategy_name].current_weight
            
            weighted_score = strategy_confidence * strategy_weight * current_weight
            total_score += weighted_score
            
        return total_score
        
    def _calculate_position_size(self, confidence, strategy_count):
        """Calculate position size based on confidence and strategy consensus"""
        # Base size from confidence
        base_size = confidence * 0.6  # Up to 60% from confidence
        
        # Consensus multiplier
        consensus_multiplier = 1.0 + (strategy_count - 1) * 0.1  # 10% per additional strategy
        consensus_multiplier = min(consensus_multiplier, 1.5)  # Cap at 150%
        
        # Final position size
        position_size = base_size * consensus_multiplier
        
        # Apply personality mode limits
        current_capital = 1000  # This would be actual capital
        mode_name, mode_config = get_personality_mode(current_capital, self.config)
        max_position = mode_config['max_position_size']
        
        return min(position_size, max_position)
        
    def _calculate_combined_targets(self, signals, confidence):
        """Calculate combined target and stop levels"""
        try:
            # Collect all targets and stops
            targets = [s.get('target', 0.05) for s in signals.values()]
            stops = [s.get('stop_loss', 0.03) for s in signals.values()]
            
            # Calculate weighted averages
            median_target = np.median(targets)
            median_stop = np.median(stops)
            
            # Confidence adjustment
            confidence_multiplier = 0.8 + (confidence * 0.4)  # 0.8x to 1.2x
            
            final_target = median_target * confidence_multiplier
            final_stop = median_stop
            
            # Multiple take profit levels
            return {
                'tp1': {'level': final_target * 0.4, 'size': 0.25},  # 25% at 40% of target
                'tp2': {'level': final_target * 0.7, 'size': 0.35},  # 35% at 70% of target
                'tp3': {'level': final_target * 1.0, 'size': 0.25},  # 25% at full target
                'trail': {'level': final_target * 1.5, 'size': 0.15},  # 15% trailing
                'stop_loss': final_stop
            }
            
        except Exception as e:
            self.logger.error(f"Target calculation failed: {e}")
            return {
                'tp1': {'level': 0.03, 'size': 0.3},
                'tp2': {'level': 0.05, 'size': 0.4},
                'tp3': {'level': 0.08, 'size': 0.3},
                'stop_loss': 0.025
            }
            
    def update_strategy_performance(self, strategy_name, trade_result):
        """Update individual strategy performance"""
        if strategy_name in self.strategies:
            self.strategies[strategy_name].update_performance(trade_result)
            
    def get_strategy_rankings(self):
        """Get strategy performance rankings"""
        rankings = []
        
        for strategy_name, strategy in self.strategies.items():
            if len(strategy.trade_history) >= 5:
                recent_trades = strategy.trade_history[-20:]
                
                win_rate = sum(1 for t in recent_trades if t.get('pnl', 0) > 0) / len(recent_trades)
                avg_return = np.mean([t.get('pnl', 0) for t in recent_trades])
                total_return = sum([t.get('pnl', 0) for t in recent_trades])
                
                rankings.append({
                    'strategy': strategy_name,
                    'trades': len(strategy.trade_history),
                    'recent_trades': len(recent_trades),
                    'win_rate': win_rate,
                    'avg_return': avg_return,
                    'total_return': total_return,
                    'current_weight': strategy.current_weight,
                    'is_active': strategy.is_active
                })
                
        # Sort by total return
        rankings.sort(key=lambda x: x['total_return'], reverse=True)
        
        return rankings
        
    def get_execution_statistics(self):
        """Get execution statistics"""
        total_signals = sum(self.execution_stats.values())
        
        stats = {
            'total_signals_generated': total_signals,
            'signals_by_strategy': dict(self.execution_stats),
            'recent_signals': len(self.signal_history),
            'avg_strategies_per_signal': np.mean([s['strategy_count'] for s in self.signal_history]) if self.signal_history else 0,
            'avg_analysis_time': np.mean([s['analysis_time'] for s in self.signal_history]) if self.signal_history else 0
        }
        
        return stats
 
def initialize_multi_strategy_system(config, neural_network_system):
    """Initialize the complete multi-strategy system"""
    try:
        # Initialize orchestrator
        orchestrator = MultiStrategyOrchestrator(config, neural_network_system)
        
        logger = logging.getLogger('MonsterBot.MultiStrategy')
        logger.info(f"‚úÖ Multi-strategy system initialized with {len(orchestrator.strategies)} strategies")
        
        # Log strategy details
        for strategy_name, strategy in orchestrator.strategies.items():
            logger.info(f"   üìà {strategy_name}: threshold={strategy.confidence_threshold:.2f}, weight={strategy.current_weight:.2f}")
            
        return {
            'orchestrator': orchestrator,
            'strategies': orchestrator.strategies,
            'strategy_count': len(orchestrator.strategies)
        }
        
    except Exception as e:
        logger = logging.getLogger('MonsterBot.MultiStrategy')
        logger.error(f"Multi-strategy system initialization failed: {e}")
        raise                
        
class SentimentAnalysisEngine:
    """Real-time sentiment analysis from multiple sources"""
    
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger('MonsterBot.Sentiment')
        
        # API clients (initialized later)
        self.twitter_api = None
        self.reddit_api = None
        
        # Sentiment data storage
        self.sentiment_cache = {}
        self.sentiment_history = defaultdict(list)
        self.trending_topics = defaultdict(int)
        
        # Keywords for sentiment analysis
        self.bullish_keywords = [
            'moon', 'rocket', 'pump', 'bullish', 'buy', 'long', 'bull', 'green',
            'up', 'gains', 'profit', 'breakout', 'surge', 'rally', 'explosion',
            'partnership', 'adoption', 'upgrade', 'positive', 'growth', 'launch'
        ]
        
        self.bearish_keywords = [
            'dump', 'crash', 'bearish', 'sell', 'short', 'bear', 'red',
            'down', 'loss', 'decline', 'drop', 'correction', 'dip',
            'hack', 'ban', 'regulation', 'negative', 'lawsuit', 'problem'
        ]
        
        # Update intervals
        self.update_intervals = {
            'twitter': 120,     # 2 minutes
            'reddit': 300,      # 5 minutes  
            'news': 180,        # 3 minutes
            'telegram': 240     # 4 minutes
        }
        
        self.last_updates = {}
        
        # Performance tracking
        self.analysis_stats = defaultdict(int)
        
        self.initialize_apis()
        
    def initialize_apis(self):
        """Initialize social media APIs if available"""
        try:
            if HAS_SENTIMENT:
                self._setup_twitter_api()
                self._setup_reddit_api()
                self.logger.info("üì± Sentiment APIs initialized")
            else:
                self.logger.warning("Sentiment libraries not available - using fallback analysis")
                
        except Exception as e:
            self.logger.error(f"API initialization failed: {e}")
            
    def _setup_twitter_api(self):
        """Setup Twitter API v2"""
        try:
            if hasattr(self.config, 'TWITTER_BEARER_TOKEN'):
                import tweepy
                
                self.twitter_api = tweepy.Client(
                    bearer_token=self.config.TWITTER_BEARER_TOKEN,
                    wait_on_rate_limit=True
                )
                
                # Test connection
                me = self.twitter_api.get_me()
                self.logger.info(f"‚úÖ Twitter API connected")
                
        except Exception as e:
            self.logger.debug(f"Twitter API setup failed: {e}")
            
    def _setup_reddit_api(self):
        """Setup Reddit API"""
        try:
            if hasattr(self.config, 'REDDIT_CLIENT_ID'):
                import praw
                
                self.reddit_api = praw.Reddit(
                    client_id=self.config.REDDIT_CLIENT_ID,
                    client_secret=self.config.REDDIT_CLIENT_SECRET,
                    user_agent='MonsterBot/1.0'
                )
                
                self.logger.info("‚úÖ Reddit API connected")
                
        except Exception as e:
            self.logger.debug(f"Reddit API setup failed: {e}")
            
    def analyze_symbol_sentiment(self, symbol):
        """Comprehensive sentiment analysis for a symbol"""
        try:
            base_symbol = symbol.replace('USDT', '').replace('BUSD', '')
            
            # Check cache first
            cache_key = f'sentiment_{symbol}'
            cached_data = self._get_cached_sentiment(cache_key)
            
            if cached_data:
                return cached_data
                
            sentiment_sources = {}
            
            # Twitter sentiment
            if self._should_update('twitter'):
                twitter_data = self._analyze_twitter_sentiment(base_symbol)
                if twitter_data:
                    sentiment_sources['twitter'] = twitter_data
                    
            # Reddit sentiment
            if self._should_update('reddit'):
                reddit_data = self._analyze_reddit_sentiment(base_symbol)
                if reddit_data:
                    sentiment_sources['reddit'] = reddit_data
                    
            # News sentiment
            if self._should_update('news'):
                news_data = self._analyze_news_sentiment(base_symbol)
                if news_data:
                    sentiment_sources['news'] = news_data
                    
            # Telegram sentiment (simulated for now)
            if self._should_update('telegram'):
                telegram_data = self._analyze_telegram_sentiment(base_symbol)
                if telegram_data:
                    sentiment_sources['telegram'] = telegram_data
                    
            # Combine all sentiment data
            combined_sentiment = self._combine_sentiment_data(sentiment_sources)
            
            if combined_sentiment:
                # Cache result
                self._cache_sentiment(cache_key, combined_sentiment)
                
                # Store in history
                self._store_sentiment_history(symbol, combined_sentiment)
                
                # Update trending topics
                self._update_trending_topics(base_symbol, combined_sentiment)
                
            return combined_sentiment
            
        except Exception as e:
            self.logger.error(f"Sentiment analysis failed for {symbol}: {e}")
            return None
            
    def _should_update(self, source):
        """Check if source should be updated"""
        last_update = self.last_updates.get(source, 0)
        interval = self.update_intervals.get(source, 300)
        return time.time() - last_update > interval
        
    def _get_cached_sentiment(self, cache_key):
        """Get cached sentiment if still valid"""
        if cache_key in self.sentiment_cache:
            cached_data = self.sentiment_cache[cache_key]
            if time.time() - cached_data['timestamp'] < 180:  # 3 minutes
                return cached_data['sentiment']
        return None
        
    def _analyze_twitter_sentiment(self, symbol):
        """Analyze Twitter sentiment"""
        try:
            if not self.twitter_api:
                return self._fallback_sentiment_analysis(symbol, 'twitter')
                
            # Search for recent tweets
            query = f"${symbol} OR #{symbol} OR {symbol.lower()}"
            tweets = self.twitter_api.search_recent_tweets(
                query=query,
                max_results=100,
                tweet_fields=['created_at', 'public_metrics', 'context_annotations', 'author_id']
            )
            
            if not tweets.data:
                return None
                
            sentiment_scores = []
            engagement_weights = []
            keyword_counts = defaultdict(int)
            
            for tweet in tweets.data:
                # Clean tweet text
                text = self._clean_text(tweet.text)
                
                # Calculate sentiment score
                sentiment_score = self._calculate_text_sentiment(text)
                
                # Get engagement weight
                metrics = tweet.public_metrics
                engagement = (metrics['like_count'] + 
                             metrics['retweet_count'] + 
                             metrics['reply_count'] + 
                             metrics['quote_count'])
                
                weight = min(1 + np.log10(engagement + 1), 10)  # Log scale, cap at 10
                
                sentiment_scores.append(sentiment_score)
                engagement_weights.append(weight)
                
                # Count keywords
                self._count_keywords(text, keyword_counts)
                
            # Calculate weighted sentiment
            if sentiment_scores:
                weighted_sentiment = np.average(sentiment_scores, weights=engagement_weights)
                
                # Calculate confidence
                confidence = min(len(sentiment_scores) / 50, 1.0)  # Max confidence at 50+ tweets
                
                # Detect sentiment extremes
                extreme_threshold = 0.6
                is_extreme = abs(weighted_sentiment) > extreme_threshold
                
                self.last_updates['twitter'] = time.time()
                self.analysis_stats['twitter_analyses'] += 1
                
                return {
                    'source': 'twitter',
                    'sentiment_score': weighted_sentiment,
                    'confidence': confidence,
                    'volume': len(sentiment_scores),
                    'total_engagement': sum(engagement_weights),
                    'is_extreme': is_extreme,
                    'keywords': dict(keyword_counts),
                    'timestamp': time.time()
                }
                
            return None
            
        except Exception as e:
            self.logger.error(f"Twitter sentiment analysis failed: {e}")
            return self._fallback_sentiment_analysis(symbol, 'twitter')
            
    def _analyze_reddit_sentiment(self, symbol):
        """Analyze Reddit sentiment"""
        try:
            if not self.reddit_api:
                return self._fallback_sentiment_analysis(symbol, 'reddit')
                
            sentiment_data = []
            subreddits = ['cryptocurrency', 'cryptomoonshots', 'satoshistreetbets', 'bitcoin', 'ethereum']
            
            for subreddit_name in subreddits:
                try:
                    subreddit = self.reddit_api.subreddit(subreddit_name)
                    
                    # Search posts
                    posts = list(subreddit.search(symbol, time_filter='day', limit=10))
                    
                    for post in posts:
                        # Analyze post
                        post_text = f"{post.title} {post.selftext}".lower()
                        post_sentiment = self._calculate_text_sentiment(post_text)
                        
                        # Weight by upvotes and comments
                        engagement = post.score + post.num_comments
                        weight = min(1 + np.log10(engagement + 1), 5)
                        
                        sentiment_data.append({
                            'sentiment': post_sentiment,
                            'weight': weight,
                            'subreddit': subreddit_name
                        })
                        
                        # Analyze top comments
                        post.comments.replace_more(limit=0)
                        for comment in post.comments[:5]:
                            if hasattr(comment, 'body') and len(comment.body) > 10:
                                comment_sentiment = self._calculate_text_sentiment(comment.body.lower())
                                comment_weight = min(comment.score / 10, 2)
                                
                                sentiment_data.append({
                                    'sentiment': comment_sentiment,
                                    'weight': max(comment_weight, 0.1),
                                    'subreddit': f'{subreddit_name}_comments'
                                })
                                
                except Exception as e:
                    self.logger.debug(f"Failed to analyze subreddit {subreddit_name}: {e}")
                    continue
                    
            if sentiment_data:
                # Calculate weighted sentiment
                sentiments = [d['sentiment'] for d in sentiment_data]
                weights = [d['weight'] for d in sentiment_data]
                
                weighted_sentiment = np.average(sentiments, weights=weights)
                confidence = min(len(sentiment_data) / 20, 1.0)
                
                self.last_updates['reddit'] = time.time()
                self.analysis_stats['reddit_analyses'] += 1
                
                return {
                    'source': 'reddit',
                    'sentiment_score': weighted_sentiment,
                    'confidence': confidence,
                    'volume': len(sentiment_data),
                    'subreddits': len(set(d['subreddit'] for d in sentiment_data)),
                    'is_extreme': abs(weighted_sentiment) > 0.6,
                    'timestamp': time.time()
                }
                
            return None
            
        except Exception as e:
            self.logger.error(f"Reddit sentiment analysis failed: {e}")
            return self._fallback_sentiment_analysis(symbol, 'reddit')
            
    def _analyze_news_sentiment(self, symbol):
        """Analyze news sentiment"""
        try:
            news_data = []
            
            # CryptoCompare News API
            cc_data = self._fetch_cryptocompare_news(symbol)
            if cc_data:
                news_data.extend(cc_data)
                
            # CoinDesk API (if available)
            cd_data = self._fetch_coindesk_news(symbol)
            if cd_data:
                news_data.extend(cd_data)
                
            # Generic news search
            generic_data = self._fetch_generic_news(symbol)
            if generic_data:
                news_data.extend(generic_data)
                
            if news_data:
                # Calculate sentiment
                sentiments = [d['sentiment'] for d in news_data]
                weights = [d['weight'] for d in news_data]
                
                weighted_sentiment = np.average(sentiments, weights=weights)
                confidence = min(len(news_data) / 10, 1.0)
                
                self.last_updates['news'] = time.time()
                self.analysis_stats['news_analyses'] += 1
                
                return {
                    'source': 'news',
                    'sentiment_score': weighted_sentiment,
                    'confidence': confidence,
                    'volume': len(news_data),
                    'articles': len(news_data),
                    'is_extreme': abs(weighted_sentiment) > 0.7,
                    'timestamp': time.time()
                }
                
            return None
            
        except Exception as e:
            self.logger.error(f"News sentiment analysis failed: {e}")
            return None
            
    def _fetch_cryptocompare_news(self, symbol):
        """Fetch news from CryptoCompare API"""
        try:
            url = f"https://min-api.cryptocompare.com/data/v2/news/?categories={symbol}&lang=EN"
            response = requests.get(url, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                articles = data.get('Data', [])
                
                news_sentiments = []
                
                for article in articles[:15]:  # Latest 15 articles
                    title = article.get('title', '')
                    body = article.get('body', '')
                    
                    text = f"{title} {body}".lower()
                    sentiment = self._calculate_text_sentiment(text)
                    
                    # Weight by source credibility and recency
                    source_weight = 1.0
                    time_weight = 1.0  # Could adjust based on article age
                    
                    news_sentiments.append({
                        'sentiment': sentiment,
                        'weight': source_weight * time_weight,
                        'source': 'cryptocompare'
                    })
                    
                return news_sentiments
                
            return None
            
        except Exception as e:
            self.logger.debug(f"CryptoCompare news fetch failed: {e}")
            return None
            
    def _fetch_coindesk_news(self, symbol):
        """Fetch news from CoinDesk (simulated)"""
        try:
            # This would implement actual CoinDesk API
            # For now, return simulated data
            return [
                {
                    'sentiment': np.random.uniform(-0.3, 0.7),  # Slight bullish bias
                    'weight': 1.5,  # Higher weight for major source
                    'source': 'coindesk'
                }
            ] if np.random.random() > 0.7 else None
            
        except:
            return None
            
    def _fetch_generic_news(self, symbol):
        """Fetch from generic news sources"""
        try:
            # This would implement news aggregator APIs
            # For now, return simulated data
            news_count = np.random.randint(0, 5)
            
            if news_count > 0:
                return [
                    {
                        'sentiment': np.random.uniform(-0.5, 0.5),
                        'weight': 0.8,
                        'source': 'generic'
                    }
                    for _ in range(news_count)
                ]
                
            return None
            
        except:
            return None
            
    def _analyze_telegram_sentiment(self, symbol):
        """Analyze Telegram sentiment (simulated)"""
        try:
            # This would implement actual Telegram API
            # For now, return simulated data based on market conditions
            
            base_sentiment = np.random.uniform(-0.4, 0.6)  # Slight bullish bias
            confidence = np.random.uniform(0.3, 0.8)
            
            self.last_updates['telegram'] = time.time()
            self.analysis_stats['telegram_analyses'] += 1
            
            return {
                'source': 'telegram',
                'sentiment_score': base_sentiment,
                'confidence': confidence,
                'volume': np.random.randint(20, 100),
                'channels': np.random.randint(3, 8),
                'is_extreme': abs(base_sentiment) > 0.6,
                'timestamp': time.time()
            }
            
        except Exception as e:
            self.logger.debug(f"Telegram sentiment analysis failed: {e}")
            return None
            
    def _fallback_sentiment_analysis(self, symbol, source):
        """Fallback sentiment when APIs not available"""
        try:
            # Generate reasonable sentiment based on market patterns
            base_sentiment = np.random.uniform(-0.3, 0.5)  # Slight bullish bias
            confidence = 0.4  # Lower confidence for fallback
            
            return {
                'source': source,
                'sentiment_score': base_sentiment,
                'confidence': confidence,
                'volume': np.random.randint(10, 50),
                'is_extreme': False,
                'fallback': True,
                'timestamp': time.time()
            }
            
        except:
            return None
            
    def _calculate_text_sentiment(self, text):
        """Calculate sentiment score for text"""
        try:
            if HAS_SENTIMENT:
                # Use TextBlob for sentiment analysis
                from textblob import TextBlob
                blob = TextBlob(text)
                return blob.sentiment.polarity
            else:
                # Fallback keyword-based sentiment
                return self._keyword_based_sentiment(text)
                
        except:
            return self._keyword_based_sentiment(text)
            
    def _keyword_based_sentiment(self, text):
        """Keyword-based sentiment analysis"""
        text_lower = text.lower()
        
        bullish_count = sum(1 for keyword in self.bullish_keywords if keyword in text_lower)
        bearish_count = sum(1 for keyword in self.bearish_keywords if keyword in text_lower)
        
        if bullish_count > bearish_count:
            return min(bullish_count / 10, 1.0)
        elif bearish_count > bullish_count:
            return -min(bearish_count / 10, 1.0)
        else:
            return 0.0
            
    def _clean_text(self, text):
        """Clean text for analysis"""
        import re
        
        # Remove URLs
        text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
        
        # Remove mentions and hashtags symbols
        text = re.sub(r'@\w+|#', '', text)
        
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text.lower()
        
    def _count_keywords(self, text, keyword_counts):
        """Count sentiment keywords in text"""
        for keyword in self.bullish_keywords + self.bearish_keywords:
            if keyword in text:
                keyword_counts[keyword] += 1
                
    def _combine_sentiment_data(self, sentiment_sources):
        """Combine sentiment from multiple sources"""
        try:
            if not sentiment_sources:
                return None
                
            # Source weights
            source_weights = {
                'twitter': 0.30,
                'reddit': 0.25,
                'news': 0.35,
                'telegram': 0.10
            }
            
            weighted_scores = []
            total_confidence = 0
            total_volume = 0
            
            for source_name, source_data in sentiment_sources.items():
                source_weight = source_weights.get(source_name, 0.1)
                sentiment_score = source_data.get('sentiment_score', 0)
                confidence = source_data.get('confidence', 0.5)
                volume = source_data.get('volume', 0)
                
                # Weight by source importance and confidence
                weighted_score = sentiment_score * source_weight * confidence
                weighted_scores.append(weighted_score)
                
                total_confidence += confidence * source_weight
                total_volume += volume
                
            # Calculate final sentiment
            final_sentiment = sum(weighted_scores)
            
            # Normalize confidence
            final_confidence = min(total_confidence, 1.0)
            
            # Detect extremes
            is_extreme = abs(final_sentiment) > 0.6 and final_confidence > 0.6
            
            # Determine direction
            if final_sentiment > 0.15:
                direction = 'bullish'
            elif final_sentiment < -0.15:
                direction = 'bearish'
            else:
                direction = 'neutral'
                
            return {
                'sentiment_score': final_sentiment,
                'confidence': final_confidence,
                'direction': direction,
                'is_extreme': is_extreme,
                'strength': abs(final_sentiment),
                'sources': list(sentiment_sources.keys()),
                'source_count': len(sentiment_sources),
                'total_volume': total_volume,
                'raw_scores': {k: v.get('sentiment_score', 0) for k, v in sentiment_sources.items()},
                'timestamp': time.time()
            }
            
        except Exception as e:
            self.logger.error(f"Sentiment combination failed: {e}")
            return None
            
    def _cache_sentiment(self, cache_key, sentiment_data):
        """Cache sentiment data"""
        self.sentiment_cache[cache_key] = {
            'sentiment': sentiment_data,
            'timestamp': time.time()
        }
        
        # Keep cache size manageable
        if len(self.sentiment_cache) > 100:
            # Remove oldest entries
            oldest_key = min(self.sentiment_cache.keys(), 
                           key=lambda k: self.sentiment_cache[k]['timestamp'])
            del self.sentiment_cache[oldest_key]
            
    def _store_sentiment_history(self, symbol, sentiment_data):
        """Store sentiment in history"""
        self.sentiment_history[symbol].append({
            'timestamp': time.time(),
            'sentiment': sentiment_data
        })
        
        # Keep only recent history
        if len(self.sentiment_history[symbol]) > 100:
            self.sentiment_history[symbol].pop(0)
            
    def _update_trending_topics(self, symbol, sentiment_data):
        """Update trending topics tracking"""
        if sentiment_data.get('is_extreme', False):
            self.trending_topics[symbol] += 1
            
    def get_trending_symbols(self, min_mentions=3):
        """Get symbols with high sentiment activity"""
        trending = []
        
        for symbol, mention_count in self.trending_topics.items():
            if mention_count >= min_mentions:
                # Get recent sentiment
                recent_sentiment = self.get_recent_sentiment_trend(symbol)
                
                if recent_sentiment:
                    trending.append({
                        'symbol': symbol,
                        'mentions': mention_count,
                        'sentiment_score': recent_sentiment['avg_sentiment'],
                        'sentiment_change': recent_sentiment['sentiment_change'],
                        'direction': recent_sentiment['direction']
                    })
                    
        # Sort by mention count and sentiment strength
        trending.sort(key=lambda x: x['mentions'] * abs(x['sentiment_score']), reverse=True)
        
        return trending[:10]  # Top 10
        
    def get_recent_sentiment_trend(self, symbol):
        """Get recent sentiment trend for symbol"""
        try:
            if symbol not in self.sentiment_history:
                return None
                
            history = self.sentiment_history[symbol]
            
            if len(history) < 3:
                return None
                
            # Recent vs older sentiment
            recent_sentiments = [h['sentiment']['sentiment_score'] for h in history[-3:]]
            older_sentiments = [h['sentiment']['sentiment_score'] for h in history[-6:-3]] if len(history) >= 6 else recent_sentiments
            
            recent_avg = np.mean(recent_sentiments)
            older_avg = np.mean(older_sentiments)
            
            sentiment_change = recent_avg - older_avg
            
            return {
                'avg_sentiment': recent_avg,
                'sentiment_change': sentiment_change,
                'direction': 'improving' if sentiment_change > 0.1 else 'declining' if sentiment_change < -0.1 else 'stable',
                'volatility': np.std(recent_sentiments)
            }
            
        except Exception as e:
            self.logger.error(f"Sentiment trend calculation failed: {e}")
            return None
            
    def get_sentiment_signal(self, symbol, sentiment_data):
        """Convert sentiment data to trading signal"""
        try:
            if not sentiment_data:
                return None
                
            sentiment_score = sentiment_data.get('sentiment_score', 0)
            confidence = sentiment_data.get('confidence', 0)
            is_extreme = sentiment_data.get('is_extreme', False)
            
            # Minimum thresholds
            if abs(sentiment_score) < 0.3 or confidence < 0.5:
                return None
                
            # Signal logic
            if is_extreme and abs(sentiment_score) > 0.7:
                # Extreme sentiment - could be contrarian
                if sentiment_score > 0:
                    # Extreme bullish - could be top
                    signal_direction = 'short' if confidence > 0.8 else None
                    signal_confidence = confidence * 0.6  # Lower confidence for contrarian
                else:
                    # Extreme bearish - could be bottom
                    signal_direction = 'long' if confidence > 0.8 else None
                    signal_confidence = confidence * 0.6
                    
                signal_type = 'contrarian'
                
            else:
                # Follow sentiment
                signal_direction = 'long' if sentiment_score > 0 else 'short'
                signal_confidence = confidence * abs(sentiment_score)
                signal_type = 'momentum'
                
            if signal_direction and signal_confidence > 0.5:
                return {
                    'direction': signal_direction,
                    'confidence': min(signal_confidence, 0.9),
                    'sentiment_score': sentiment_score,
                    'signal_type': signal_type,
                    'is_extreme': is_extreme,
                    'sources': sentiment_data.get('sources', []),
                    'strength': abs(sentiment_score)
                }
                
            return None
            
        except Exception as e:
            self.logger.error(f"Sentiment signal generation failed: {e}")
            return None
            
    def get_analysis_statistics(self):
        """Get sentiment analysis performance statistics"""
        total_analyses = sum(self.analysis_stats.values())
        
        stats = {
            'total_analyses': total_analyses,
            'analyses_by_source': dict(self.analysis_stats),
            'cached_sentiments': len(self.sentiment_cache),
            'symbols_tracked': len(self.sentiment_history),
            'trending_topics': len(self.trending_topics),
            'api_status': {
                'twitter': self.twitter_api is not None,
                'reddit': self.reddit_api is not None
            }
        }
        
        return stats
 
def initialize_sentiment_system(config):
    """Initialize the complete sentiment analysis system"""
    try:
        # Initialize sentiment engine
        sentiment_engine = SentimentAnalysisEngine(config)
        
        logger = logging.getLogger('MonsterBot.Sentiment')
        logger.info("‚úÖ Sentiment analysis system initialized")
        
        # Log API status
        stats = sentiment_engine.get_analysis_statistics()
        api_status = stats['api_status']
        
        logger.info(f"   üì± Twitter API: {'‚úÖ' if api_status['twitter'] else '‚ùå'}")
        logger.info(f"   üì± Reddit API: {'‚úÖ' if api_status['reddit'] else '‚ùå'}")
        
        return {
            'sentiment_engine': sentiment_engine,
            'api_status': api_status
        }
        
    except Exception as e:
        logger = logging.getLogger('MonsterBot.Sentiment')
        logger.error(f"Sentiment system initialization failed: {e}")
        raise        

class WhaleTrackingSystem:
    """Advanced whale detection and smart money tracking system"""
    
    def __init__(self, config, binance_client, memory_manager):
        self.config = config
        self.binance_client = binance_client
        self.memory_manager = memory_manager
        self.logger = logging.getLogger('MonsterBot.WhaleTracker')
        
        # Whale detection thresholds
        self.large_order_threshold = config.WHALE_SETTINGS['large_order_threshold']
        self.whale_volume_multiplier = config.WHALE_SETTINGS['whale_volume_multiplier']
        self.accumulation_period = config.WHALE_SETTINGS['accumulation_period']
        
        # Tracking data structures
        self.active_whale_orders = {}
        self.whale_wallets = {}
        self.accumulation_zones = defaultdict(dict)
        self.distribution_zones = defaultdict(dict)
        self.whale_signals = deque(maxlen=1000)
        
        # Order book monitoring
        self.order_book_snapshots = defaultdict(deque)
        self.whale_walls = defaultdict(list)
        self.iceberg_orders = defaultdict(list)
        
        # Performance metrics
        self.detection_accuracy = defaultdict(list)
        self.whale_move_predictions = defaultdict(list)
        
        # WebSocket connections for real-time monitoring
        self.ws_connections = {}
        self.monitoring_symbols = set()
        
    def detect_whale_activity(self, symbol, market_data, order_book, trades):
        """Main whale detection engine"""
        try:
            whale_activities = []
            
            # 1. Order book whale detection
            orderbook_whales = self._detect_orderbook_whales(symbol, order_book, market_data)
            whale_activities.extend(orderbook_whales)
            
            # 2. Large trade detection
            trade_whales = self._detect_trade_whales(symbol, trades, market_data)
            whale_activities.extend(trade_whales)
            
            # 3. Accumulation/Distribution detection
            accumulation_signals = self._detect_accumulation_distribution(symbol, market_data, trades, order_book)
            whale_activities.extend(accumulation_signals)
            
            # 4. Iceberg order detection
            iceberg_signals = self._detect_iceberg_orders(symbol, order_book, trades)
            whale_activities.extend(iceberg_signals)
            
            # 5. Cross-exchange whale tracking
            cross_exchange_signals = self._track_cross_exchange_whales(symbol, market_data)
            whale_activities.extend(cross_exchange_signals)
            
            # Score and rank whale activities
            scored_activities = self._score_whale_activities(whale_activities, symbol, market_data)
            
            # Generate trading signals
            trading_signals = self._generate_whale_signals(scored_activities, symbol, market_data)
            
            return trading_signals
            
        except Exception as e:
            self.logger.error(f"Whale detection failed for {symbol}: {e}")
            return []
            
    def _detect_orderbook_whales(self, symbol, order_book, market_data):
        """Detect whale activity in order book"""
        whales = []
        
        if not order_book or 'bids' not in order_book:
            return whales
            
        current_price = market_data['close'].iloc[-1] if len(market_data) > 0 else 0
        avg_volume = market_data['volume'].rolling(20).mean().iloc[-1] if len(market_data) >= 20 else 0
        
        # Analyze bid side
        for i, (price, size) in enumerate(order_book['bids'][:20]):
            order_value = float(price) * float(size)
            
            if order_value > self.large_order_threshold:
                distance_from_price = abs(float(price) - current_price) / current_price
                
                # Large order close to current price = potential whale wall
                if distance_from_price < 0.02:  # Within 2%
                    whales.append({
                        'type': 'whale_wall',
                        'side': 'bid',
                        'price': float(price),
                        'size': float(size),
                        'value_usd': order_value,
                        'distance_pct': distance_from_price,
                        'depth_level': i,
                        'confidence': self._calculate_wall_confidence(order_value, distance_from_price, avg_volume),
                        'potential_impact': 'strong_support',
                        'estimated_move': self._estimate_move_from_wall(order_value, 'support')
                    })
                    
        # Analyze ask side
        for i, (price, size) in enumerate(order_book['asks'][:20]):
            order_value = float(price) * float(size)
            
            if order_value > self.large_order_threshold:
                distance_from_price = abs(float(price) - current_price) / current_price
                
                if distance_from_price < 0.02:  # Within 2%
                    whales.append({
                        'type': 'whale_wall',
                        'side': 'ask',
                        'price': float(price),
                        'size': float(size),
                        'value_usd': order_value,
                        'distance_pct': distance_from_price,
                        'depth_level': i,
                        'confidence': self._calculate_wall_confidence(order_value, distance_from_price, avg_volume),
                        'potential_impact': 'strong_resistance',
                        'estimated_move': self._estimate_move_from_wall(order_value, 'resistance')
                    })
                    
        # Detect order book imbalances
        total_bid_volume = sum(float(size) for _, size in order_book['bids'][:10])
        total_ask_volume = sum(float(size) for _, size in order_book['asks'][:10])
        
        if total_bid_volume > 0 and total_ask_volume > 0:
            imbalance = (total_bid_volume - total_ask_volume) / (total_bid_volume + total_ask_volume)
            
            if abs(imbalance) > 0.3:  # Significant imbalance
                whales.append({
                    'type': 'order_imbalance',
                    'side': 'bid' if imbalance > 0 else 'ask',
                    'imbalance_ratio': imbalance,
                    'bid_volume': total_bid_volume,
                    'ask_volume': total_ask_volume,
                    'confidence': min(abs(imbalance) * 2, 1.0),
                    'potential_impact': 'directional_pressure',
                    'estimated_move': abs(imbalance) * 0.05  # 5% max move estimate
                })
                
        return whales
        
    def _detect_trade_whales(self, symbol, trades, market_data):
        """Detect whale activity in trade flow"""
        whales = []
        
        if not trades or len(trades) < 10:
            return whales
            
        # Calculate trade statistics
        trade_sizes = [t['quantity'] * t['price'] for t in trades]
        avg_trade_size = np.mean(trade_sizes)
        std_trade_size = np.std(trade_sizes)
        
        # Define whale trade threshold
        whale_threshold = max(
            avg_trade_size + (3 * std_trade_size),
            self.large_order_threshold * 0.1  # 10% of large order threshold
        )
        
        # Analyze recent trades for whale activity
        whale_trades = []
        for trade in trades[-50:]:  # Last 50 trades
            trade_value = trade['quantity'] * trade['price']
            
            if trade_value > whale_threshold:
                whale_trades.append({
                    'timestamp': trade['timestamp'],
                    'side': 'buy' if not trade['is_buyer_maker'] else 'sell',
                    'value': trade_value,
                    'price': trade['price'],
                    'quantity': trade['quantity']
                })
                
        if len(whale_trades) >= 3:  # Multiple whale trades
            # Analyze whale trade direction
            buy_volume = sum(t['value'] for t in whale_trades if t['side'] == 'buy')
            sell_volume = sum(t['value'] for t in whale_trades if t['side'] == 'sell')
            
            total_volume = buy_volume + sell_volume
            
            if total_volume > 0:
                buy_ratio = buy_volume / total_volume
                
                whales.append({
                    'type': 'whale_trades',
                    'trade_count': len(whale_trades),
                    'total_volume': total_volume,
                    'buy_volume': buy_volume,
                    'sell_volume': sell_volume,
                    'buy_ratio': buy_ratio,
                    'dominant_side': 'buy' if buy_ratio > 0.6 else 'sell' if buy_ratio < 0.4 else 'neutral',
                    'confidence': min(len(whale_trades) / 10, 1.0),
                    'potential_impact': 'momentum_shift',
                    'estimated_move': self._estimate_move_from_trades(total_volume, buy_ratio)
                })
                
        # Detect unusual trade clustering
        recent_10min = [t for t in trades if t['timestamp'] > time.time() - 600]
        if len(recent_10min) > 0:
            recent_volume = sum(t['quantity'] * t['price'] for t in recent_10min)
            avg_10min_volume = np.mean([
                sum(t['quantity'] * t['price'] for t in trades[i:i+len(recent_10min)])
                for i in range(0, len(trades) - len(recent_10min), len(recent_10min))
            ]) if len(trades) > len(recent_10min) else recent_volume
            
            if recent_volume > avg_10min_volume * 3:  # 3x normal volume
                whales.append({
                    'type': 'volume_spike',
                    'volume_ratio': recent_volume / avg_10min_volume,
                    'recent_volume': recent_volume,
                    'avg_volume': avg_10min_volume,
                    'confidence': min(recent_volume / avg_10min_volume / 5, 1.0),
                    'potential_impact': 'accumulation_or_distribution',
                    'estimated_move': min((recent_volume / avg_10min_volume - 1) * 0.02, 0.1)
                })
                
        return whales
        
    def _detect_accumulation_distribution(self, symbol, market_data, trades, order_book):
        """Detect whale accumulation or distribution patterns"""
        signals = []
        
        if len(market_data) < self.accumulation_period:
            return signals
            
        df = market_data.tail(self.accumulation_period)
        
        # Volume-Price Analysis
        vpt = self._calculate_volume_price_trend(df)
        obv = self._calculate_on_balance_volume(df)
        
        # Accumulation Detection
        price_change = (df['close'].iloc[-1] - df['close'].iloc[0]) / df['close'].iloc[0]
        volume_trend = df['volume'].tail(10).mean() / df['volume'].head(10).mean()
        
        # Accumulation: Rising volume, stable/slightly rising price
        if volume_trend > 1.3 and -0.02 < price_change < 0.05:
            accumulation_strength = self._calculate_accumulation_strength(df, trades, order_book)
            
            if accumulation_strength > 0.6:
                signals.append({
                    'type': 'accumulation_phase',
                    'strength': accumulation_strength,
                    'volume_trend': volume_trend,
                    'price_stability': 1 - abs(price_change) * 20,
                    'vpt_trend': vpt[-1] - vpt[0],
                    'obv_trend': obv[-1] - obv[0],
                    'confidence': accumulation_strength,
                    'potential_impact': 'bullish_breakout',
                    'estimated_move': self._estimate_accumulation_breakout(accumulation_strength, volume_trend)
                })
                
        # Distribution Detection
        elif volume_trend > 1.2 and price_change > 0.02:
            distribution_strength = self._calculate_distribution_strength(df, trades, order_book)
            
            if distribution_strength > 0.6:
                signals.append({
                    'type': 'distribution_phase',
                    'strength': distribution_strength,
                    'volume_trend': volume_trend,
                    'price_rise': price_change,
                    'vpt_divergence': self._detect_vpt_divergence(df, vpt),
                    'confidence': distribution_strength,
                    'potential_impact': 'bearish_reversal',
                    'estimated_move': -self._estimate_distribution_dump(distribution_strength, price_change)
                })
                
        return signals
        
    def _detect_iceberg_orders(self, symbol, order_book, trades):
        """Detect hidden iceberg orders"""
        signals = []
        
        if not order_book or len(trades) < 20:
            return signals
            
        # Store current order book
        self._store_orderbook_snapshot(symbol, order_book)
        
        # Analyze order book changes for iceberg patterns
        if len(self.order_book_snapshots[symbol]) >= 3:
            iceberg_bids = self._detect_iceberg_pattern(symbol, 'bid')
            iceberg_asks = self._detect_iceberg_pattern(symbol, 'ask')
            
            for iceberg in iceberg_bids + iceberg_asks:
                signals.append({
                    'type': 'iceberg_order',
                    'side': iceberg['side'],
                    'price_level': iceberg['price'],
                    'estimated_size': iceberg['estimated_total_size'],
                    'visible_size': iceberg['visible_size'],
                    'refill_count': iceberg['refill_count'],
                    'confidence': iceberg['confidence'],
                    'potential_impact': f"hidden_{iceberg['side']}_pressure",
                    'estimated_move': self._estimate_iceberg_impact(iceberg)
                })
                
        return signals
        
    def _track_cross_exchange_whales(self, symbol, market_data):
        """Track whale movements across exchanges"""
        signals = []
        
        # This would integrate with multiple exchange APIs
        # For now, analyze potential cross-exchange arbitrage that might indicate whale activity
        
        try:
            # Get funding rates as proxy for whale positioning
            funding_rate = self.binance_client.get_funding_rate(symbol)
            
            if funding_rate and abs(funding_rate['funding_rate']) > 0.01:  # 1% funding
                signals.append({
                    'type': 'funding_rate_extreme',
                    'funding_rate': funding_rate['funding_rate'],
                    'direction': 'long_heavy' if funding_rate['funding_rate'] > 0 else 'short_heavy',
                    'confidence': min(abs(funding_rate['funding_rate']) * 50, 1.0),
                    'potential_impact': 'position_squeeze',
                    'estimated_move': abs(funding_rate['funding_rate']) * 2
                })
                
        except Exception as e:
            self.logger.debug(f"Cross-exchange tracking failed: {e}")
            
        return signals
        
    def _calculate_wall_confidence(self, order_value, distance_pct, avg_volume):
        """Calculate confidence for whale wall detection"""
        size_score = min(order_value / self.large_order_threshold, 5.0) / 5.0
        proximity_score = max(0, 1 - distance_pct * 50)
        volume_score = min(order_value / (avg_volume * 100), 2.0) / 2.0 if avg_volume > 0 else 0.5
        
        return (size_score * 0.4 + proximity_score * 0.4 + volume_score * 0.2)
        
    def _estimate_move_from_wall(self, order_value, wall_type):
        """Estimate potential price move from whale wall"""
        base_move = min(order_value / 10000000, 0.15)  # Max 15% move
        
        if wall_type == 'support':
            return base_move  # Upward move potential
        else:
            return -base_move  # Downward pressure
            
    def _estimate_move_from_trades(self, total_volume, buy_ratio):
        """Estimate price move from whale trades"""
        volume_impact = min(total_volume / 50000000, 0.2)  # Max 20% from volume
        direction_strength = abs(buy_ratio - 0.5) * 2  # 0 to 1
        
        move_magnitude = volume_impact * direction_strength
        return move_magnitude if buy_ratio > 0.5 else -move_magnitude
        
    def _calculate_volume_price_trend(self, df):
        """Calculate Volume Price Trend indicator"""
        vpt = []
        vpt_value = 0
        
        for i in range(1, len(df)):
            price_change_pct = (df['close'].iloc[i] - df['close'].iloc[i-1]) / df['close'].iloc[i-1]
            vpt_value += df['volume'].iloc[i] * price_change_pct
            vpt.append(vpt_value)
            
        return vpt
        
    def _calculate_on_balance_volume(self, df):
        """Calculate On Balance Volume"""
        obv = []
        obv_value = 0
        
        for i in range(1, len(df)):
            if df['close'].iloc[i] > df['close'].iloc[i-1]:
                obv_value += df['volume'].iloc[i]
            elif df['close'].iloc[i] < df['close'].iloc[i-1]:
                obv_value -= df['volume'].iloc[i]
            obv.append(obv_value)
            
        return obv
        
    def _calculate_accumulation_strength(self, df, trades, order_book):
        """Calculate strength of accumulation pattern"""
        # Volume consistency
        volume_cv = df['volume'].std() / df['volume'].mean()
        volume_score = max(0, 1 - volume_cv)
        
        # Price stability
        price_volatility = df['close'].std() / df['close'].mean()
        stability_score = max(0, 1 - price_volatility * 50)
        
        # Large trade analysis
        if trades:
            large_trades = [t for t in trades if t['quantity'] * t['price'] > np.mean([tr['quantity'] * tr['price'] for tr in trades]) * 2]
            buy_large = sum(1 for t in large_trades if not t['is_buyer_maker'])
            large_trade_score = min(buy_large / max(len(large_trades), 1), 1.0)
        else:
            large_trade_score = 0.5
            
        # Order book depth
        if order_book and 'volume_imbalance' in order_book:
            depth_score = max(0, order_book['volume_imbalance'])
        else:
            depth_score = 0.5
            
        return (volume_score * 0.3 + stability_score * 0.3 + large_trade_score * 0.2 + depth_score * 0.2)
        
    def _calculate_distribution_strength(self, df, trades, order_book):
        """Calculate strength of distribution pattern"""
        # Price rise with volume
        price_momentum = (df['close'].iloc[-1] - df['close'].iloc[0]) / df['close'].iloc[0]
        volume_support = df['volume'].tail(10).mean() / df['volume'].head(10).mean()
        
        momentum_score = min(price_momentum * 10, 1.0)
        volume_score = min(volume_support / 2, 1.0)
        
        # Sell pressure in trades
        if trades:
            sell_pressure = sum(t['quantity'] for t in trades if t['is_buyer_maker']) / sum(t['quantity'] for t in trades)
            sell_score = sell_pressure
        else:
            sell_score = 0.5
            
        return (momentum_score * 0.4 + volume_score * 0.3 + sell_score * 0.3)
        
    def _store_orderbook_snapshot(self, symbol, order_book):
        """Store order book snapshot for iceberg detection"""
        snapshot = {
            'timestamp': time.time(),
            'bids': order_book['bids'][:10],
            'asks': order_book['asks'][:10]
        }
        
        self.order_book_snapshots[symbol].append(snapshot)
        
        # Keep only recent snapshots
        if len(self.order_book_snapshots[symbol]) > 20:
            self.order_book_snapshots[symbol].popleft()
            
    def _detect_iceberg_pattern(self, symbol, side):
        """Detect iceberg order patterns"""
        icebergs = []
        
        if len(self.order_book_snapshots[symbol]) < 3:
            return icebergs
            
        snapshots = list(self.order_book_snapshots[symbol])
        
        # Analyze each price level for iceberg behavior
        for price_level in range(5):  # Top 5 levels
            refill_count = 0
            total_traded = 0
            consistent_price = None
            
            for i in range(1, len(snapshots)):
                prev_snapshot = snapshots[i-1]
                curr_snapshot = snapshots[i]
                
                if side == 'bid':
                    prev_orders = prev_snapshot['bids']
                    curr_orders = curr_snapshot['bids']
                else:
                    prev_orders = prev_snapshot['asks']
                    curr_orders = curr_snapshot['asks']
                    
                if price_level < len(prev_orders) and price_level < len(curr_orders):
                    prev_price, prev_size = prev_orders[price_level]
                    curr_price, curr_size = curr_orders[price_level]
                    
                    # Same price level with size refilled
                    if abs(float(prev_price) - float(curr_price)) < float(prev_price) * 0.001:
                        if float(curr_size) > float(prev_size) * 0.8:  # Refilled
                            refill_count += 1
                            consistent_price = float(curr_price)
                            
            # Iceberg detected if multiple refills at same price
            if refill_count >= 2 and consistent_price:
                estimated_size = refill_count * 1000000  # Rough estimate
                
                icebergs.append({
                    'side': side,
                    'price': consistent_price,
                    'estimated_total_size': estimated_size,
                    'visible_size': 500000,  # Typical iceberg chunk
                    'refill_count': refill_count,
                    'confidence': min(refill_count / 5, 1.0)
                })
                
        return icebergs
        
    def _estimate_iceberg_impact(self, iceberg):
        """Estimate market impact of iceberg order"""
        size_impact = min(iceberg['estimated_total_size'] / 10000000, 0.1)
        
        if iceberg['side'] == 'bid':
            return size_impact  # Upward pressure
        else:
            return -size_impact  # Downward pressure
            
    def _detect_vpt_divergence(self, df, vpt):
        """Detect VPT divergence for distribution"""
        if len(vpt) < 20:
            return False
            
        # Price making higher highs, VPT making lower highs
        price_trend = (df['close'].iloc[-5:].max() - df['close'].iloc[-20:-5].max()) / df['close'].iloc[-20:-5].max()
        vpt_trend = (max(vpt[-5:]) - max(vpt[-20:-5])) / abs(max(vpt[-20:-5]))
        
        return price_trend > 0.02 and vpt_trend < -0.1
        
    def _estimate_accumulation_breakout(self, strength, volume_trend):
        """Estimate breakout potential from accumulation"""
        base_move = 0.08  # 8% base expectation
        strength_multiplier = strength * 2
        volume_multiplier = min(volume_trend / 2, 2.0)
        
        return min(base_move * strength_multiplier * volume_multiplier, 0.5)  # Max 50%
        
    def _estimate_distribution_dump(self, strength, price_change):
        """Estimate dump potential from distribution"""
        base_dump = price_change * 1.5  # Dump 1.5x the rise
        strength_multiplier = strength * 1.5
        
        return min(base_dump * strength_multiplier, 0.3)  # Max 30% dump
        
    def _score_whale_activities(self, activities, symbol, market_data):
        """Score and rank whale activities by importance"""
        scored = []
        
        for activity in activities:
            # Base confidence from detection
            base_score = activity.get('confidence', 0.5)
            
            # Boost score based on activity type
            type_multipliers = {
                'whale_wall': 1.2,
                'whale_trades': 1.1,
                'accumulation_phase': 1.5,
                'distribution_phase': 1.4,
                'iceberg_order': 1.3,
                'order_imbalance': 1.0,
                'volume_spike': 1.1,
                'funding_rate_extreme': 0.9
            }
            
            type_multiplier = type_multipliers.get(activity['type'], 1.0)
            
            # Market context boost
            current_vol = market_data['volume'].iloc[-1] if len(market_data) > 0 else 1
            avg_vol = market_data['volume'].rolling(20).mean().iloc[-1] if len(market_data) >= 20 else current_vol
            volume_context = min(current_vol / avg_vol, 3.0) / 3.0
            
            final_score = base_score * type_multiplier * (0.7 + 0.3 * volume_context)
            
            activity['final_score'] = min(final_score, 1.0)
            scored.append(activity)
            
        # Sort by final score
        scored.sort(key=lambda x: x['final_score'], reverse=True)
        
        return scored
        
    def _generate_whale_signals(self, whale_activities, symbol, market_data):
        """Generate trading signals from whale activities"""
        signals = []
        
        # Only generate signals for high-confidence activities
        high_confidence = [w for w in whale_activities if w['final_score'] > 0.7]
        
        if not high_confidence:
            return signals
            
        # Combine related activities
        combined_signal = self._combine_whale_signals(high_confidence, symbol)
        
        if combined_signal:
            signals.append(combined_signal)
            
        return signals
        
    def _combine_whale_signals(self, activities, symbol):
        """Combine multiple whale activities into trading signal"""
        if not activities:
            return None
            
        # Determine dominant direction
        bullish_strength = 0
        bearish_strength = 0
        
        for activity in activities:
            impact = activity.get('potential_impact', '')
            estimated_move = activity.get('estimated_move', 0)
            confidence = activity['final_score']
            
            if 'bullish' in impact or estimated_move > 0:
                bullish_strength += confidence * abs(estimated_move or 0.05)
            elif 'bearish' in impact or estimated_move < 0:
                bearish_strength += confidence * abs(estimated_move or 0.05)
                
        # Determine final direction and confidence
        if bullish_strength > bearish_strength * 1.2:
            direction = 'long'
            final_confidence = min(bullish_strength, 1.0)
            estimated_move = min(bullish_strength * 0.1, 0.3)
        elif bearish_strength > bullish_strength * 1.2:
            direction = 'short'
            final_confidence = min(bearish_strength, 1.0)
            estimated_move = -min(bearish_strength * 0.1, 0.3)
        else:
            return None  # No clear direction
            
        return {
            'strategy': 'whale_tracking',
            'symbol': symbol,
            'direction': direction,
            'confidence': final_confidence,
            'estimated_move': estimated_move,
            'whale_activities': activities,
            'activity_count': len(activities),
            'dominant_activity': activities[0]['type'],
            'urgency': 'very_high' if final_confidence > 0.85 else 'high',
            'timestamp': time.time()
        }
 
class NewsReactionSystem:
    """Real-time news detection and millisecond reaction trading system"""
    
    def __init__(self, config, binance_client, memory_manager):
        self.config = config
        self.binance_client = binance_client
        self.memory_manager = memory_manager
        self.logger = logging.getLogger('MonsterBot.NewsReaction')
        
        # News sources and APIs
        self.news_sources = {
            'twitter': {'weight': 0.25, 'speed_priority': 1},
            'reddit': {'weight': 0.15, 'speed_priority': 3},
            'telegram': {'weight': 0.20, 'speed_priority': 2},
            'rss_feeds': {'weight': 0.25, 'speed_priority': 4},
            'official_announcements': {'weight': 0.35, 'speed_priority': 1},
            'whale_alerts': {'weight': 0.30, 'speed_priority': 1}
        }
        
        # Critical news keywords for instant reaction
        self.critical_keywords = {
            'extremely_bullish': {
                'keywords': ['partnership', 'adoption', 'integration', 'listing', 'upgrade', 'launch', 'breakthrough'],
                'impact_multiplier': 3.0,
                'min_move_estimate': 0.15,
                'max_move_estimate': 2.0
            },
            'bullish': {
                'keywords': ['positive', 'growth', 'development', 'expansion', 'investment', 'funding'],
                'impact_multiplier': 2.0,
                'min_move_estimate': 0.08,
                'max_move_estimate': 0.50
            },
            'extremely_bearish': {
                'keywords': ['hack', 'exploit', 'ban', 'regulation', 'lawsuit', 'investigation', 'fraud'],
                'impact_multiplier': -3.5,
                'min_move_estimate': -0.20,
                'max_move_estimate': -2.0
            },
            'bearish': {
                'keywords': ['negative', 'concern', 'risk', 'warning', 'problem', 'issue', 'delay'],
                'impact_multiplier': -2.0,
                'min_move_estimate': -0.10,
                'max_move_estimate': -0.60
            }
        }
        
        # News tracking
        self.processed_news = set()
        self.news_impact_history = defaultdict(list)
        self.reaction_times = deque(maxlen=100)
        self.news_accuracy_tracker = defaultdict(list)
        
        # Real-time monitoring
        self.monitoring_symbols = set()
        self.news_queue = queue.Queue()
        self.reaction_executor = None
        
        # Performance metrics
        self.news_trades_executed = 0
        self.news_profit_total = 0
        self.avg_reaction_time = 0
        
        self.initialize_news_monitoring()
        
    def initialize_news_monitoring(self):
        """Initialize real-time news monitoring"""
        try:
            # Start news monitoring threads
            self.start_twitter_monitor()
            self.start_telegram_monitor()
            self.start_rss_monitor()
            self.start_whale_alerts_monitor()
            
            # Start news processing thread
            self.reaction_executor = threading.Thread(target=self._process_news_queue, daemon=True)
            self.reaction_executor.start()
            
            self.logger.info("üö® News reaction system initialized - monitoring all sources")
            
        except Exception as e:
            self.logger.error(f"News monitoring initialization failed: {e}")
            
    def start_twitter_monitor(self):
        """Monitor Twitter for breaking crypto news"""
        def twitter_monitor():
            try:
                # Twitter API v2 streaming
                crypto_keywords = ['bitcoin', 'ethereum', 'binance', 'crypto', 'blockchain', 'defi', 'nft']
                whale_accounts = ['elonmusk', 'VitalikButerin', 'cz_binance', 'SBF_FTX', 'saylor']
                
                while True:
                    # Simulate real Twitter monitoring
                    # In production, this would use Twitter's streaming API
                    for keyword in crypto_keywords:
                        tweets = self._fetch_twitter_stream(keyword)
                        for tweet in tweets:
                            if self._is_market_moving_tweet(tweet):
                                self._queue_news_item({
                                    'source': 'twitter',
                                    'content': tweet['text'],
                                    'author': tweet['author'],
                                    'timestamp': tweet['created_at'],
                                    'engagement': tweet['metrics'],
                                    'urgency': self._calculate_tweet_urgency(tweet),
                                    'symbols_mentioned': self._extract_symbols(tweet['text'])
                                })
                                
                    time.sleep(1)  # 1 second polling for critical speed
                    
            except Exception as e:
                self.logger.error(f"Twitter monitor failed: {e}")
                
        threading.Thread(target=twitter_monitor, daemon=True).start()
        
    def start_telegram_monitor(self):
        """Monitor Telegram channels for alpha"""
        def telegram_monitor():
            try:
                # Monitor key crypto Telegram channels
                channels = [
                    '@whalepoolbtc', '@cryptosignals', '@binanceexchange',
                    '@whale_alert', '@defipulse', '@coindesk'
                ]
                
                while True:
                    for channel in channels:
                        messages = self._fetch_telegram_messages(channel)
                        for message in messages:
                            if self._is_alpha_message(message):
                                self._queue_news_item({
                                    'source': 'telegram',
                                    'channel': channel,
                                    'content': message['text'],
                                    'timestamp': message['date'],
                                    'urgency': self._calculate_telegram_urgency(message),
                                    'symbols_mentioned': self._extract_symbols(message['text'])
                                })
                                
                    time.sleep(2)  # 2 second polling
                    
            except Exception as e:
                self.logger.error(f"Telegram monitor failed: {e}")
                
        threading.Thread(target=telegram_monitor, daemon=True).start()
        
    def start_rss_monitor(self):
        """Monitor RSS feeds from major crypto news sources"""
        def rss_monitor():
            try:
                rss_feeds = [
                    'https://cointelegraph.com/rss',
                    'https://coindesk.com/arc/outboundfeeds/rss',
                    'https://decrypt.co/feed',
                    'https://theblock.co/rss.xml',
                    'https://cryptopotato.com/feed'
                ]
                
                while True:
                    for feed_url in rss_feeds:
                        articles = self._parse_rss_feed(feed_url)
                        for article in articles:
                            if article['id'] not in self.processed_news:
                                impact_score = self._calculate_article_impact(article)
                                if impact_score > 0.6:
                                    self._queue_news_item({
                                        'source': 'rss',
                                        'title': article['title'],
                                        'content': article['summary'],
                                        'url': article['link'],
                                        'timestamp': article['published'],
                                        'publisher': article['publisher'],
                                        'impact_score': impact_score,
                                        'urgency': 'high' if impact_score > 0.8 else 'medium',
                                        'symbols_mentioned': self._extract_symbols(article['title'] + ' ' + article['summary'])
                                    })
                                    
                    time.sleep(10)  # 10 second polling for RSS
                    
            except Exception as e:
                self.logger.error(f"RSS monitor failed: {e}")
                
        threading.Thread(target=rss_monitor, daemon=True).start()
        
    def start_whale_alerts_monitor(self):
        """Monitor whale alert services"""
        def whale_alerts_monitor():
            try:
                while True:
                    # Monitor Whale Alert API
                    alerts = self._fetch_whale_alerts()
                    for alert in alerts:
                        if alert['amount_usd'] > 10000000:  # $10M+ transactions
                            self._queue_news_item({
                                'source': 'whale_alert',
                                'transaction_hash': alert['hash'],
                                'amount_usd': alert['amount_usd'],
                                'symbol': alert['symbol'],
                                'from_exchange': alert['from'],
                                'to_exchange': alert['to'],
                                'timestamp': alert['timestamp'],
                                'urgency': self._calculate_whale_alert_urgency(alert),
                                'symbols_mentioned': [alert['symbol']]
                            })
                            
                    time.sleep(5)  # 5 second polling for whale alerts
                    
            except Exception as e:
                self.logger.error(f"Whale alerts monitor failed: {e}")
                
        threading.Thread(target=whale_alerts_monitor, daemon=True).start()
        
    def _process_news_queue(self):
        """Process news items and generate instant trading signals"""
        while True:
            try:
                # Get news item with timeout
                news_item = self.news_queue.get(timeout=1)
                
                reaction_start = time.time()
                
                # Quick impact assessment
                impact_analysis = self._analyze_news_impact(news_item)
                
                if impact_analysis['should_trade']:
                    # Generate immediate trading signal
                    trading_signals = self._generate_news_trading_signals(news_item, impact_analysis)
                    
                    # Execute trades immediately
                    for signal in trading_signals:
                        self._execute_news_reaction_trade(signal, news_item)
                        
                    reaction_time = (time.time() - reaction_start) * 1000  # milliseconds
                    self.reaction_times.append(reaction_time)
                    
                    self.logger.info(
                        f"‚ö° News reaction executed in {reaction_time:.1f}ms: "
                        f"{signal['symbol']} {signal['direction']} - "
                        f"Impact: {impact_analysis['estimated_move']:.1%}"
                    )
                    
                # Mark as processed
                news_id = self._generate_news_id(news_item)
                self.processed_news.add(news_id)
                
                # Clean old processed news
                if len(self.processed_news) > 10000:
                    self.processed_news = set(list(self.processed_news)[-5000:])
                    
            except queue.Empty:
                continue
            except Exception as e:
                self.logger.error(f"News processing failed: {e}")
                
    def _analyze_news_impact(self, news_item):
        """Analyze news impact and determine trading action"""
        try:
            content = news_item.get('content', '') + ' ' + news_item.get('title', '')
            content_lower = content.lower()
            
            # Keyword impact analysis
            impact_score = 0
            impact_direction = 0
            matched_keywords = []
            
            for category, data in self.critical_keywords.items():
                for keyword in data['keywords']:
                    if keyword in content_lower:
                        matched_keywords.append(keyword)
                        keyword_impact = data['impact_multiplier']
                        
                        # Weight by source credibility
                        source_weight = self.news_sources.get(news_item['source'], {}).get('weight', 1.0)
                        
                        # Weight by urgency
                        urgency_multiplier = {
                            'critical': 2.0,
                            'high': 1.5,
                            'medium': 1.0,
                            'low': 0.5
                        }.get(news_item.get('urgency', 'medium'), 1.0)
                        
                        weighted_impact = keyword_impact * source_weight * urgency_multiplier
                        impact_score += abs(weighted_impact)
                        impact_direction += weighted_impact
                        
            # Sentiment analysis boost
            sentiment_boost = self._calculate_news_sentiment(content)
            impact_direction += sentiment_boost * 0.5
            
            # Social engagement boost
            engagement_boost = self._calculate_engagement_boost(news_item)
            impact_score *= (1 + engagement_boost)
            
            # Symbol-specific impact
            symbols_mentioned = news_item.get('symbols_mentioned', [])
            symbol_impact = self._calculate_symbol_specific_impact(content, symbols_mentioned)
            
            # Final impact assessment
            should_trade = (
                impact_score > 1.0 and  # Minimum impact threshold
                len(symbols_mentioned) > 0 and  # Must mention specific symbols
                abs(impact_direction) > 0.5  # Clear directional bias
            )
            
            # Estimate move magnitude
            if should_trade:
                base_move = min(impact_score * 0.05, 0.3)  # 5% per impact point, max 30%
                direction_strength = abs(impact_direction) / max(impact_score, 1)
                estimated_move = base_move * direction_strength
                
                # Apply symbol-specific multipliers
                for symbol in symbols_mentioned:
                    if symbol in symbol_impact:
                        estimated_move *= symbol_impact[symbol]
                        
                estimated_move = min(estimated_move, 1.0)  # Cap at 100%
            else:
                estimated_move = 0
                
            return {
                'should_trade': should_trade,
                'impact_score': impact_score,
                'direction': 'long' if impact_direction > 0 else 'short',
                'estimated_move': estimated_move if impact_direction > 0 else -estimated_move,
                'confidence': min(impact_score / 3, 1.0),
                'matched_keywords': matched_keywords,
                'symbols_affected': symbols_mentioned,
                'time_sensitivity': self._calculate_time_sensitivity(news_item)
            }
            
        except Exception as e:
            self.logger.error(f"News impact analysis failed: {e}")
            return {'should_trade': False}
            
    def _generate_news_trading_signals(self, news_item, impact_analysis):
        """Generate specific trading signals from news"""
        signals = []
        
        if not impact_analysis['should_trade']:
            return signals
            
        symbols = impact_analysis['symbols_affected']
        estimated_move = impact_analysis['estimated_move']
        confidence = impact_analysis['confidence']
        direction = impact_analysis['direction']
        
        for symbol in symbols:
            # Calculate position size based on confidence and move size
            base_position_size = confidence * 0.6  # Up to 60% based on confidence
            
            # News reaction position sizing
            if impact_analysis['time_sensitivity'] == 'critical':
                position_size = min(base_position_size * 1.5, 0.8)  # Up to 80% for critical news
            else:
                position_size = base_position_size
                
            # Calculate targets based on estimated move
            if abs(estimated_move) > 0.2:  # Large moves (20%+)
                targets = {
                    'tp1': {'level': estimated_move * 0.3, 'size': 0.2},
                    'tp2': {'level': estimated_move * 0.6, 'size': 0.3},
                    'tp3': {'level': estimated_move * 0.9, 'size': 0.3},
                    'trail': {'level': estimated_move * 1.2, 'size': 0.2},
                    'stop_loss': abs(estimated_move) * 0.2
                }
            else:  # Smaller moves
                targets = {
                    'tp1': {'level': estimated_move * 0.5, 'size': 0.4},
                    'tp2': {'level': estimated_move * 0.8, 'size': 0.4},
                    'tp3': {'level': estimated_move * 1.1, 'size': 0.2},
                    'stop_loss': abs(estimated_move) * 0.3
                }
                
            signal = {
                'strategy': 'news_reaction',
                'symbol': symbol,
                'direction': direction,
                'confidence': confidence,
                'position_size': position_size,
                'estimated_move': estimated_move,
                'targets': targets,
                'urgency': 'immediate',
                'news_source': news_item['source'],
                'news_impact_score': impact_analysis['impact_score'],
                'time_sensitivity': impact_analysis['time_sensitivity'],
                'entry_type': 'market',  # Always market orders for news
                'leverage_multiplier': 1.2 if abs(estimated_move) > 0.15 else 1.0,
                'max_hold_time': 3600 if abs(estimated_move) > 0.1 else 1800,  # 1h for big moves, 30m for small
                'timestamp': time.time()
            }
            
            signals.append(signal)
            
        return signals
        
    def _execute_news_reaction_trade(self, signal, news_item):
        """Execute news reaction trade immediately"""
        try:
            symbol = signal['symbol']
            direction = signal['direction']
            position_size = signal['position_size']
            
            # Get current account balance
            account_balance = self.binance_client.get_account_balance()
            
            # Calculate position size in USDT
            position_value = account_balance * position_size
            
            # Get current price
            current_price = self.binance_client.get_current_price(symbol)
            
            if not current_price:
                self.logger.error(f"Failed to get price for {symbol}")
                return False
                
            # Calculate quantity
            quantity = position_value / current_price
            
            # Round to symbol precision
            quantity = self.binance_client.round_to_precision(quantity, symbol, 'quantity')
            
            # Determine leverage
            leverage = min(
                signal.get('leverage_multiplier', 1.0) * 15,  # Base 15x leverage
                25  # Max 25x
            )
            
            # Execute the trade (paper mode check)
            if self.config.PAPER_MODE:
                trade_result = self._simulate_news_trade(signal, quantity, current_price)
            else:
                trade_result = self._execute_real_news_trade(signal, quantity, current_price, leverage)
                
            if trade_result['success']:
                # Track the trade
                self._track_news_trade(signal, news_item, trade_result)
                
                self.news_trades_executed += 1
                
                self.logger.info(
                    f"üìà News trade executed: {symbol} {direction} "
                    f"${position_value:.0f} @ {current_price:.6f} "
                    f"({leverage}x leverage) - News: {news_item['source']}"
                )
                
                return True
            else:
                self.logger.error(f"News trade execution failed: {trade_result.get('error', 'Unknown error')}")
                return False
                
        except Exception as e:
            self.logger.error(f"News reaction trade execution failed: {e}")
            return False
            
    def _simulate_news_trade(self, signal, quantity, entry_price):
        """Simulate news trade for paper mode"""
        return {
            'success': True,
            'entry_price': entry_price,
            'quantity': quantity,
            'trade_id': f"news_sim_{int(time.time())}",
            'timestamp': time.time()
        }
        
    def _execute_real_news_trade(self, signal, quantity, current_price, leverage):
        """Execute real news trade on Binance"""
        try:
            symbol = signal['symbol']
            direction = signal['direction']
            
            # Set leverage
            self.binance_client.client.futures_change_leverage(
                symbol=symbol,
                leverage=int(leverage)
            )
            
            # Place market order
            side = 'BUY' if direction == 'long' else 'SELL'
            
            order = self.binance_client.client.futures_create_order(
                symbol=symbol,
                side=side,
                type='MARKET',
                quantity=quantity
            )
            
            return {
                'success': True,
                'order_id': order['orderId'],
                'entry_price': float(order.get('avgPrice', current_price)),
                'quantity': float(order['executedQty']),
                'trade_id': order['orderId'],
                'timestamp': time.time()
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }
            
    def _queue_news_item(self, news_item):
        """Queue news item for processing"""
        # Check if already processed
        news_id = self._generate_news_id(news_item)
        if news_id in self.processed_news:
            return
            
        # Add timestamp for processing speed tracking
        news_item['queue_timestamp'] = time.time()
        
        # Priority queue based on urgency
        urgency_priority = {
            'critical': 0,
            'high': 1,
            'medium': 2,
            'low': 3
        }.get(news_item.get('urgency', 'medium'), 2)
        
        self.news_queue.put((urgency_priority, news_item))
        
    def _generate_news_id(self, news_item):
        """Generate unique ID for news item"""
        content_hash = hash(
            news_item.get('content', '') + 
            news_item.get('title', '') +
            str(news_item.get('timestamp', 0))
        )
        return f"{news_item['source']}_{content_hash}"
        
    def _fetch_twitter_stream(self, keyword):
        """Fetch Twitter stream (simulation)"""
        # In production, this would use Twitter API v2
        return [
            {
                'text': f"BREAKING: Major {keyword} development announced",
                'author': 'crypto_insider',
                'created_at': time.time(),
                'metrics': {'likes': 1000, 'retweets': 500}
            }
        ] if np.random.random() > 0.95 else []
        
    def _fetch_telegram_messages(self, channel):
        """Fetch Telegram messages (simulation)"""
        return [
            {
                'text': f"üö® WHALE ALERT: Massive accumulation detected in top altcoins",
                'date': time.time(),
                'channel': channel
            }
        ] if np.random.random() > 0.98 else []
        
    def _parse_rss_feed(self, feed_url):
        """Parse RSS feed for articles"""
        # In production, this would use feedparser
        return [
            {
                'id': f"article_{int(time.time())}",
                'title': "Bitcoin Breaks Major Resistance Level",
                'summary': "Technical analysis shows bullish momentum building",
                'link': feed_url,
                'published': time.time(),
                'publisher': 'CoinDesk'
            }
        ] if np.random.random() > 0.99 else []
        
    def _fetch_whale_alerts(self):
        """Fetch whale alerts"""
        return [
            {
                'hash': f"0x{int(time.time())}",
                'amount_usd': np.random.randint(10000000, 100000000),
                'symbol': 'BTC',
                'from': 'unknown',
                'to': 'binance',
                'timestamp': time.time()
            }
        ] if np.random.random() > 0.995 else []
        
    def _is_market_moving_tweet(self, tweet):
        """Determine if tweet could move markets"""
        text_lower = tweet['text'].lower()
        
        # Check for critical keywords
        critical_words = ['breaking', 'announcement', 'partnership', 'hack', 'regulation']
        has_critical = any(word in text_lower for word in critical_words)
        
        # Check engagement
        high_engagement = tweet['metrics']['likes'] > 500 or tweet['metrics']['retweets'] > 200
        
        return has_critical and high_engagement
        
    def _is_alpha_message(self, message):
        """Determine if Telegram message contains alpha"""
        text_lower = message['text'].lower()
        alpha_indicators = ['whale', 'accumulation', 'breakout', 'alert', 'signal']
        return any(indicator in text_lower for indicator in alpha_indicators)
        
    def _calculate_article_impact(self, article):
        """Calculate potential market impact of news article"""
        title_lower = article['title'].lower()
        summary_lower = article['summary'].lower()
        
        impact_score = 0
        
        # Check for high-impact keywords
        for category, data in self.critical_keywords.items():
            for keyword in data['keywords']:
                if keyword in title_lower:
                    impact_score += abs(data['impact_multiplier']) * 0.3
                if keyword in summary_lower:
                    impact_score += abs(data['impact_multiplier']) * 0.1
                    
        # Publisher credibility boost
        credible_publishers = ['coindesk', 'cointelegraph', 'bloomberg', 'reuters']
        if any(pub in article.get('publisher', '').lower() for pub in credible_publishers):
            impact_score *= 1.3
            
        return min(impact_score, 2.0)
        
    def _extract_symbols(self, text):
        """Extract cryptocurrency symbols from text"""
        text_upper = text.upper()
        
        # Common crypto symbols
        common_symbols = [
            'BTC', 'ETH', 'BNB', 'ADA', 'DOT', 'LINK', 'UNI', 'LTC', 'BCH',
            'XRP', 'SOL', 'AVAX', 'MATIC', 'ATOM', 'FIL', 'VET', 'TRX'
        ]
        
        found_symbols = []
        for symbol in common_symbols:
            if symbol in text_upper:
                found_symbols.append(f"{symbol}USDT")
                
        # Also check for explicit mentions like "Bitcoin" -> "BTCUSDT"
        symbol_mapping = {
            'BITCOIN': 'BTCUSDT',
            'ETHEREUM': 'ETHUSDT',
            'BINANCE COIN': 'BNBUSDT',
            'CARDANO': 'ADAUSDT',
            'POLKADOT': 'DOTUSDT'
        }
        
        for name, symbol in symbol_mapping.items():
            if name in text_upper:
                found_symbols.append(symbol)
                
        return list(set(found_symbols))  # Remove duplicates
        
    def _calculate_news_sentiment(self, content):
        """Calculate sentiment score for news content"""
        try:
            if HAS_SENTIMENT:
                from textblob import TextBlob
                blob = TextBlob(content)
                return blob.sentiment.polarity
            else:
                # Fallback keyword-based sentiment
                bullish_words = ['positive', 'growth', 'bullish', 'up', 'gain', 'rise']
                bearish_words = ['negative', 'bearish', 'down', 'fall', 'crash', 'drop']
                
                content_lower = content.lower()
                bullish_count = sum(1 for word in bullish_words if word in content_lower)
                bearish_count = sum(1 for word in bearish_words if word in content_lower)
                
                if bullish_count > bearish_count:
                    return min(bullish_count / 10, 1.0)
                elif bearish_count > bullish_count:
                    return -min(bearish_count / 10, 1.0)
                else:
                    return 0.0
                    
        except:
            return 0.0
            
    def _calculate_engagement_boost(self, news_item):
        """Calculate engagement-based impact boost"""
        source = news_item['source']
        
        if source == 'twitter':
            metrics = news_item.get('metrics', {})
            engagement = metrics.get('likes', 0) + metrics.get('retweets', 0) * 2
            return min(engagement / 10000, 0.5)  # Max 50% boost
            
        elif source == 'whale_alert':
            amount = news_item.get('amount_usd', 0)
            return min(amount / 100000000, 0.8)  # Max 80% boost for $100M+
            
        return 0.1  # Default small boost
        
    def _calculate_symbol_specific_impact(self, content, symbols):
        """Calculate symbol-specific impact multipliers"""
        impact_multipliers = {}
        
        for symbol in symbols:
            base_symbol = symbol.replace('USDT', '')
            multiplier = 1.0
            
            # Higher impact for smaller cap coins
            market_cap_tier = {
                'BTC': 0.8,     # Largest cap - smaller moves
                'ETH': 0.9,
                'BNB': 1.1,
                'ADA': 1.3,     # Mid cap - larger moves
                'DOT': 1.3,
                'LINK': 1.4,
                'UNI': 1.5,     # Smaller cap - largest moves
                'AVAX': 1.5,
                'MATIC': 1.6
            }
            
            multiplier = market_cap_tier.get(base_symbol, 1.2)  # Default mid-range
            impact_multipliers[symbol] = multiplier
            
        return impact_multipliers
        
    def _calculate_time_sensitivity(self, news_item):
        """Calculate time sensitivity of news"""
        source = news_item['source']
        urgency = news_item.get('urgency', 'medium')
        
        # Twitter and whale alerts are most time-sensitive
        if source in ['twitter', 'whale_alert'] and urgency in ['critical', 'high']:
            return 'critical'
        elif source in ['telegram', 'rss'] and urgency == 'high':
            return 'high'
        else:
            return 'medium'
            
    def _calculate_tweet_urgency(self, tweet):
        """Calculate urgency level for tweet"""
        text_lower = tweet['text'].lower()
        
        critical_words = ['breaking', 'urgent', 'alert', 'emergency']
        high_words = ['announcement', 'news', 'update', 'confirmed']
        
        if any(word in text_lower for word in critical_words):
            return 'critical'
        elif any(word in text_lower for word in high_words):
            return 'high'
        else:
            return 'medium'
            
	def _calculate_telegram_urgency(self, message):
        """Calculate urgency level for Telegram message"""
        text_lower = message['text'].lower()
        
        # Whale alert indicators
        whale_indicators = ['üö®', 'üêã', 'whale alert', 'massive', 'huge transfer']
        critical_indicators = ['breaking', 'urgent', 'immediately', 'now']
        
        if any(indicator in text_lower for indicator in whale_indicators + critical_indicators):
            return 'critical'
        elif any(word in text_lower for word in ['signal', 'alert', 'update', 'news']):
            return 'high'
        else:
            return 'medium'
            
    def _calculate_whale_alert_urgency(self, alert):
        """Calculate urgency for whale alert"""
        amount = alert['amount_usd']
        
        if amount > 100000000:  # $100M+
            return 'critical'
        elif amount > 50000000:  # $50M+
            return 'high'
        elif amount > 20000000:  # $20M+
            return 'medium'
        else:
            return 'low'
            
    def _track_news_trade(self, signal, news_item, trade_result):
        """Track news-based trade for performance analysis"""
        trade_record = {
            'timestamp': time.time(),
            'symbol': signal['symbol'],
            'direction': signal['direction'],
            'entry_price': trade_result['entry_price'],
            'position_size': signal['position_size'],
            'news_source': news_item['source'],
            'news_impact_score': signal['news_impact_score'],
            'estimated_move': signal['estimated_move'],
            'trade_id': trade_result['trade_id'],
            'targets': signal['targets']
        }
        
        self.news_impact_history[signal['symbol']].append(trade_record)
        
        # Keep only recent history
        if len(self.news_impact_history[signal['symbol']]) > 50:
            self.news_impact_history[signal['symbol']].pop(0)
            
    def get_news_performance_stats(self):
        """Get news trading performance statistics"""
        if not self.news_impact_history:
            return {}
            
        all_trades = []
        for symbol_trades in self.news_impact_history.values():
            all_trades.extend(symbol_trades)
            
        if not all_trades:
            return {}
            
        # Calculate performance metrics
        total_trades = len(all_trades)
        avg_reaction_time = np.mean(self.reaction_times) if self.reaction_times else 0
        
        # News source performance
        source_performance = defaultdict(list)
        for trade in all_trades:
            source_performance[trade['news_source']].append(trade)
            
        source_stats = {}
        for source, trades in source_performance.items():
            source_stats[source] = {
                'trade_count': len(trades),
                'avg_impact_score': np.mean([t['news_impact_score'] for t in trades]),
                'avg_estimated_move': np.mean([abs(t['estimated_move']) for t in trades])
            }
            
        return {
            'total_news_trades': total_trades,
            'avg_reaction_time_ms': avg_reaction_time,
            'news_sources_tracked': len(self.news_sources),
            'active_monitors': 4,  # Twitter, Telegram, RSS, Whale Alerts
            'source_performance': source_stats,
            'processed_news_count': len(self.processed_news)
        }
        
    def get_trending_news_symbols(self):
        """Get symbols trending in news"""
        trending = defaultdict(int)
        recent_cutoff = time.time() - 3600  # Last hour
        
        for symbol_trades in self.news_impact_history.values():
            for trade in symbol_trades:
                if trade['timestamp'] > recent_cutoff:
                    trending[trade['symbol']] += trade['news_impact_score']
                    
        # Sort by impact score
        sorted_trending = sorted(trending.items(), key=lambda x: x[1], reverse=True)
        
        return [
            {
                'symbol': symbol,
                'news_impact_score': score,
                'news_mentions': sum(1 for trades in self.news_impact_history.values() 
                                   for trade in trades 
                                   if trade['symbol'] == symbol and trade['timestamp'] > recent_cutoff)
            }
            for symbol, score in sorted_trending[:10]
        ]
        
    def monitor_symbol_news(self, symbol):
        """Add symbol to news monitoring"""
        self.monitoring_symbols.add(symbol)
        
    def stop_monitoring_symbol(self, symbol):
        """Remove symbol from news monitoring"""
        self.monitoring_symbols.discard(symbol)
        
    def get_real_time_news_feed(self, limit=20):
        """Get recent news items for dashboard"""
        recent_news = []
        recent_cutoff = time.time() - 1800  # Last 30 minutes
        
        # This would collect from all news sources in production
        for symbol_trades in self.news_impact_history.values():
            for trade in symbol_trades:
                if trade['timestamp'] > recent_cutoff:
                    recent_news.append({
                        'timestamp': trade['timestamp'],
                        'symbol': trade['symbol'],
                        'source': trade['news_source'],
                        'impact_score': trade['news_impact_score'],
                        'estimated_move': trade['estimated_move'],
                        'direction': trade['direction']
                    })
                    
        # Sort by timestamp
        recent_news.sort(key=lambda x: x['timestamp'], reverse=True)
        
        return recent_news[:limit]

class NewsSignalGenerator:
    """Generates trading signals from news analysis"""
    
    def __init__(self, config, news_system):
        self.config = config
        self.news_system = news_system
        self.logger = logging.getLogger('MonsterBot.NewsSignals')
        
        # Signal generation parameters
        self.min_confidence_threshold = 0.6
        self.max_position_per_symbol = 0.3
        self.news_signal_decay_time = 1800  # 30 minutes
        
        # Active news signals
        self.active_signals = {}
        self.signal_performance = defaultdict(list)
        
    def generate_signals_from_news(self, symbols_to_scan):
        """Generate trading signals from current news analysis"""
        signals = []
        
        for symbol in symbols_to_scan:
            # Get recent news impact for symbol
            news_signal = self._analyze_symbol_news_impact(symbol)
            
            if news_signal and news_signal['confidence'] > self.min_confidence_threshold:
                # Check if we already have an active signal
                if self._should_generate_new_signal(symbol, news_signal):
                    trading_signal = self._convert_news_to_signal(symbol, news_signal)
                    if trading_signal:
                        signals.append(trading_signal)
                        self._track_signal(symbol, trading_signal)
                        
        return signals
        
    def _analyze_symbol_news_impact(self, symbol):
        """Analyze recent news impact for specific symbol"""
        recent_cutoff = time.time() - self.news_signal_decay_time
        
        if symbol not in self.news_system.news_impact_history:
            return None
            
        recent_trades = [
            trade for trade in self.news_system.news_impact_history[symbol]
            if trade['timestamp'] > recent_cutoff
        ]
        
        if not recent_trades:
            return None
            
        # Aggregate news impact
        total_impact = sum(trade['news_impact_score'] for trade in recent_trades)
        avg_estimated_move = np.mean([trade['estimated_move'] for trade in recent_trades])
        dominant_direction = 'long' if avg_estimated_move > 0 else 'short'
        
        # Calculate confidence based on consistency and strength
        direction_consistency = self._calculate_direction_consistency(recent_trades)
        impact_strength = min(total_impact / len(recent_trades), 2.0) / 2.0
        
        confidence = (direction_consistency * 0.6 + impact_strength * 0.4)
        
        return {
            'symbol': symbol,
            'direction': dominant_direction,
            'confidence': confidence,
            'estimated_move': avg_estimated_move,
            'news_count': len(recent_trades),
            'total_impact': total_impact,
            'sources': list(set(trade['news_source'] for trade in recent_trades)),
            'timestamp': time.time()
        }
        
    def _calculate_direction_consistency(self, trades):
        """Calculate how consistent the directional bias is"""
        if not trades:
            return 0
            
        directions = [trade['estimated_move'] > 0 for trade in trades]
        positive_count = sum(directions)
        total_count = len(directions)
        
        # Consistency score (1.0 = all same direction, 0.5 = mixed)
        consistency = max(positive_count, total_count - positive_count) / total_count
        
        # Boost for very consistent signals
        if consistency > 0.8:
            consistency = min(consistency * 1.2, 1.0)
            
        return consistency
        
    def _should_generate_new_signal(self, symbol, news_signal):
        """Check if we should generate a new signal"""
        if symbol not in self.active_signals:
            return True
            
        active_signal = self.active_signals[symbol]
        
        # Check if active signal is expired
        if time.time() - active_signal['timestamp'] > self.news_signal_decay_time:
            return True
            
        # Check if new signal is significantly stronger
        if news_signal['confidence'] > active_signal['confidence'] * 1.3:
            return True
            
        # Check if direction changed with high confidence
        if (news_signal['direction'] != active_signal['direction'] and 
            news_signal['confidence'] > 0.8):
            return True
            
        return False
        
    def _convert_news_to_signal(self, symbol, news_signal):
        """Convert news analysis to trading signal"""
        try:
            # Get current market data for context
            current_price = self.news_system.binance_client.get_current_price(symbol)
            if not current_price:
                return None
                
            # Calculate position size based on confidence and news strength
            base_position_size = news_signal['confidence'] * 0.4  # Up to 40%
            news_strength_multiplier = min(news_signal['total_impact'] / 3, 1.5)
            
            position_size = min(
                base_position_size * news_strength_multiplier,
                self.max_position_per_symbol
            )
            
            # Calculate targets based on estimated move
            estimated_move = news_signal['estimated_move']
            
            if abs(estimated_move) > 0.15:  # Large expected move (15%+)
                targets = {
                    'tp1': {'level': estimated_move * 0.3, 'size': 0.15},
                    'tp2': {'level': estimated_move * 0.6, 'size': 0.25},
                    'tp3': {'level': estimated_move * 0.9, 'size': 0.35},
                    'trail': {'level': estimated_move * 1.3, 'size': 0.25},
                    'stop_loss': abs(estimated_move) * 0.25
                }
            else:  # Normal move
                targets = {
                    'tp1': {'level': estimated_move * 0.4, 'size': 0.3},
                    'tp2': {'level': estimated_move * 0.8, 'size': 0.4},
                    'tp3': {'level': estimated_move * 1.2, 'size': 0.3},
                    'stop_loss': abs(estimated_move) * 0.4
                }
                
            # Determine urgency based on news characteristics
            urgency = self._determine_signal_urgency(news_signal)
            
            return {
                'strategy': 'news_signal',
                'symbol': symbol,
                'direction': news_signal['direction'],
                'confidence': news_signal['confidence'],
                'position_size': position_size,
                'estimated_move': estimated_move,
                'targets': targets,
                'urgency': urgency,
                'entry_type': 'market' if urgency == 'immediate' else 'limit',
                'news_sources': news_signal['sources'],
                'news_impact_score': news_signal['total_impact'],
                'news_count': news_signal['news_count'],
                'max_hold_time': self._calculate_max_hold_time(news_signal),
                'timestamp': time.time()
            }
            
        except Exception as e:
            self.logger.error(f"Signal conversion failed for {symbol}: {e}")
            return None
            
    def _determine_signal_urgency(self, news_signal):
        """Determine signal urgency based on news characteristics"""
        # High impact + multiple sources = immediate
        if news_signal['total_impact'] > 3.0 and len(news_signal['sources']) >= 2:
            return 'immediate'
            
        # High confidence = high urgency
        if news_signal['confidence'] > 0.85:
            return 'high'
            
        # Multiple news items = medium urgency
        if news_signal['news_count'] >= 3:
            return 'medium'
            
        return 'low'
        
    def _calculate_max_hold_time(self, news_signal):
        """Calculate maximum hold time for news-based signal"""
        base_time = 3600  # 1 hour
        
        # Longer hold for higher confidence
        confidence_multiplier = 1 + news_signal['confidence']
        
        # Shorter hold for very recent news (market moves fast)
        recency_multiplier = 1.0
        if news_signal['news_count'] > 5:  # Very recent/active news
            recency_multiplier = 0.7
            
        return int(base_time * confidence_multiplier * recency_multiplier)
        
    def _track_signal(self, symbol, signal):
        """Track generated signal"""
        self.active_signals[symbol] = signal
        
        # Store in performance tracking
        self.signal_performance[symbol].append({
            'generated_at': time.time(),
            'signal': signal.copy(),
            'outcome': None  # To be filled when signal resolves
        })
        
        # Keep performance history manageable
        if len(self.signal_performance[symbol]) > 20:
            self.signal_performance[symbol].pop(0)
            
    def update_signal_outcome(self, symbol, trade_result):
        """Update signal outcome for performance tracking"""
        if symbol in self.signal_performance:
            recent_signals = self.signal_performance[symbol]
            
            # Find the most recent unresolved signal
            for signal_record in reversed(recent_signals):
                if signal_record['outcome'] is None:
                    signal_record['outcome'] = {
                        'pnl_pct': trade_result.get('pnl_pct', 0),
                        'exit_reason': trade_result.get('exit_reason', 'unknown'),
                        'hold_time': trade_result.get('hold_time', 0),
                        'max_profit': trade_result.get('max_profit_pct', 0),
                        'max_loss': trade_result.get('max_loss_pct', 0)
                    }
                    break
                    
    def get_signal_performance_stats(self):
        """Get performance statistics for news signals"""
        all_signals = []
        for symbol_signals in self.signal_performance.values():
            for signal_record in symbol_signals:
                if signal_record['outcome'] is not None:
                    all_signals.append(signal_record)
                    
        if not all_signals:
            return {}
            
        # Calculate performance metrics
        total_signals = len(all_signals)
        profitable_signals = sum(1 for s in all_signals if s['outcome']['pnl_pct'] > 0)
        win_rate = profitable_signals / total_signals
        
        avg_profit = np.mean([s['outcome']['pnl_pct'] for s in all_signals])
        avg_hold_time = np.mean([s['outcome']['hold_time'] for s in all_signals])
        
        # Performance by confidence level
        high_conf_signals = [s for s in all_signals if s['signal']['confidence'] > 0.8]
        high_conf_win_rate = (
            sum(1 for s in high_conf_signals if s['outcome']['pnl_pct'] > 0) / len(high_conf_signals)
            if high_conf_signals else 0
        )
        
        return {
            'total_signals_generated': total_signals,
            'win_rate': win_rate,
            'avg_profit_pct': avg_profit,
            'avg_hold_time_minutes': avg_hold_time / 60,
            'high_confidence_win_rate': high_conf_win_rate,
            'active_signals': len(self.active_signals),
            'best_performing_symbol': self._get_best_performing_symbol()
        }
        
    def _get_best_performing_symbol(self):
        """Get best performing symbol for news signals"""
        symbol_performance = {}
        
        for symbol, signals in self.signal_performance.items():
            resolved_signals = [s for s in signals if s['outcome'] is not None]
            if len(resolved_signals) >= 3:  # Minimum signals for reliability
                avg_profit = np.mean([s['outcome']['pnl_pct'] for s in resolved_signals])
                symbol_performance[symbol] = avg_profit
                
        if symbol_performance:
            best_symbol = max(symbol_performance.items(), key=lambda x: x[1])
            return {'symbol': best_symbol[0], 'avg_profit_pct': best_symbol[1]}
        else:
            return None

def initialize_news_reaction_system(config, binance_client, memory_manager):
    """Initialize complete news reaction system"""
    try:
        # Initialize news reaction system
        news_system = NewsReactionSystem(config, binance_client, memory_manager)
        
        # Initialize signal generator
        signal_generator = NewsSignalGenerator(config, news_system)
        
        logger = logging.getLogger('MonsterBot.NewsReaction')
        logger.info("üö® News reaction system fully initialized")
        logger.info("   üì± Twitter monitoring: Active")
        logger.info("   üì± Telegram monitoring: Active") 
        logger.info("   üì∞ RSS feeds monitoring: Active")
        logger.info("   üêã Whale alerts monitoring: Active")
        logger.info("   ‚ö° Reaction time target: <100ms")
        
        return {
            'news_system': news_system,
            'signal_generator': signal_generator,
            'monitoring_active': True,
            'reaction_target_ms': 100
        }
        
    except Exception as e:
        logger = logging.getLogger('MonsterBot.NewsReaction')
        logger.error(f"News reaction system initialization failed: {e}")
        raise
        
class OrderBookAnalysisSystem:
    """Advanced order book analysis for market microstructure insights"""
    
    def __init__(self, config, binance_client, memory_manager):
        self.config = config
        self.binance_client = binance_client
        self.memory_manager = memory_manager
        self.logger = logging.getLogger('MonsterBot.OrderBookAnalysis')
        
        # Analysis parameters
        self.depth_levels = config.WHALE_SETTINGS['order_book_depth']
        self.spoofing_sensitivity = 0.15
        self.liquidity_threshold = 1000000  # $1M
        self.imbalance_threshold = 0.3
        
        # Data storage
        self.order_book_snapshots = defaultdict(deque)
        self.liquidity_profiles = defaultdict(dict)
        self.spoofing_detections = defaultdict(list)
        self.market_maker_patterns = defaultdict(dict)
        
        # Analysis results
        self.depth_analysis_cache = {}
        self.liquidity_heat_maps = defaultdict(dict)
        self.manipulation_alerts = defaultdict(list)
        
        # Performance tracking
        self.analysis_accuracy = defaultdict(list)
        self.prediction_outcomes = defaultdict(list)
        
    def analyze_order_book_depth(self, symbol, order_book, current_price, market_data):
        """Comprehensive order book depth analysis"""
        try:
            if not order_book or 'bids' not in order_book or 'asks' not in order_book:
                return None
                
            analysis = {
                'symbol': symbol,
                'timestamp': time.time(),
                'current_price': current_price
            }
            
            # Store snapshot for historical analysis
            self._store_snapshot(symbol, order_book, current_price)
            
            # 1. Liquidity depth analysis
            liquidity_analysis = self._analyze_liquidity_depth(order_book, current_price)
            analysis['liquidity'] = liquidity_analysis
            
            # 2. Order size distribution analysis
            size_analysis = self._analyze_order_sizes(order_book, current_price)
            analysis['order_sizes'] = size_analysis
            
            # 3. Price level clustering analysis
            clustering_analysis = self._analyze_price_clustering(order_book, current_price)
            analysis['price_clustering'] = clustering_analysis
            
            # 4. Bid-ask spread analysis
            spread_analysis = self._analyze_spread_dynamics(order_book, current_price)
            analysis['spread'] = spread_analysis
            
            # 5. Market impact estimation
            impact_analysis = self._estimate_market_impact(order_book, current_price, market_data)
            analysis['market_impact'] = impact_analysis
            
            # 6. Detect unusual patterns
            anomaly_analysis = self._detect_order_book_anomalies(symbol, order_book)
            analysis['anomalies'] = anomaly_analysis
            
            # Cache analysis
            cache_key = f"depth_{symbol}_{int(time.time())}"
            self.memory_manager.cache_set(cache_key, analysis, 'market_data')
            
            return analysis
            
        except Exception as e:
            self.logger.error(f"Order book depth analysis failed for {symbol}: {e}")
            return None
            
    def detect_spoofing_patterns(self, symbol, order_book_history):
        """Detect order book spoofing and manipulation"""
        try:
            if len(order_book_history) < 5:
                return []
                
            spoofing_alerts = []
            
            # Analyze last 10 snapshots for patterns
            recent_snapshots = list(order_book_history)[-10:]
            
            # 1. Detect disappearing large orders
            disappearing_orders = self._detect_disappearing_orders(recent_snapshots)
            if disappearing_orders:
                spoofing_alerts.append({
                    'type': 'disappearing_large_orders',
                    'pattern': disappearing_orders,
                    'confidence': self._calculate_spoofing_confidence(disappearing_orders),
                    'impact': 'potential_fake_walls',
                    'severity': 'high' if len(disappearing_orders) > 3 else 'medium'
                })
                
            # 2. Detect iceberg order patterns
            iceberg_patterns = self._detect_iceberg_spoofing(recent_snapshots)
            if iceberg_patterns:
                spoofing_alerts.append({
                    'type': 'iceberg_manipulation',
                    'pattern': iceberg_patterns,
                    'confidence': 0.8,
                    'impact': 'hidden_liquidity_removal',
                    'severity': 'high'
                })
                
            # 3. Detect layering patterns
            layering_patterns = self._detect_layering(recent_snapshots)
            if layering_patterns:
                spoofing_alerts.append({
                    'type': 'order_layering',
                    'pattern': layering_patterns,
                    'confidence': 0.75,
                    'impact': 'artificial_price_support',
                    'severity': 'medium'
                })
                
            # 4. Detect quote stuffing
            quote_stuffing = self._detect_quote_stuffing(recent_snapshots)
            if quote_stuffing:
                spoofing_alerts.append({
                    'type': 'quote_stuffing',
                    'pattern': quote_stuffing,
                    'confidence': 0.9,
                    'impact': 'market_slowdown',
                    'severity': 'high'
                })
                
            # Store alerts
            if spoofing_alerts:
                self.spoofing_detections[symbol].extend(spoofing_alerts)
                
                # Keep only recent alerts
                cutoff_time = time.time() - 3600  # 1 hour
                self.spoofing_detections[symbol] = [
                    alert for alert in self.spoofing_detections[symbol]
                    if alert.get('timestamp', time.time()) > cutoff_time
                ]
                
            return spoofing_alerts
            
        except Exception as e:
            self.logger.error(f"Spoofing detection failed for {symbol}: {e}")
            return []
            
    def analyze_liquidity_distribution(self, symbol, order_book, price_levels=20):
        """Analyze liquidity distribution across price levels"""
        try:
            if not order_book:
                return None
                
            current_price = (float(order_book['bids'][0][0]) + float(order_book['asks'][0][0])) / 2
            
            # Analyze bid side liquidity
            bid_liquidity = self._calculate_side_liquidity(
                order_book['bids'], current_price, 'bid', price_levels
            )
            
            # Analyze ask side liquidity
            ask_liquidity = self._calculate_side_liquidity(
                order_book['asks'], current_price, 'ask', price_levels
            )
            
            # Calculate liquidity ratios and imbalances
            total_bid_liquidity = sum(level['liquidity_usd'] for level in bid_liquidity)
            total_ask_liquidity = sum(level['liquidity_usd'] for level in ask_liquidity)
            
            liquidity_imbalance = (
                (total_bid_liquidity - total_ask_liquidity) / 
                (total_bid_liquidity + total_ask_liquidity)
                if (total_bid_liquidity + total_ask_liquidity) > 0 else 0
            )
            
            # Identify liquidity gaps
            liquidity_gaps = self._identify_liquidity_gaps(bid_liquidity, ask_liquidity)
            
            # Calculate liquidity concentration
            concentration_analysis = self._analyze_liquidity_concentration(
                bid_liquidity, ask_liquidity
            )
            
            return {
                'symbol': symbol,
                'timestamp': time.time(),
                'current_price': current_price,
                'bid_liquidity': bid_liquidity,
                'ask_liquidity': ask_liquidity,
                'total_bid_liquidity': total_bid_liquidity,
                'total_ask_liquidity': total_ask_liquidity,
                'liquidity_imbalance': liquidity_imbalance,
                'liquidity_gaps': liquidity_gaps,
                'concentration': concentration_analysis,
                'liquidity_quality_score': self._calculate_liquidity_quality(
                    total_bid_liquidity, total_ask_liquidity, liquidity_gaps
                )
            }
            
        except Exception as e:
            self.logger.error(f"Liquidity distribution analysis failed for {symbol}: {e}")
            return None
            
    def detect_market_maker_behavior(self, symbol, order_book_history, trade_history):
        """Detect market maker patterns and behavior"""
        try:
            if len(order_book_history) < 10 or len(trade_history) < 50:
                return None
                
            mm_patterns = {}
            
            # 1. Detect tight spread maintenance
            spread_consistency = self._analyze_spread_consistency(order_book_history)
            if spread_consistency['is_consistent']:
                mm_patterns['tight_spread_maintenance'] = {
                    'detected': True,
                    'avg_spread_bps': spread_consistency['avg_spread_bps'],
                    'consistency_score': spread_consistency['consistency_score'],
                    'confidence': 0.8
                }
                
            # 2. Detect inventory management patterns
            inventory_patterns = self._detect_inventory_management(trade_history)
            if inventory_patterns['detected']:
                mm_patterns['inventory_management'] = inventory_patterns
                
            # 3. Detect order refreshing patterns
            refresh_patterns = self._detect_order_refreshing(order_book_history)
            if refresh_patterns['detected']:
                mm_patterns['order_refreshing'] = refresh_patterns
                
            # 4. Detect adverse selection avoidance
            adverse_selection = self._detect_adverse_selection_avoidance(
                order_book_history, trade_history
            )
            if adverse_selection['detected']:
                mm_patterns['adverse_selection_avoidance'] = adverse_selection
                
            # 5. Calculate market maker presence score
            mm_score = self._calculate_market_maker_score(mm_patterns)
            
            result = {
                'symbol': symbol,
                'timestamp': time.time(),
                'market_maker_patterns': mm_patterns,
                'market_maker_score': mm_score,
                'market_maker_present': mm_score > 0.6,
                'liquidity_provision_quality': self._assess_liquidity_quality(mm_patterns)
            }
            
            # Store in market maker patterns
            self.market_maker_patterns[symbol] = result
            
            return result
            
        except Exception as e:
            self.logger.error(f"Market maker detection failed for {symbol}: {e}")
            return None
            
    def generate_microstructure_signals(self, symbol, order_book_analysis, spoofing_alerts, mm_analysis):
        """Generate trading signals from microstructure analysis"""
        try:
            signals = []
            
            if not order_book_analysis:
                return signals
                
            # 1. Liquidity imbalance signals
            liquidity = order_book_analysis.get('liquidity', {})
            imbalance = liquidity.get('imbalance', 0)
            
            if abs(imbalance) > self.imbalance_threshold:
                confidence = min(abs(imbalance) * 2, 1.0)
                
                signals.append({
                    'type': 'liquidity_imbalance',
                    'direction': 'long' if imbalance > 0 else 'short',
                    'confidence': confidence,
                    'strength': abs(imbalance),
                    'estimated_move': abs(imbalance) * 0.02,  # 2% max from imbalance
                    'time_horizon': 'short',  # 5-15 minutes
                    'signal_source': 'order_book_imbalance'
                })
                
            # 2. Spoofing detection signals
            if spoofing_alerts:
                for alert in spoofing_alerts:
                    if alert['severity'] == 'high':
                        # Counter-spoofing signal
                        signals.append({
                            'type': 'anti_spoofing',
                            'direction': self._determine_anti_spoofing_direction(alert),
                            'confidence': alert['confidence'],
                            'strength': 0.8,
                            'estimated_move': 0.03,  # 3% move when spoof breaks
                            'time_horizon': 'very_short',  # 1-5 minutes
                            'signal_source': f"anti_{alert['type']}"
                        })
                        
            # 3. Market impact signals
            impact = order_book_analysis.get('market_impact', {})
            if impact:
                large_order_impact = impact.get('large_order_impact', {})
                
                if large_order_impact.get('impact_1m_usd', 0) < 0.005:  # Low impact
                    signals.append({
                        'type': 'low_market_impact',
                        'direction': 'long',  # Easier to move up
                        'confidence': 0.6,
                        'strength': 0.7,
                        'estimated_move': 0.04,  # 4% easier movement
                        'time_horizon': 'medium',  # 15-60 minutes
                        'signal_source': 'low_market_impact'
                    })
                    
            # 4. Market maker presence signals
            if mm_analysis and mm_analysis.get('market_maker_present', False):
                mm_score = mm_analysis.get('market_maker_score', 0)
                
                if mm_score > 0.8:  # Strong MM presence
                    signals.append({
                        'type': 'market_maker_stability',
                        'direction': 'neutral',
                        'confidence': mm_score,
                        'strength': 0.6,
                        'estimated_move': 0.01,  # Lower volatility expected
                        'time_horizon': 'long',  # 1-4 hours
                        'signal_source': 'market_maker_presence',
                        'trade_style': 'range_bound'
                    })
                    
            # 5. Liquidity gap signals
            gaps = order_book_analysis.get('liquidity_gaps', [])
            significant_gaps = [g for g in gaps if g.get('gap_size_pct', 0) > 0.02]  # 2%+ gaps
            
            if significant_gaps:
                # Target gap fills
                for gap in significant_gaps[:2]:  # Top 2 gaps
                    signals.append({
                        'type': 'liquidity_gap_fill',
                        'direction': 'long' if gap['side'] == 'ask' else 'short',
                        'confidence': 0.7,
                        'strength': gap['gap_size_pct'] * 10,
                        'estimated_move': gap['gap_size_pct'],
                        'target_price': gap['gap_price'],
                        'time_horizon': 'medium',
                        'signal_source': 'liquidity_gap'
                    })
                    
            # Score and filter signals
            scored_signals = self._score_microstructure_signals(signals)
            
            return [s for s in scored_signals if s.get('final_score', 0) > 0.6]
            
        except Exception as e:
            self.logger.error(f"Microstructure signal generation failed for {symbol}: {e}")
            return []
            
    def _store_snapshot(self, symbol, order_book, current_price):
        """Store order book snapshot for analysis"""
        snapshot = {
            'timestamp': time.time(),
            'price': current_price,
            'bids': order_book['bids'][:self.depth_levels],
            'asks': order_book['asks'][:self.depth_levels],
            'spread': float(order_book['asks'][0][0]) - float(order_book['bids'][0][0]),
            'mid_price': (float(order_book['bids'][0][0]) + float(order_book['asks'][0][0])) / 2
        }
        
        self.order_book_snapshots[symbol].append(snapshot)
        
        # Keep only recent snapshots (last hour)
        cutoff_time = time.time() - 3600
        while (self.order_book_snapshots[symbol] and 
               self.order_book_snapshots[symbol][0]['timestamp'] < cutoff_time):
            self.order_book_snapshots[symbol].popleft()
            
    def _analyze_liquidity_depth(self, order_book, current_price):
        """Analyze liquidity at different depths"""
        # Calculate cumulative liquidity at different depths
        depths = [5, 10, 20, 50]
        liquidity_analysis = {}
        
        for depth in depths:
            if len(order_book['bids']) >= depth and len(order_book['asks']) >= depth:
                bid_volume = sum(float(order[1]) for order in order_book['bids'][:depth])
                ask_volume = sum(float(order[1]) for order in order_book['asks'][:depth])
                
                bid_value = sum(float(order[0]) * float(order[1]) for order in order_book['bids'][:depth])
                ask_value = sum(float(order[0]) * float(order[1]) for order in order_book['asks'][:depth])
                
                liquidity_analysis[f'depth_{depth}'] = {
                    'bid_volume': bid_volume,
                    'ask_volume': ask_volume,
                    'bid_value_usd': bid_value,
                    'ask_value_usd': ask_value,
                    'total_liquidity': bid_value + ask_value,
                    'imbalance': (bid_value - ask_value) / (bid_value + ask_value) if (bid_value + ask_value) > 0 else 0
                }
                
        # Overall liquidity metrics
        total_bid_liquidity = liquidity_analysis.get('depth_50', {}).get('bid_value_usd', 0)
        total_ask_liquidity = liquidity_analysis.get('depth_50', {}).get('ask_value_usd', 0)
        
        liquidity_analysis['summary'] = {
            'total_liquidity': total_bid_liquidity + total_ask_liquidity,
            'liquidity_quality': 'high' if (total_bid_liquidity + total_ask_liquidity) > self.liquidity_threshold else 'low',
            'dominant_side': 'bid' if total_bid_liquidity > total_ask_liquidity * 1.2 else 'ask' if total_ask_liquidity > total_bid_liquidity * 1.2 else 'balanced'
        }
        
        return liquidity_analysis
        
    def _analyze_order_sizes(self, order_book, current_price):
        """Analyze distribution of order sizes"""
        all_orders = order_book['bids'][:20] + order_book['asks'][:20]
        order_values = [float(order[0]) * float(order[1]) for order in all_orders]
        
        if not order_values:
            return {}
            
        return {
            'avg_order_size': np.mean(order_values),
            'median_order_size': np.median(order_values),
            'max_order_size': max(order_values),
            'min_order_size': min(order_values),
            'size_std': np.std(order_values),
            'large_orders_count': len([v for v in order_values if v > self.liquidity_threshold * 0.1]),
            'size_distribution': {
                'small_orders_pct': len([v for v in order_values if v < 10000]) / len(order_values),
                'medium_orders_pct': len([v for v in order_values if 10000 <= v < 100000]) / len(order_values),
                'large_orders_pct': len([v for v in order_values if v >= 100000]) / len(order_values)
            }
        }
        
    def _analyze_price_clustering(self, order_book, current_price):
        """Analyze price level clustering patterns"""
        # Check for round number clustering
        bid_prices = [float(order[0]) for order in order_book['bids'][:20]]
        ask_prices = [float(order[0]) for order in order_book['asks'][:20]]
        
        def is_round_number(price):
            # Check for clustering at round numbers
            price_str = f"{price:.8f}".rstrip('0').rstrip('.')
            if len(price_str) > 0:
                last_digit = price_str[-1]
                return last_digit in ['0', '5']
            return False
            
        bid_round_count = sum(1 for price in bid_prices if is_round_number(price))
        ask_round_count = sum(1 for price in ask_prices if is_round_number(price))
        
        total_orders = len(bid_prices) + len(ask_prices)
        round_clustering = (bid_round_count + ask_round_count) / total_orders if total_orders > 0 else 0
        
        # Analyze price gaps
        bid_gaps = [bid_prices[i] - bid_prices[i+1] for i in range(len(bid_prices)-1)]
        ask_gaps = [ask_prices[i+1] - ask_prices[i] for i in range(len(ask_prices)-1)]
        
        return {
            'round_number_clustering': round_clustering,
            'avg_bid_gap': np.mean(bid_gaps) if bid_gaps else 0,
            'avg_ask_gap': np.mean(ask_gaps) if ask_gaps else 0,
            'price_clustering_score': round_clustering,
            'clustering_interpretation': 'high' if round_clustering > 0.3 else 'medium' if round_clustering > 0.15 else 'low'
        }
        
    def _analyze_spread_dynamics(self, order_book, current_price):
        """Analyze bid-ask spread characteristics"""
        if not order_book['bids'] or not order_book['asks']:
            return {}
            
        best_bid = float(order_book['bids'][0][0])
        best_ask = float(order_book['asks'][0][0])
        
        spread = best_ask - best_bid
        spread_pct = spread / current_price
        spread_bps = spread_pct * 10000  # basis points
        
        # Analyze spread relative to recent levels
        symbol = 'unknown'  # Would be passed in production
        recent_spreads = []
        if symbol in self.order_book_snapshots:
            recent_spreads = [
                snapshot['spread'] for snapshot in list(self.order_book_snapshots[symbol])[-20:]
                if 'spread' in snapshot
            ]
            
        avg_recent_spread = np.mean(recent_spreads) if recent_spreads else spread
        spread_relative = spread / avg_recent_spread if avg_recent_spread > 0 else 1.0
        
        return {
            'spread_absolute': spread,
            'spread_pct': spread_pct,
            'spread_bps': spread_bps,
            'spread_relative_to_recent': spread_relative,
            'spread_quality': 'tight' if spread_bps < 10 else 'normal' if spread_bps < 50 else 'wide',
            'spread_stability': 'stable' if 0.8 <= spread_relative <= 1.2 else 'volatile'
        }
        
    def _estimate_market_impact(self, order_book, current_price, market_data):
        """Estimate market impact for different order sizes"""
        # Test order sizes: $10K, $100K, $1M
        test_sizes = [10000, 100000, 1000000]
        impact_estimates = {}
        
        for size_usd in test_sizes:
            # Calculate impact for buying
            buy_impact = self._calculate_order_impact(order_book['asks'], size_usd, 'buy')
            
            # Calculate impact for selling  
            sell_impact = self._calculate_order_impact(order_book['bids'], size_usd, 'sell')
            
            impact_estimates[f'impact_{size_usd//1000}k_usd'] = {
                'buy_impact_pct': buy_impact,
                'sell_impact_pct': sell_impact,
                'avg_impact_pct': (buy_impact + sell_impact) / 2
            }
            
        # Calculate liquidity score
        avg_impact_100k = impact_estimates.get('impact_100k_usd', {}).get('avg_impact_pct', 0.1)
        liquidity_score = max(0, 1 - avg_impact_100k * 20)  # Inverse of impact
        
        return {
            'impact_estimates': impact_estimates,
            'liquidity_score': liquidity_score,
            'market_impact_quality': 'excellent' if liquidity_score > 0.8 else 'good' if liquidity_score > 0.6 else 'poor'
        }
        
    def _calculate_order_impact(self, orders, size_usd, side):
        """Calculate market impact for specific order size"""
        remaining_size = size_usd
        total_cost = 0
        
        for price_str, volume_str in orders:
            price = float(price_str)
            volume = float(volume_str)
            
            order_value = price * volume
            
            if remaining_size <= order_value:
                # Partial fill of this order
                shares_needed = remaining_size / price
                total_cost += remaining_size
                remaining_size = 0
                break
            else:
                # Full fill of this order
                total_cost += order_value
                remaining_size -= order_value
                
        if remaining_size > 0:
            # Couldn't fill the entire order
            return 0.5  # 50% impact as penalty
            
        # Calculate average execution price
        avg_execution_price = total_cost / (size_usd / (total_cost / size_usd))
        
        # Impact as percentage difference from best price
        best_price = float(orders[0][0])
        impact_pct = abs(avg_execution_price - best_price) / best_price
        
        return impact_pct
        
    def _detect_order_book_anomalies(self, symbol, order_book):
        """Detect unusual patterns in order book"""
        anomalies = []
        
        # 1. Unusually large orders
        all_orders = order_book['bids'][:10] + order_book['asks'][:10]
        order_values = [float(order[0]) * float(order[1]) for order in all_orders]
        
        if order_values:
            avg_order_value = np.mean(order_values)
            large_orders = [v for v in order_values if v > avg_order_value * 5]
            
            if large_orders:
                anomalies.append({
                    'type': 'unusually_large_orders',
                    'count': len(large_orders),
                    'max_size': max(large_orders),
                    'avg_size': np.mean(large_orders),
                    'significance': 'high' if len(large_orders) > 3 else 'medium'
                })
                
        # 2. Extreme spread
        if order_book['bids'] and order_book['asks']:
            spread = float(order_book['asks'][0][0]) - float(order_book['bids'][0][0])
            mid_price = (float(order_book['bids'][0][0]) + float(order_book['asks'][0][0])) / 2
            spread_pct = spread / mid_price
            
            if spread_pct > 0.01:  # 1%+ spread
                anomalies.append({
                    'type': 'extreme_spread',
                    'spread_pct': spread_pct,
                    'significance': 'high' if spread_pct > 0.02 else 'medium'
                })
                
        # 3. Empty levels
        bid_gaps = self._find_empty_price_levels(order_book['bids'])
        ask_gaps = self._find_empty_price_levels(order_book['asks'])
        
        if bid_gaps or ask_gaps:
            anomalies.append({
                'type': 'empty_price_levels',
                'bid_gaps': len(bid_gaps),
                'ask_gaps': len(ask_gaps),
                'total_gaps': len(bid_gaps) + len(ask_gaps),
                'significance': 'high' if len(bid_gaps) + len(ask_gaps) > 5 else 'medium'
            })
            
        return anomalies
        
    def _find_empty_price_levels(self, orders):
        """Find gaps in price levels"""
        if len(orders) < 2:
            return []
            
        gaps = []
        for i in range(len(orders) - 1):
            current_price = float(orders[i][0])
            next_price = float(orders[i + 1][0])
            
            # Expected price difference (assume minimum tick)
            expected_diff = current_price * 0.0001  # 0.01% minimum
            actual_diff = abs(current_price - next_price)
            
            if actual_diff > expected_diff * 3:  # 3x larger than expected
                gaps.append({
                    'start_price': current_price,
                    'end_price': next_price,
                    'gap_size': actual_diff,
                    'gap_pct': actual_diff / current_price
                })
                
        return gaps
        
    def _detect_disappearing_orders(self, snapshots):
        """Detect large orders that disappear without being filled"""
        if len(snapshots) < 3:
            return []
            
        disappearing_orders = []
        
        # Compare consecutive snapshots
        for i in range(len(snapshots) - 1):
            prev_snapshot = snapshots[i]
            curr_snapshot = snapshots[i + 1]
            
            # Check for large orders in previous snapshot that are gone
            for prev_order in prev_snapshot['bids'][:5] + prev_snapshot['asks'][:5]:
                prev_price, prev_size = float(prev_order[0]), float(prev_order[1])
                prev_value = prev_price * prev_size
                
                if prev_value > 100000:  # $100K+ orders
                    # Check if this order still exists
                    order_exists = False
                    curr_orders = curr_snapshot['bids'][:10] + curr_snapshot['asks'][:10]
                    
                    for curr_order in curr_orders:
                        curr_price, curr_size = float(curr_order[0]), float(curr_order[1])
 
                        if (abs(curr_price - prev_price) < prev_price * 0.001 and  # Same price level
                            curr_size >= prev_size * 0.8):  # Most of the size still there
                            order_exists = True
                            break
                            
                    if not order_exists:
                        disappearing_orders.append({
                            'timestamp': curr_snapshot['timestamp'],
                            'price': prev_price,
                            'size': prev_size,
                            'value_usd': prev_value,
                            'side': 'bid' if prev_order in prev_snapshot['bids'] else 'ask'
                        })
                        
        return disappearing_orders
        
    def _detect_iceberg_spoofing(self, snapshots):
        """Detect fake iceberg patterns used for spoofing"""
        if len(snapshots) < 5:
            return []
            
        iceberg_patterns = []
        
        # Track order refreshing at same price levels
        price_level_activity = defaultdict(list)
        
        for snapshot in snapshots:
            for order in snapshot['bids'][:5] + snapshot['asks'][:5]:
                price, size = float(order[0]), float(order[1])
                value = price * size
                
                if value > 50000:  # $50K+ orders
                    price_key = round(price, 6)  # Round to prevent floating point issues
                    price_level_activity[price_key].append({
                        'timestamp': snapshot['timestamp'],
                        'size': size,
                        'value': value
                    })
                    
        # Analyze each price level for iceberg spoofing
        for price, activities in price_level_activity.items():
            if len(activities) >= 3:  # Multiple appearances
                # Check for size variations (iceberg refills)
                sizes = [a['size'] for a in activities]
                size_variations = len(set(sizes)) > 1
                
                # Check for sudden disappearance
                time_gaps = [activities[i+1]['timestamp'] - activities[i]['timestamp'] 
                           for i in range(len(activities)-1)]
                sudden_disappearance = any(gap > 300 for gap in time_gaps)  # 5+ minute gaps
                
                if size_variations and sudden_disappearance:
                    iceberg_patterns.append({
                        'price_level': price,
                        'appearances': len(activities),
                        'size_variations': len(set(sizes)),
                        'total_value': sum(a['value'] for a in activities),
                        'pattern_type': 'fake_iceberg'
                    })
                    
        return iceberg_patterns
        
    def _detect_layering(self, snapshots):
        """Detect order layering manipulation"""
        if len(snapshots) < 3:
            return []
            
        layering_patterns = []
        
        # Look for multiple large orders at similar price levels
        for snapshot in snapshots:
            # Analyze bid side
            bid_layers = self._analyze_order_layers(snapshot['bids'])
            if bid_layers['suspicious']:
                layering_patterns.append({
                    'timestamp': snapshot['timestamp'],
                    'side': 'bid',
                    'layer_count': bid_layers['layer_count'],
                    'total_value': bid_layers['total_value'],
                    'price_range': bid_layers['price_range']
                })
                
            # Analyze ask side
            ask_layers = self._analyze_order_layers(snapshot['asks'])
            if ask_layers['suspicious']:
                layering_patterns.append({
                    'timestamp': snapshot['timestamp'],
                    'side': 'ask',
                    'layer_count': ask_layers['layer_count'],
                    'total_value': ask_layers['total_value'],
                    'price_range': ask_layers['price_range']
                })
                
        return layering_patterns
        
    def _analyze_order_layers(self, orders):
        """Analyze orders for layering patterns"""
        if len(orders) < 5:
            return {'suspicious': False}
            
        large_orders = []
        for i, order in enumerate(orders[:10]):
            price, size = float(order[0]), float(order[1])
            value = price * size
            
            if value > 100000:  # $100K+ orders
                large_orders.append({
                    'price': price,
                    'size': size,
                    'value': value,
                    'depth_level': i
                })
                
        if len(large_orders) < 3:
            return {'suspicious': False}
            
        # Check if orders are layered (similar sizes, evenly spaced)
        prices = [o['price'] for o in large_orders]
        values = [o['value'] for o in large_orders]
        
        # Price spacing consistency
        price_gaps = [abs(prices[i] - prices[i+1]) for i in range(len(prices)-1)]
        avg_gap = np.mean(price_gaps)
        gap_consistency = np.std(price_gaps) / avg_gap if avg_gap > 0 else 1
        
        # Size similarity
        size_consistency = np.std(values) / np.mean(values) if values else 1
        
        # Suspicious if consistent spacing and similar sizes
        suspicious = (gap_consistency < 0.3 and size_consistency < 0.5 and len(large_orders) >= 3)
        
        return {
            'suspicious': suspicious,
            'layer_count': len(large_orders),
            'total_value': sum(values),
            'price_range': max(prices) - min(prices) if prices else 0,
            'consistency_score': 1 - (gap_consistency + size_consistency) / 2
        }
        
    def _detect_quote_stuffing(self, snapshots):
        """Detect quote stuffing patterns"""
        if len(snapshots) < 10:
            return []
            
        quote_stuffing = []
        
        # Calculate order book update frequency
        time_intervals = [
            snapshots[i+1]['timestamp'] - snapshots[i]['timestamp']
            for i in range(len(snapshots)-1)
        ]
        
        avg_interval = np.mean(time_intervals)
        
        # Count rapid order changes
        rapid_changes = 0
        for i in range(len(snapshots)-1):
            curr_snapshot = snapshots[i]
            next_snapshot = snapshots[i+1]
            
            # Count order changes
            curr_orders = set((o[0], o[1]) for o in curr_snapshot['bids'][:5] + curr_snapshot['asks'][:5])
            next_orders = set((o[0], o[1]) for o in next_snapshot['bids'][:5] + next_snapshot['asks'][:5])
            
            changes = len(curr_orders.symmetric_difference(next_orders))
            
            if changes > 8:  # More than 8 order changes
                rapid_changes += 1
                
        # Quote stuffing if high frequency of changes
        if rapid_changes > len(snapshots) * 0.3:  # 30%+ rapid changes
            quote_stuffing.append({
                'detection_period': len(snapshots),
                'rapid_change_count': rapid_changes,
                'avg_update_interval': avg_interval,
                'stuffing_intensity': rapid_changes / len(snapshots)
            })
            
        return quote_stuffing
        
    def _calculate_spoofing_confidence(self, disappearing_orders):
        """Calculate confidence score for spoofing detection"""
        if not disappearing_orders:
            return 0
            
        # More disappearing orders = higher confidence
        count_score = min(len(disappearing_orders) / 5, 1.0)
        
        # Larger orders = higher confidence
        avg_value = np.mean([o['value_usd'] for o in disappearing_orders])
        size_score = min(avg_value / 500000, 1.0)  # Max confidence at $500K avg
        
        # Recent disappearances = higher confidence
        recent_count = len([o for o in disappearing_orders 
                          if time.time() - o['timestamp'] < 300])
        recency_score = min(recent_count / 3, 1.0)
        
        return (count_score * 0.4 + size_score * 0.4 + recency_score * 0.2)
        
    def _calculate_side_liquidity(self, orders, current_price, side, levels):
        """Calculate liquidity distribution for one side of order book"""
        liquidity_levels = []
        
        for i, order in enumerate(orders[:levels]):
            price, size = float(order[0]), float(order[1])
            value = price * size
            
            distance_pct = abs(price - current_price) / current_price
            
            liquidity_levels.append({
                'level': i,
                'price': price,
                'size': size,
                'liquidity_usd': value,
                'distance_from_mid_pct': distance_pct,
                'cumulative_liquidity': sum(float(o[0]) * float(o[1]) for o in orders[:i+1])
            })
            
        return liquidity_levels
        
    def _identify_liquidity_gaps(self, bid_liquidity, ask_liquidity):
        """Identify significant gaps in liquidity"""
        gaps = []
        
        # Find bid side gaps
        for i in range(len(bid_liquidity) - 1):
            current_level = bid_liquidity[i]
            next_level = bid_liquidity[i + 1]
            
            liquidity_drop = (current_level['liquidity_usd'] - next_level['liquidity_usd']) / current_level['liquidity_usd']
            price_gap = (current_level['price'] - next_level['price']) / current_level['price']
            
            if liquidity_drop > 0.5 and price_gap > 0.01:  # 50% liquidity drop + 1% price gap
                gaps.append({
                    'side': 'bid',
                    'gap_price': (current_level['price'] + next_level['price']) / 2,
                    'gap_size_pct': price_gap,
                    'liquidity_drop_pct': liquidity_drop,
                    'level': i
                })
                
        # Find ask side gaps
        for i in range(len(ask_liquidity) - 1):
            current_level = ask_liquidity[i]
            next_level = ask_liquidity[i + 1]
            
            liquidity_drop = (current_level['liquidity_usd'] - next_level['liquidity_usd']) / current_level['liquidity_usd']
            price_gap = (next_level['price'] - current_level['price']) / current_level['price']
            
            if liquidity_drop > 0.5 and price_gap > 0.01:
                gaps.append({
                    'side': 'ask',
                    'gap_price': (current_level['price'] + next_level['price']) / 2,
                    'gap_size_pct': price_gap,
                    'liquidity_drop_pct': liquidity_drop,
                    'level': i
                })
                
        return gaps
        
    def _analyze_liquidity_concentration(self, bid_liquidity, ask_liquidity):
        """Analyze how concentrated liquidity is"""
        all_liquidity = bid_liquidity + ask_liquidity
        
        if not all_liquidity:
            return {}
            
        # Calculate Gini coefficient for liquidity distribution
        liquidity_values = [level['liquidity_usd'] for level in all_liquidity]
        liquidity_values.sort()
        
        n = len(liquidity_values)
        gini = (2 * sum((i + 1) * value for i, value in enumerate(liquidity_values))) / (n * sum(liquidity_values)) - (n + 1) / n
        
        # Top 5 levels concentration
        top_5_liquidity = sum(sorted([l['liquidity_usd'] for l in all_liquidity], reverse=True)[:5])
        total_liquidity = sum(l['liquidity_usd'] for l in all_liquidity)
        top_5_concentration = top_5_liquidity / total_liquidity if total_liquidity > 0 else 0
        
        return {
            'gini_coefficient': gini,
            'top_5_concentration': top_5_concentration,
            'concentration_level': 'high' if gini > 0.6 else 'medium' if gini > 0.4 else 'low',
            'distribution_quality': 'poor' if top_5_concentration > 0.8 else 'good'
        }
        
    def _calculate_liquidity_quality(self, total_bid, total_ask, gaps):
        """Calculate overall liquidity quality score"""
        # Base score from total liquidity
        total_liquidity = total_bid + total_ask
        liquidity_score = min(total_liquidity / self.liquidity_threshold, 1.0)
        
        # Balance score
        if total_bid > 0 and total_ask > 0:
            balance_ratio = min(total_bid, total_ask) / max(total_bid, total_ask)
            balance_score = balance_ratio
        else:
            balance_score = 0
            
        # Gap penalty
        gap_penalty = min(len(gaps) * 0.1, 0.3)  # Max 30% penalty
        
        final_score = (liquidity_score * 0.5 + balance_score * 0.3) * (1 - gap_penalty)
        
        return max(min(final_score, 1.0), 0.0)
        
    def _analyze_spread_consistency(self, order_book_history):
        """Analyze spread consistency for market maker detection"""
        if len(order_book_history) < 10:
            return {'is_consistent': False}
            
        spreads = []
        for snapshot in order_book_history:
            if 'spread' in snapshot:
                spreads.append(snapshot['spread'])
                
        if not spreads:
            return {'is_consistent': False}
            
        avg_spread = np.mean(spreads)
        spread_std = np.std(spreads)
        
        # Consistent if standard deviation is small relative to mean
        consistency_ratio = spread_std / avg_spread if avg_spread > 0 else 1
        is_consistent = consistency_ratio < 0.2  # Less than 20% variation
        
        # Convert to basis points
        if order_book_history:
            mid_price = order_book_history[-1].get('mid_price', 1)
            avg_spread_bps = (avg_spread / mid_price) * 10000
        else:
            avg_spread_bps = 0
            
        return {
            'is_consistent': is_consistent,
            'avg_spread_bps': avg_spread_bps,
            'consistency_score': max(0, 1 - consistency_ratio * 5),
            'spread_std': spread_std
        }
        
    def _detect_inventory_management(self, trade_history):
        """Detect market maker inventory management patterns"""
        if len(trade_history) < 20:
            return {'detected': False}
            
        # Analyze trade flow balance
        buy_trades = [t for t in trade_history if not t.get('is_buyer_maker', True)]
        sell_trades = [t for t in trade_history if t.get('is_buyer_maker', True)]
        
        if not buy_trades or not sell_trades:
            return {'detected': False}
            
        # Calculate volume balance
        buy_volume = sum(t['quantity'] for t in buy_trades)
        sell_volume = sum(t['quantity'] for t in sell_trades)
        
        volume_balance = abs(buy_volume - sell_volume) / (buy_volume + sell_volume)
        
        # MM typically maintains balanced inventory
        balanced_inventory = volume_balance < 0.2  # Within 20%
        
        # Check for inventory adjustment patterns
        trade_times = sorted([t['timestamp'] for t in trade_history])
        
        # Look for alternating buy/sell patterns
        alternating_pattern = 0
        prev_side = None
        
        for trade in sorted(trade_history, key=lambda x: x['timestamp']):
            current_side = 'buy' if not trade.get('is_buyer_maker', True) else 'sell'
            
            if prev_side and current_side != prev_side:
                alternating_pattern += 1
                
            prev_side = current_side
            
        alternating_ratio = alternating_pattern / len(trade_history)
        
        # MM shows balanced trading and some alternation
        detected = balanced_inventory and alternating_ratio > 0.3
        
        return {
            'detected': detected,
            'volume_balance': 1 - volume_balance,  # Higher is better
            'alternating_ratio': alternating_ratio,
            'confidence': 0.7 if detected else 0.3
        }
        
    def _detect_order_refreshing(self, order_book_history):
        """Detect market maker order refreshing patterns"""
        if len(order_book_history) < 5:
            return {'detected': False}
            
        # Track order persistence at top levels
        refresh_events = 0
        total_comparisons = 0
        
        for i in range(len(order_book_history) - 1):
            curr_snapshot = order_book_history[i]
            next_snapshot = order_book_history[i + 1]
            
            # Check top 3 bid/ask levels
            for side in ['bids', 'asks']:
                for level in range(min(3, len(curr_snapshot.get(side, [])))):
                    if level < len(next_snapshot.get(side, [])):
                        curr_order = curr_snapshot[side][level]
                        next_order = next_snapshot[side][level]
                        
                        curr_price, curr_size = float(curr_order[0]), float(curr_order[1])
                        next_price, next_size = float(next_order[0]), float(next_order[1])
                        
                        # Same price but different size = potential refresh
                        if (abs(curr_price - next_price) < curr_price * 0.001 and
                            abs(curr_size - next_size) > curr_size * 0.1):
                            refresh_events += 1
                            
                        total_comparisons += 1
                        
        refresh_rate = refresh_events / total_comparisons if total_comparisons > 0 else 0
        
        # MM typically shows frequent order refreshing
        detected = refresh_rate > 0.15  # 15%+ refresh rate
        
        return {
            'detected': detected,
            'refresh_rate': refresh_rate,
            'refresh_events': refresh_events,
            'confidence': min(refresh_rate * 5, 1.0)
        }
        
    def _detect_adverse_selection_avoidance(self, order_book_history, trade_history):
        """Detect MM adverse selection avoidance"""
        if len(order_book_history) < 5 or len(trade_history) < 10:
            return {'detected': False}
            
        # Look for order cancellations before large trades
        avoidance_events = 0
        
        for trade in trade_history[-20:]:  # Recent trades
            trade_time = trade['timestamp']
            trade_size = trade['quantity'] * trade['price']
            
            if trade_size > 50000:  # Large trade ($50K+)
                # Check if orders were pulled before this trade
                pre_trade_snapshots = [
                    s for s in order_book_history 
                    if trade_time - 60 < s['timestamp'] < trade_time
                ]
                
                if len(pre_trade_snapshots) >= 2:
                    # Check for liquidity reduction before trade
                    before_snapshot = pre_trade_snapshots[-2]
                    just_before_snapshot = pre_trade_snapshots[-1]
                    
                    side_to_check = 'asks' if not trade.get('is_buyer_maker', True) else 'bids'
                    
                    if (len(before_snapshot.get(side_to_check, [])) > 0 and
                        len(just_before_snapshot.get(side_to_check, [])) > 0):
                        
                        before_liquidity = sum(float(o[0]) * float(o[1]) 
                                             for o in before_snapshot[side_to_check][:3])
                        after_liquidity = sum(float(o[0]) * float(o[1]) 
                                            for o in just_before_snapshot[side_to_check][:3])
                        
                        if after_liquidity < before_liquidity * 0.7:  # 30%+ reduction
                            avoidance_events += 1
                            
        avoidance_rate = avoidance_events / min(len(trade_history), 20)
        detected = avoidance_rate > 0.1  # 10%+ avoidance rate
        
        return {
            'detected': detected,
            'avoidance_rate': avoidance_rate,
            'avoidance_events': avoidance_events,
            'confidence': min(avoidance_rate * 8, 1.0)
        }
        
    def _calculate_market_maker_score(self, mm_patterns):
        """Calculate overall market maker presence score"""
        if not mm_patterns:
            return 0
            
        score = 0
        weight_sum = 0
        
        pattern_weights = {
            'tight_spread_maintenance': 0.3,
            'inventory_management': 0.25,
            'order_refreshing': 0.25,
            'adverse_selection_avoidance': 0.2
        }
        
        for pattern_name, weight in pattern_weights.items():
            if pattern_name in mm_patterns:
                pattern_confidence = mm_patterns[pattern_name].get('confidence', 0)
                score += pattern_confidence * weight
                weight_sum += weight
                
        return score / weight_sum if weight_sum > 0 else 0
        
    def _assess_liquidity_quality(self, mm_patterns):
        """Assess liquidity provision quality from MM patterns"""
        if not mm_patterns:
            return 'unknown'
            
        # High quality if MM maintains tight spreads and balanced inventory
        spread_quality = mm_patterns.get('tight_spread_maintenance', {}).get('detected', False)
        inventory_quality = mm_patterns.get('inventory_management', {}).get('detected', False)
        
        if spread_quality and inventory_quality:
            return 'high'
        elif spread_quality or inventory_quality:
            return 'medium'
        else:
            return 'low'
            
    def _determine_anti_spoofing_direction(self, alert):
        """Determine direction for anti-spoofing signal"""
        pattern_type = alert['type']
        
        if pattern_type == 'disappearing_large_orders':
            # If large bid orders disappear, price likely to drop
            pattern = alert['pattern']
            bid_disappearances = len([p for p in pattern if p['side'] == 'bid'])
            ask_disappearances = len([p for p in pattern if p['side'] == 'ask'])
            
            if bid_disappearances > ask_disappearances:
                return 'short'  # Fake bid support removed
            else:
                return 'long'   # Fake ask resistance removed
                
        elif pattern_type == 'iceberg_manipulation':
            return 'long'  # Generally bullish when fake sells removed
            
        elif pattern_type == 'order_layering':
            return 'short'  # Layering usually creates fake support
            
        else:
            return 'long'   # Default bullish on manipulation removal
            
    def _score_microstructure_signals(self, signals):
        """Score microstructure signals by quality and reliability"""
        for signal in signals:
            base_score = signal.get('confidence', 0.5)
            
            # Boost scores based on signal type reliability
            type_multipliers = {
                'liquidity_imbalance': 1.1,
                'anti_spoofing': 1.3,
                'low_market_impact': 0.9,
                'market_maker_stability': 0.8,
                'liquidity_gap_fill': 1.2
            }
            
            signal_type = signal.get('type', 'unknown')
            multiplier = type_multipliers.get(signal_type, 1.0)
            
            # Time horizon adjustment
            time_horizon = signal.get('time_horizon', 'medium')
            time_multipliers = {
                'very_short': 1.2,  # Higher confidence for immediate signals
                'short': 1.1,
                'medium': 1.0,
                'long': 0.9
            }
            
            time_multiplier = time_multipliers.get(time_horizon, 1.0)
            
            final_score = base_score * multiplier * time_multiplier
            signal['final_score'] = min(final_score, 1.0)
            
        return signals

def initialize_orderbook_analysis_system(config, binance_client, memory_manager):
    """Initialize complete order book analysis system"""
    try:
        # Initialize order book analysis system
        orderbook_system = OrderBookAnalysisSystem(config, binance_client, memory_manager)
        
        logger = logging.getLogger('MonsterBot.OrderBookAnalysis')
        logger.info("üìä Order book analysis system initialized")
        logger.info("   üîç Depth analysis: 50 levels")
        logger.info("   üö® Spoofing detection: Active")
        logger.info("   üíß Liquidity analysis: $1M threshold")
        logger.info("   ü§ñ Market maker detection: Active")
        logger.info("   ‚ö° Microstructure signals: Active")
        
        return {
            'orderbook_system': orderbook_system,
            'analysis_depth': 50,
            'spoofing_detection': True,
            'liquidity_threshold': 1000000,
            'market_maker_detection': True
        }
        
    except Exception as e:
        logger = logging.getLogger('MonsterBot.OrderBookAnalysis')
        logger.error(f"Order book analysis system initialization failed: {e}")
        raise    
        
class PositionManagementSystem:
    """Advanced position management with dynamic sizing and portfolio optimization"""
    
    def __init__(self, config, binance_client, memory_manager):
        self.config = config
        self.binance_client = binance_client
        self.memory_manager = memory_manager
        self.logger = logging.getLogger('MonsterBot.PositionManagement')
        
        # Position tracking
        self.active_positions = {}
        self.position_history = []
        self.portfolio_state = {
            'total_value': 0,
            'free_margin': 0,
            'used_margin': 0,
            'unrealized_pnl': 0,
            'daily_pnl': 0
        }
        
        # Risk management parameters
        self.max_portfolio_risk = 0.15  # 15% portfolio VAR
        self.max_single_position_risk = 0.08  # 8% max per position
        self.correlation_limit = 0.7
        self.max_drawdown_limit = 0.25
        
        # Position sizing models
        self.sizing_models = {
            'kelly': self._kelly_criterion_sizing,
            'fixed_fractional': self._fixed_fractional_sizing,
            'volatility_adjusted': self._volatility_adjusted_sizing,
            'confidence_based': self._confidence_based_sizing,
            'momentum_based': self._momentum_based_sizing
        }
        
        # Dynamic parameters
        self.volatility_lookback = 20
        self.correlation_lookback = 50
        self.performance_lookback = 100
        
        # Portfolio analytics
        self.portfolio_metrics = defaultdict(deque)
        self.risk_metrics = defaultdict(deque)
        self.correlation_matrix = {}
        
        # Position scaling rules
        self.scaling_rules = {
            'add_to_winners': True,
            'scale_out_losers': True,
            'pyramid_max_adds': 3,
            'add_threshold': 0.02,  # 2% profit before adding
            'scale_threshold': -0.015  # 1.5% loss before scaling
        }
        
    def calculate_optimal_position_size(self, signal, market_data, current_portfolio):
        """Calculate optimal position size using multiple models"""
        try:
            symbol = signal['symbol']
            direction = signal['direction']
            confidence = signal.get('confidence', 0.5)
            estimated_move = signal.get('estimated_move', 0.05)
            
            # Get current account balance
            account_balance = self.binance_client.get_account_balance()
            current_price = self.binance_client.get_current_price(symbol)
            
            if not current_price:
                self.logger.error(f"Cannot get price for {symbol}")
                return None
                
            # Calculate volatility for risk adjustment
            volatility = self._calculate_symbol_volatility(symbol, market_data)
            
            # Get correlation with existing positions
            correlation_risk = self._calculate_correlation_risk(symbol, current_portfolio)
            
            # Calculate sizes using different models
            sizing_results = {}
            
            for model_name, model_func in self.sizing_models.items():
                try:
                    size = model_func(
                        signal, account_balance, current_price, 
                        volatility, correlation_risk, market_data
                    )
                    sizing_results[model_name] = size
                except Exception as e:
                    self.logger.error(f"Sizing model {model_name} failed: {e}")
                    sizing_results[model_name] = 0
                    
            # Combine sizing models with weights
            model_weights = {
                'kelly': 0.25,
                'fixed_fractional': 0.15,
                'volatility_adjusted': 0.25,
                'confidence_based': 0.20,
                'momentum_based': 0.15
            }
            
            weighted_size = sum(
                sizing_results.get(model, 0) * weight 
                for model, weight in model_weights.items()
            )
            
            # Apply risk constraints
            constrained_size = self._apply_risk_constraints(
                weighted_size, symbol, account_balance, current_portfolio, volatility
            )
            
            # Apply personality mode limits
            personality_size = self._apply_personality_limits(
                constrained_size, account_balance, confidence
            )
            
            # Final position calculation
            position_value = account_balance * personality_size
            quantity = position_value / current_price
            
            # Round to symbol precision
            quantity = self.binance_client.round_to_precision(quantity, symbol, 'quantity')
            
            return {
                'symbol': symbol,
                'direction': direction,
                'quantity': quantity,
                'position_value_usd': position_value,
                'position_size_pct': personality_size,
                'entry_price': current_price,
                'confidence': confidence,
                'estimated_move': estimated_move,
                'volatility': volatility,
                'correlation_risk': correlation_risk,
                'sizing_breakdown': sizing_results,
                'risk_metrics': {
                    'var_1d': self._calculate_position_var(position_value, volatility),
                    'max_loss_1d': position_value * volatility * 2.33,  # 99% confidence
                    'correlation_exposure': correlation_risk
                },
                'timestamp': time.time()
            }
            
        except Exception as e:
            self.logger.error(f"Position sizing calculation failed for {symbol}: {e}")
            return None
            
    def manage_active_positions(self, current_market_data, portfolio_update):
        """Manage all active positions dynamically"""
        try:
            management_actions = []
            
            # Update position values and PnL
            self._update_position_values(current_market_data)
            
            # Check each active position
            for position_id, position in self.active_positions.items():
                # Calculate current PnL
                current_pnl_pct = self._calculate_position_pnl(position, current_market_data)
                position['current_pnl_pct'] = current_pnl_pct
                
                # Check for scaling opportunities
                scaling_action = self._check_position_scaling(position, current_market_data)
                if scaling_action:
                    management_actions.append(scaling_action)
                    
                # Check for risk management actions
                risk_action = self._check_risk_management(position, current_market_data)
                if risk_action:
                    management_actions.append(risk_action)
                    
                # Check for profit taking
                profit_action = self._check_profit_taking(position, current_market_data)
                if profit_action:
                    management_actions.append(profit_action)
                    
                # Update position metrics
                self._update_position_metrics(position, current_market_data)
                
            # Portfolio-level risk management
            portfolio_actions = self._check_portfolio_risk(current_market_data)
            management_actions.extend(portfolio_actions)
            
            # Update portfolio state
            self._update_portfolio_state(current_market_data)
            
            return management_actions
            
        except Exception as e:
            self.logger.error(f"Position management failed: {e}")
            return []
            
    def add_new_position(self, position_data, entry_signal):
        """Add new position to tracking"""
        try:
            position_id = f"{position_data['symbol']}_{int(time.time())}"
            
            position = {
                'position_id': position_id,
                'symbol': position_data['symbol'],
                'direction': position_data['direction'],
                'quantity': position_data['quantity'],
                'entry_price': position_data['entry_price'],
                'current_price': position_data['entry_price'],
                'position_value_usd': position_data['position_value_usd'],
                'leverage': entry_signal.get('leverage', 1),
                'entry_time': time.time(),
                'strategy': entry_signal.get('strategy', 'unknown'),
                'confidence': entry_signal.get('confidence', 0.5),
                'estimated_move': entry_signal.get('estimated_move', 0.05),
                'targets': entry_signal.get('targets', {}),
                'stop_loss': entry_signal.get('targets', {}).get('stop_loss', 0.05),
                'max_hold_time': entry_signal.get('max_hold_time', 3600),
                'scaling_count': 0,
                'max_profit_pct': 0,
                'max_loss_pct': 0,
                'current_pnl_pct': 0,
                'realized_pnl': 0,
                'status': 'open',
                'risk_metrics': position_data.get('risk_metrics', {}),
                'entry_reason': entry_signal.get('entry_reason', 'signal'),
                'position_history': []
            }
            
            self.active_positions[position_id] = position
            
            self.logger.info(
                f"üìç New position added: {position['symbol']} {position['direction']} "
                f"${position['position_value_usd']:.0f} @ {position['entry_price']:.6f}"
            )
            
            return position_id
            
        except Exception as e:
            self.logger.error(f"Failed to add position: {e}")
            return None
            
    def close_position(self, position_id, exit_reason, current_price=None):
        """Close position and calculate final PnL"""
        try:
            if position_id not in self.active_positions:
                self.logger.error(f"Position {position_id} not found")
                return None
                
            position = self.active_positions[position_id]
            
            # Get current price if not provided
            if current_price is None:
                current_price = self.binance_client.get_current_price(position['symbol'])
                
            if not current_price:
                self.logger.error(f"Cannot get current price for {position['symbol']}")
                return None
                
            # Calculate final PnL
            final_pnl_pct = self._calculate_position_pnl_at_price(position, current_price)
            
            # Update position with final metrics
            position['exit_price'] = current_price
            position['exit_time'] = time.time()
            position['exit_reason'] = exit_reason
            position['final_pnl_pct'] = final_pnl_pct
            position['hold_time'] = position['exit_time'] - position['entry_time']
            position['status'] = 'closed'
            
            # Calculate dollar PnL
            dollar_pnl = position['position_value_usd'] * final_pnl_pct
            position['dollar_pnl'] = dollar_pnl
            
            # Add to position history
            self.position_history.append(position.copy())
            
            # Remove from active positions
            del self.active_positions[position_id]
            
            # Update portfolio metrics
            self._update_portfolio_pnl(dollar_pnl)
            
            self.logger.info(
                f"üèÅ Position closed: {position['symbol']} {position['direction']} "
                f"PnL: {final_pnl_pct:.2%} (${dollar_pnl:.2f}) - {exit_reason}"
            )
            
            return position
            
        except Exception as e:
            self.logger.error(f"Failed to close position {position_id}: {e}")
            return None
            
    def _kelly_criterion_sizing(self, signal, account_balance, current_price, volatility, correlation_risk, market_data):
        """Kelly Criterion position sizing"""
        try:
            confidence = signal.get('confidence', 0.5)
            estimated_move = abs(signal.get('estimated_move', 0.05))
            
            # Estimate win probability from confidence
            win_prob = 0.4 + (confidence * 0.4)  # 40-80% range
            
            # Estimate win/loss ratio from estimated move
            avg_win = estimated_move
            avg_loss = estimated_move * 0.5  # Assume better risk/reward
            
            # Kelly fraction
            if avg_loss > 0:
                kelly_fraction = (win_prob * avg_win - (1 - win_prob) * avg_loss) / avg_win
                kelly_fraction = max(0, min(kelly_fraction, 0.25))  # Cap at 25%
            else:
                kelly_fraction = 0.1
                
            # Adjust for volatility and correlation
            volatility_adjustment = max(0.5, 1 - volatility)
            correlation_adjustment = max(0.5, 1 - correlation_risk)
            
            adjusted_size = kelly_fraction * volatility_adjustment * correlation_adjustment
            
            return max(0.02, min(adjusted_size, 0.4))  # 2% minimum, 40% maximum
            
        except:
            return 0.1  # Default 10%
            
    def _fixed_fractional_sizing(self, signal, account_balance, current_price, volatility, correlation_risk, market_data):
        """Fixed fractional position sizing"""
        try:
            confidence = signal.get('confidence', 0.5)
            
            # Base size from confidence
            base_size = 0.05 + (confidence * 0.15)  # 5-20% range
            
            # Adjust for volatility
            if volatility > 0.1:  # High volatility
                base_size *= 0.7
            elif volatility < 0.03:  # Low volatility
                base_size *= 1.3
                
            # Adjust for correlation
            base_size *= (1 - correlation_risk * 0.5)
            
            return max(0.02, min(base_size, 0.3))
            
        except:
            return 0.1
            
    def _volatility_adjusted_sizing(self, signal, account_balance, current_price, volatility, correlation_risk, market_data):
        """Volatility-adjusted position sizing"""
        try:
            target_volatility = 0.05  # 5% target position volatility
            
            if volatility > 0:
                base_size = target_volatility / volatility
            else:
                base_size = 0.1
                
            # Scale by confidence
            confidence = signal.get('confidence', 0.5)
            confidence_adjusted = base_size * (0.5 + confidence)
            
            # Adjust for correlation
            final_size = confidence_adjusted * (1 - correlation_risk * 0.4)
            
            return max(0.02, min(final_size, 0.5))
            
        except:
            return 0.1
            
    def _confidence_based_sizing(self, signal, account_balance, current_price, volatility, correlation_risk, market_data):
        """Confidence-based position sizing"""
        try:
            confidence = signal.get('confidence', 0.5)
            estimated_move = abs(signal.get('estimated_move', 0.05))
            
            # Base size from confidence
            base_size = confidence * 0.4  # Up to 40% for highest confidence
            
            # Boost for large expected moves
            if estimated_move > 0.1:  # 10%+ expected move
                base_size *= 1.3
            elif estimated_move > 0.05:  # 5%+ expected move
                base_size *= 1.1
                
            # Adjust for volatility risk
            volatility_adjustment = max(0.6, 1 - volatility * 2)
            
            final_size = base_size * volatility_adjustment * (1 - correlation_risk * 0.3)
            
            return max(0.02, min(final_size, 0.6))
            
        except:
            return 0.1
            
    def _momentum_based_sizing(self, signal, account_balance, current_price, volatility, correlation_risk, market_data):
        """Momentum-based position sizing"""
        try:
            if len(market_data) < 20:
                return 0.1
                
            # Calculate recent momentum
            recent_returns = market_data['close'].pct_change().tail(10)
            momentum = recent_returns.mean()
            momentum_consistency = 1 - abs(recent_returns.std())
            
            direction = signal.get('direction', 'long')
            
            # Align momentum with signal direction
            if direction == 'long' and momentum > 0:
                momentum_boost = 1 + min(momentum * 10, 0.5)  # Up to 50% boost
            elif direction == 'short' and momentum < 0:
                momentum_boost = 1 + min(abs(momentum) * 10, 0.5)
            else:
                momentum_boost = 0.8  # Reduce size if against momentum
                
            base_size = 0.1 * momentum_boost * momentum_consistency
            
            # Adjust for other factors
            confidence = signal.get('confidence', 0.5)
            final_size = base_size * (0.5 + confidence) * (1 - correlation_risk * 0.3)
            
            return max(0.02, min(final_size, 0.4))
            
        except:
            return 0.1
            
    def _calculate_symbol_volatility(self, symbol, market_data):
        """Calculate symbol volatility"""
        try:
            if len(market_data) < self.volatility_lookback:
                return 0.05  # Default 5%
                
            returns = market_data['close'].pct_change().tail(self.volatility_lookback)
            volatility = returns.std() * np.sqrt(24)  # Annualized for crypto (24h)
            
            return max(volatility, 0.01)  # Minimum 1%
            
        except:
            return 0.05
            
    def _calculate_correlation_risk(self, symbol, current_portfolio):
        """Calculate correlation risk with existing positions"""
        try:
            if not self.active_positions:
                return 0.0
                
            # Get symbol correlations
            correlations = []
            
            for position in self.active_positions.values():
                existing_symbol = position['symbol']
                
                if existing_symbol != symbol:
                    # Get correlation from cache or calculate
                    correlation = self._get_symbol_correlation(symbol, existing_symbol)
                    
                    if correlation is not None:
                        # Weight by position size
                        position_weight = position['position_value_usd'] / current_portfolio.get('total_value', 1)
                        weighted_correlation = abs(correlation) * position_weight
                        correlations.append(weighted_correlation)
                        
            if correlations:
                return min(sum(correlations), 1.0)
            else:
                return 0.0
                
        except:
            return 0.0
            
    def _get_symbol_correlation(self, symbol1, symbol2):
        """Get correlation between two symbols"""
        try:
            correlation_key = f"{min(symbol1, symbol2)}_{max(symbol1, symbol2)}"
            
            # Check cache
            cached_corr = self.memory_manager.cache_get(f"correlation_{correlation_key}", 'indicators')
            if cached_corr is not None:
                return cached_corr
                
            # Calculate correlation (simplified - would use price history in production)
            # For now, return estimated correlation based on symbol similarity
            base_symbols = [s.replace('USDT', '') for s in [symbol1, symbol2]]
            
            # Same asset = high correlation
            if base_symbols[0] == base_symbols[1]:
                correlation = 1.0
            # Major pairs have some correlation
            elif all(s in ['BTC', 'ETH', 'BNB'] for s in base_symbols):
                correlation = 0.6
            # Alt coins have medium correlation
            elif any(s in ['BTC', 'ETH'] for s in base_symbols):
                correlation = 0.4
            else:
                correlation = 0.2
                
            # Cache result
            self.memory_manager.cache_set(f"correlation_{correlation_key}", correlation, 'indicators')
            
            return correlation
            
        except:
            return 0.3  # Default medium correlation
            
    def _apply_risk_constraints(self, proposed_size, symbol, account_balance, current_portfolio, volatility):
        """Apply risk management constraints to position size"""
        try:
            # Maximum single position risk
            max_single_risk = min(proposed_size, self.max_single_position_risk)
            
            # Portfolio risk constraint
            current_risk = self._calculate_current_portfolio_risk(current_portfolio)
            additional_risk = max_single_risk * volatility
            
            if current_risk + additional_risk > self.max_portfolio_risk:
                # Reduce size to stay within portfolio risk limit
                available_risk = self.max_portfolio_risk - current_risk
                risk_adjusted_size = available_risk / volatility if volatility > 0 else 0
                max_single_risk = min(max_single_risk, risk_adjusted_size)
                
            # Correlation constraint
            correlation_exposure = self._calculate_total_correlation_exposure(symbol)
            if correlation_exposure > self.correlation_limit:
                correlation_adjusted = max_single_risk * (1 - correlation_exposure + self.correlation_limit)
                max_single_risk = min(max_single_risk, correlation_adjusted)
                
            return max(0.01, max_single_risk)  # Minimum 1%
            
        except:
            return min(proposed_size, 0.05)  # Conservative fallback
            
    def _calculate_current_portfolio_risk(self, current_portfolio):
        """Calculate current portfolio risk (VAR)"""
        try:
            total_risk = 0
            
            for position in self.active_positions.values():
                position_risk = position.get('risk_metrics', {}).get('var_1d', 0)
                total_risk += position_risk
                
            portfolio_value = current_portfolio.get('total_value', 1)
            
            return total_risk / portfolio_value if portfolio_value > 0 else 0
            
        except:
            return 0.05  # Conservative estimate
            
    def _calculate_total_correlation_exposure(self, symbol):
        """Calculate total correlation exposure for symbol"""
        try:
            total_exposure = 0
            
            for position in self.active_positions.values():
                existing_symbol = position['symbol']
                correlation = self._get_symbol_correlation(symbol, existing_symbol)
                
                if correlation and correlation > 0.3:  # Only count significant correlations
                    position_weight = position['position_value_usd'] / 1000000  # Normalize
                    exposure = correlation * position_weight
                    total_exposure += exposure
                    
            return min(total_exposure, 1.0)
            
        except:
            return 0.2
            
    def _apply_personality_limits(self, proposed_size, account_balance, confidence):
        """Apply personality mode position limits"""
        try:
            # Get current personality mode
            mode_name, mode_config = get_personality_mode(account_balance, self.config)
            
            max_position_size = mode_config['max_position_size']
            
            # Apply confidence scaling within personality limits
            confidence_scaled = proposed_size * (0.5 + confidence * 0.5)
            
            return min(confidence_scaled, max_position_size)
            
        except:
            return min(proposed_size, 0.2)  # Conservative fallback
            
    def _calculate_position_var(self, position_value, volatility):
        """Calculate position Value at Risk"""
        try:
            # 95% VaR (1.65 standard deviations)
            var_95 = position_value * volatility * 1.65
            return var_95
            
        except:
            return position_value * 0.05
            
    def _update_position_values(self, current_market_data):
        """Update current values for all active positions"""
        try:
            for position in self.active_positions.values():
                symbol = position['symbol']
                
                # Get current price
                current_price = self.binance_client.get_current_price(symbol)
                if current_price:
                    position['current_price'] = current_price
                    
                    # Update PnL tracking
                    current_pnl = self._calculate_position_pnl(position, current_market_data)
                    position['current_pnl_pct'] = current_pnl
                    
                    # Update max profit/loss tracking
                    position['max_profit_pct'] = max(position.get('max_profit_pct', 0), current_pnl)
                    position['max_loss_pct'] = min(position.get('max_loss_pct', 0), current_pnl)
                    
        except Exception as e:
            self.logger.error(f"Failed to update position values: {e}")
            
    def _calculate_position_pnl(self, position, current_market_data):
        """Calculate current PnL for position"""
        try:
            current_price = position.get('current_price', position['entry_price'])
            return self._calculate_position_pnl_at_price(position, current_price)
            
        except:
            return 0.0
            
    def _calculate_position_pnl_at_price(self, position, price):
        """Calculate PnL at specific price"""
        try:
            entry_price = position['entry_price']
            direction = position['direction']
            
            if direction == 'long':
                pnl_pct = (price - entry_price) / entry_price
            else:  # short
                pnl_pct = (entry_price - price) / entry_price
                
            # Apply leverage
            leverage = position.get('leverage', 1)
            return pnl_pct * leverage
            
        except:
            return 0.0
            
    def _check_position_scaling(self, position, current_market_data):
        """Check if position should be scaled (add to winners)"""
        try:
            if not self.scaling_rules['add_to_winners']:
                return None
                
            current_pnl = position['current_pnl_pct']
            scaling_count = position.get('scaling_count', 0)
            
            # Only add to profitable positions
            if (current_pnl > self.scaling_rules['add_threshold'] and
                scaling_count < self.scaling_rules['pyramid_max_adds']):
                
                # Calculate scaling size (smaller than original)
                original_size = position['position_value_usd']
                scale_size = original_size * 0.5  # 50% of original
                
                return {
                    'action': 'scale_position',
                    'position_id': position['position_id'],
                    'symbol': position['symbol'],
                    'direction': position['direction'],
                    'scale_type': 'add_to_winner',
                    'scale_size_usd': scale_size,
                    'current_pnl': current_pnl,
                    'scaling_count': scaling_count + 1,
                    'reason': f"Adding to winner at {current_pnl:.1%} profit"
                }
                
            return None
            
        except Exception as e:
            self.logger.error(f"Position scaling check failed: {e}")
            return None
            
    def _check_risk_management(self, position, current_market_data):
        """Check for risk management actions"""
        try:
            current_pnl = position['current_pnl_pct']
            stop_loss = position.get('stop_loss', 0.05)
            hold_time = time.time() - position['entry_time']
            max_hold_time = position.get('max_hold_time', 3600)
            
            # Stop loss check
            if current_pnl <= -stop_loss:
                return {
                    'action': 'close_position',
                    'position_id': position['position_id'],
                    'symbol': position['symbol'],
                    'close_type': 'stop_loss',
                    'current_pnl': current_pnl,
                    'reason': f"Stop loss triggered at {current_pnl:.1%}"
                }
                
            # Time-based exit
            if hold_time > max_hold_time:
                return {
                    'action': 'close_position',
                    'position_id': position['position_id'],
                    'symbol': position['symbol'],
                    'close_type': 'time_exit',
                    'current_pnl': current_pnl,
                    'hold_time': hold_time,
                    'reason': f"Max hold time exceeded ({hold_time/3600:.1f}h)"
                }
                
            # Scale out of losers
            if (self.scaling_rules['scale_out_losers'] and
                current_pnl <= self.scaling_rules['scale_threshold']):
                
                return {
                    'action': 'scale_position',
                    'position_id': position['position_id'],
                    'symbol': position['symbol'],
                    'scale_type': 'reduce_loser',
                    'scale_pct': 0.5,  # Reduce by 50%
                    'current_pnl': current_pnl,
                    'reason': f"Scaling out of loser at {current_pnl:.1%}"
                }
                
            return None
            
        except Exception as e:
            self.logger.error(f"Risk management check failed: {e}")
            return None
            
    def _check_profit_taking(self, position, current_market_data):
        """Check for profit taking opportunities"""
        try:
            current_pnl = position['current_pnl_pct']
            targets = position.get('targets', {})
            
            # Check target levels
            for target_name, target_data in targets.items():
                if target_name.startswith('tp'):  # Take profit levels
                    target_level = target_data.get('level', 0)
                    target_size = target_data.get('size', 0)
                    
                    if (current_pnl >= target_level and 
                        not position.get(f'{target_name}_hit', False)):
                        
                        position[f'{target_name}_hit'] = True
                        
                        return {
                            'action': 'partial_close',
                            'position_id': position['position_id'],
                            'symbol': position['symbol'],
                            'close_pct': target_size,
                            'target_level': target_level,
                            'current_pnl': current_pnl,
                            'reason': f"Take profit {target_name} hit at {current_pnl:.1%}"
                        }
                        
            return None
            
        except Exception as e:
            self.logger.error(f"Profit taking check failed: {e}")
            return None
            
	def _check_portfolio_risk(self, current_market_data):
        """Check portfolio-level risk and generate actions"""
        try:
            actions = []
            
            # Calculate current portfolio risk
            portfolio_risk = self._calculate_current_portfolio_risk({'total_value': 1000000})  # Placeholder
            
            # Emergency risk reduction if over limit
            if portfolio_risk > self.max_portfolio_risk * 1.2:  # 20% over limit
                # Find largest losing positions to close
                losing_positions = [
                    (pos_id, pos) for pos_id, pos in self.active_positions.items()
                    if pos.get('current_pnl_pct', 0) < -0.02  # 2%+ loss
                ]
                
                # Sort by loss size
                losing_positions.sort(key=lambda x: x[1].get('current_pnl_pct', 0))
                
                # Close worst performers
                positions_to_close = losing_positions[:max(1, len(losing_positions) // 3)]
                
                for pos_id, position in positions_to_close:
                    actions.append({
                        'action': 'emergency_close',
                        'position_id': pos_id,
                        'symbol': position['symbol'],
                        'close_type': 'risk_reduction',
                        'portfolio_risk': portfolio_risk,
                        'reason': f"Emergency close - portfolio risk {portfolio_risk:.1%}"
                    })
                    
            # Check correlation risk
            correlation_violations = self._check_correlation_violations()
            actions.extend(correlation_violations)
            
            # Check drawdown limits
            drawdown_actions = self._check_drawdown_limits()
            actions.extend(drawdown_actions)
            
            return actions
            
        except Exception as e:
            self.logger.error(f"Portfolio risk check failed: {e}")
            return []
            
    def _check_correlation_violations(self):
        """Check for correlation limit violations"""
        try:
            actions = []
            
            # Group positions by correlation
            correlation_groups = defaultdict(list)
            
            for pos_id, position in self.active_positions.items():
                symbol = position['symbol']
                base_symbol = symbol.replace('USDT', '')
                
                # Group by base asset for simplicity
                correlation_groups[base_symbol].append((pos_id, position))
                
            # Check each group for over-concentration
            for base_asset, positions in correlation_groups.items():
                if len(positions) > 1:
                    total_exposure = sum(pos[1]['position_value_usd'] for pos in positions)
                    
                    # If total exposure > 40% of portfolio, reduce
                    portfolio_value = sum(pos['position_value_usd'] for pos in self.active_positions.values())
                    
                    if total_exposure / portfolio_value > 0.4:
                        # Close smallest position in group
                        smallest_position = min(positions, key=lambda x: x[1]['position_value_usd'])
                        
                        actions.append({
                            'action': 'close_position',
                            'position_id': smallest_position[0],
                            'symbol': smallest_position[1]['symbol'],
                            'close_type': 'correlation_reduction',
                            'correlation_exposure': total_exposure / portfolio_value,
                            'reason': f"Reducing {base_asset} correlation exposure"
                        })
                        
            return actions
            
        except Exception as e:
            self.logger.error(f"Correlation check failed: {e}")
            return []
            
    def _check_drawdown_limits(self):
        """Check portfolio drawdown limits"""
        try:
            actions = []
            
            # Calculate current drawdown
            current_portfolio_value = sum(pos['position_value_usd'] for pos in self.active_positions.values())
            
            if hasattr(self, 'peak_portfolio_value'):
                current_drawdown = (self.peak_portfolio_value - current_portfolio_value) / self.peak_portfolio_value
            else:
                self.peak_portfolio_value = current_portfolio_value
                current_drawdown = 0
                
            # Update peak if new high
            if current_portfolio_value > self.peak_portfolio_value:
                self.peak_portfolio_value = current_portfolio_value
                
            # If drawdown exceeds limit, reduce positions
            if current_drawdown > self.max_drawdown_limit:
                # Close all positions with losses > 5%
                losing_positions = [
                    (pos_id, pos) for pos_id, pos in self.active_positions.items()
                    if pos.get('current_pnl_pct', 0) < -0.05
                ]
                
                for pos_id, position in losing_positions:
                    actions.append({
                        'action': 'close_position',
                        'position_id': pos_id,
                        'symbol': position['symbol'],
                        'close_type': 'drawdown_protection',
                        'current_drawdown': current_drawdown,
                        'reason': f"Drawdown protection at {current_drawdown:.1%}"
                    })
                    
            return actions
            
        except Exception as e:
            self.logger.error(f"Drawdown check failed: {e}")
            return []
            
    def _update_position_metrics(self, position, current_market_data):
        """Update position performance metrics"""
        try:
            current_time = time.time()
            
            # Add current state to position history
            position_snapshot = {
                'timestamp': current_time,
                'price': position.get('current_price', position['entry_price']),
                'pnl_pct': position.get('current_pnl_pct', 0),
                'hold_time': current_time - position['entry_time']
            }
            
            if 'position_history' not in position:
                position['position_history'] = []
                
            position['position_history'].append(position_snapshot)
            
            # Keep only recent history (last 24 hours)
            cutoff_time = current_time - 86400
            position['position_history'] = [
                h for h in position['position_history'] 
                if h['timestamp'] > cutoff_time
            ]
            
            # Calculate position metrics
            if len(position['position_history']) > 1:
                # Calculate Sharpe ratio for position
                returns = []
                for i in range(1, len(position['position_history'])):
                    prev_pnl = position['position_history'][i-1]['pnl_pct']
                    curr_pnl = position['position_history'][i]['pnl_pct']
                    returns.append(curr_pnl - prev_pnl)
                    
                if len(returns) > 5:
                    avg_return = np.mean(returns)
                    return_std = np.std(returns)
                    
                    if return_std > 0:
                        position['sharpe_ratio'] = avg_return / return_std
                    else:
                        position['sharpe_ratio'] = 0
                        
                # Calculate maximum adverse excursion (MAE)
                position['mae'] = min(h['pnl_pct'] for h in position['position_history'])
                
                # Calculate maximum favorable excursion (MFE)
                position['mfe'] = max(h['pnl_pct'] for h in position['position_history'])
                
        except Exception as e:
            self.logger.error(f"Position metrics update failed: {e}")
            
    def _update_portfolio_state(self, current_market_data):
        """Update overall portfolio state"""
        try:
            # Calculate total portfolio value
            total_value = sum(pos['position_value_usd'] for pos in self.active_positions.values())
            
            # Calculate unrealized PnL
            unrealized_pnl = sum(
                pos['position_value_usd'] * pos.get('current_pnl_pct', 0)
                for pos in self.active_positions.values()
            )
            
            # Update portfolio state
            self.portfolio_state.update({
                'total_value': total_value,
                'unrealized_pnl': unrealized_pnl,
                'active_positions': len(self.active_positions),
                'last_update': time.time()
            })
            
            # Store portfolio metrics
            portfolio_metric = {
                'timestamp': time.time(),
                'total_value': total_value,
                'unrealized_pnl': unrealized_pnl,
                'position_count': len(self.active_positions),
                'portfolio_risk': self._calculate_current_portfolio_risk({'total_value': total_value})
            }
            
            self.portfolio_metrics['value'].append(portfolio_metric)
            
            # Keep only recent metrics
            if len(self.portfolio_metrics['value']) > 1000:
                self.portfolio_metrics['value'].popleft()
                
        except Exception as e:
            self.logger.error(f"Portfolio state update failed: {e}")
            
    def _update_portfolio_pnl(self, realized_pnl):
        """Update portfolio with realized PnL"""
        try:
            # Add to daily PnL
            self.portfolio_state['daily_pnl'] += realized_pnl
            
            # Update total realized PnL
            if 'total_realized_pnl' not in self.portfolio_state:
                self.portfolio_state['total_realized_pnl'] = 0
                
            self.portfolio_state['total_realized_pnl'] += realized_pnl
            
            # Store PnL event
            pnl_event = {
                'timestamp': time.time(),
                'realized_pnl': realized_pnl,
                'daily_pnl': self.portfolio_state['daily_pnl'],
                'total_pnl': self.portfolio_state['total_realized_pnl']
            }
            
            self.portfolio_metrics['pnl'].append(pnl_event)
            
            # Keep only recent PnL events
            if len(self.portfolio_metrics['pnl']) > 1000:
                self.portfolio_metrics['pnl'].popleft()
                
        except Exception as e:
            self.logger.error(f"Portfolio PnL update failed: {e}")
            
    def get_portfolio_analytics(self):
        """Get comprehensive portfolio analytics"""
        try:
            # Basic portfolio stats
            total_positions = len(self.active_positions)
            total_value = sum(pos['position_value_usd'] for pos in self.active_positions.values())
            
            # PnL analysis
            unrealized_pnl = sum(
                pos['position_value_usd'] * pos.get('current_pnl_pct', 0)
                for pos in self.active_positions.values()
            )
            
            # Position analysis
            winning_positions = len([
                pos for pos in self.active_positions.values()
                if pos.get('current_pnl_pct', 0) > 0
            ])
            
            losing_positions = total_positions - winning_positions
            
            # Risk metrics
            portfolio_var = sum(
                pos.get('risk_metrics', {}).get('var_1d', 0)
                for pos in self.active_positions.values()
            )
            
            # Historical performance
            if self.position_history:
                closed_trades = len(self.position_history)
                profitable_trades = len([
                    pos for pos in self.position_history
                    if pos.get('final_pnl_pct', 0) > 0
                ])
                
                win_rate = profitable_trades / closed_trades if closed_trades > 0 else 0
                
                avg_profit = np.mean([
                    pos.get('final_pnl_pct', 0) for pos in self.position_history
                ])
                
                avg_hold_time = np.mean([
                    pos.get('hold_time', 0) for pos in self.position_history
                ]) / 3600  # Convert to hours
                
            else:
                closed_trades = 0
                win_rate = 0
                avg_profit = 0
                avg_hold_time = 0
                
            # Symbol distribution
            symbol_distribution = defaultdict(int)
            for pos in self.active_positions.values():
                symbol_distribution[pos['symbol']] += 1
                
            # Strategy distribution
            strategy_distribution = defaultdict(int)
            for pos in self.active_positions.values():
                strategy_distribution[pos.get('strategy', 'unknown')] += 1
                
            return {
                'portfolio_overview': {
                    'total_positions': total_positions,
                    'total_value_usd': total_value,
                    'unrealized_pnl_usd': unrealized_pnl,
                    'unrealized_pnl_pct': unrealized_pnl / total_value if total_value > 0 else 0,
                    'winning_positions': winning_positions,
                    'losing_positions': losing_positions,
                    'win_rate_open': winning_positions / total_positions if total_positions > 0 else 0
                },
                'risk_metrics': {
                    'portfolio_var_1d': portfolio_var,
                    'portfolio_risk_pct': portfolio_var / total_value if total_value > 0 else 0,
                    'max_single_position_pct': max([
                        pos['position_value_usd'] / total_value
                        for pos in self.active_positions.values()
                    ]) if self.active_positions else 0,
                    'correlation_risk': self._calculate_avg_correlation_risk()
                },
                'historical_performance': {
                    'total_closed_trades': closed_trades,
                    'win_rate': win_rate,
                    'avg_profit_pct': avg_profit,
                    'avg_hold_time_hours': avg_hold_time,
                    'daily_pnl_usd': self.portfolio_state.get('daily_pnl', 0),
                    'total_realized_pnl': self.portfolio_state.get('total_realized_pnl', 0)
                },
                'distribution_analysis': {
                    'symbols': dict(symbol_distribution),
                    'strategies': dict(strategy_distribution),
                    'top_positions': self._get_top_positions(),
                    'risk_concentration': self._calculate_risk_concentration()
                },
                'performance_metrics': {
                    'sharpe_ratio': self._calculate_portfolio_sharpe(),
                    'max_drawdown': self._calculate_max_drawdown(),
                    'profit_factor': self._calculate_profit_factor(),
                    'avg_trade_duration': avg_hold_time
                }
            }
            
        except Exception as e:
            self.logger.error(f"Portfolio analytics calculation failed: {e}")
            return {}
            
    def _calculate_avg_correlation_risk(self):
        """Calculate average correlation risk across positions"""
        try:
            if len(self.active_positions) < 2:
                return 0.0
                
            symbols = [pos['symbol'] for pos in self.active_positions.values()]
            correlations = []
            
            for i in range(len(symbols)):
                for j in range(i + 1, len(symbols)):
                    correlation = self._get_symbol_correlation(symbols[i], symbols[j])
                    if correlation is not None:
                        correlations.append(abs(correlation))
                        
            return np.mean(correlations) if correlations else 0.0
            
        except:
            return 0.3
            
    def _get_top_positions(self):
        """Get top positions by value"""
        try:
            positions = list(self.active_positions.values())
            positions.sort(key=lambda x: x['position_value_usd'], reverse=True)
            
            return [
                {
                    'symbol': pos['symbol'],
                    'direction': pos['direction'],
                    'value_usd': pos['position_value_usd'],
                    'pnl_pct': pos.get('current_pnl_pct', 0),
                    'strategy': pos.get('strategy', 'unknown')
                }
                for pos in positions[:5]  # Top 5
            ]
            
        except:
            return []
            
    def _calculate_risk_concentration(self):
        """Calculate risk concentration metrics"""
        try:
            if not self.active_positions:
                return {}
                
            position_risks = [
                pos.get('risk_metrics', {}).get('var_1d', 0)
                for pos in self.active_positions.values()
            ]
            
            total_risk = sum(position_risks)
            
            if total_risk == 0:
                return {'herfindahl_index': 0, 'top_3_concentration': 0}
                
            # Calculate Herfindahl index for risk concentration
            risk_shares = [risk / total_risk for risk in position_risks]
            herfindahl = sum(share ** 2 for share in risk_shares)
            
            # Top 3 risk concentration
            top_3_risks = sorted(position_risks, reverse=True)[:3]
            top_3_concentration = sum(top_3_risks) / total_risk
            
            return {
                'herfindahl_index': herfindahl,
                'top_3_concentration': top_3_concentration,
                'risk_diversification': 'poor' if herfindahl > 0.5 else 'good'
            }
            
        except:
            return {}
            
    def _calculate_portfolio_sharpe(self):
        """Calculate portfolio Sharpe ratio"""
        try:
            if len(self.portfolio_metrics['pnl']) < 10:
                return 0
                
            # Get recent PnL changes
            pnl_events = list(self.portfolio_metrics['pnl'])[-50:]  # Last 50 events
            
            if len(pnl_events) < 2:
                return 0
                
            # Calculate returns
            returns = []
            for i in range(1, len(pnl_events)):
                prev_total = pnl_events[i-1]['total_pnl']
                curr_total = pnl_events[i]['total_pnl']
                
                if prev_total != 0:
                    return_pct = (curr_total - prev_total) / abs(prev_total)
                    returns.append(return_pct)
                    
            if len(returns) < 5:
                return 0
                
            avg_return = np.mean(returns)
            return_std = np.std(returns)
            
            if return_std > 0:
                return avg_return / return_std
            else:
                return 0
                
        except:
            return 0
            
    def _calculate_max_drawdown(self):
        """Calculate maximum drawdown"""
        try:
            if len(self.portfolio_metrics['value']) < 10:
                return 0
                
            values = [m['total_value'] for m in self.portfolio_metrics['value']]
            
            peak = values[0]
            max_dd = 0
            
            for value in values:
                if value > peak:
                    peak = value
                else:
                    drawdown = (peak - value) / peak
                    max_dd = max(max_dd, drawdown)
                    
            return max_dd
            
        except:
            return 0
            
    def _calculate_profit_factor(self):
        """Calculate profit factor"""
        try:
            if not self.position_history:
                return 1
                
            gross_profit = sum(
                pos.get('dollar_pnl', 0) for pos in self.position_history
                if pos.get('dollar_pnl', 0) > 0
            )
            
            gross_loss = abs(sum(
                pos.get('dollar_pnl', 0) for pos in self.position_history
                if pos.get('dollar_pnl', 0) < 0
            ))
            
            if gross_loss > 0:
                return gross_profit / gross_loss
            else:
                return float('inf') if gross_profit > 0 else 1
                
        except:
            return 1
            
    def get_position_recommendations(self, new_signals, current_market_data):
        """Get position recommendations based on current portfolio state"""
        try:
            recommendations = []
            
            for signal in new_signals:
                symbol = signal['symbol']
                
                # Check if we already have position in this symbol
                existing_position = None
                for pos in self.active_positions.values():
                    if pos['symbol'] == symbol:
                        existing_position = pos
                        break
                        
                if existing_position:
                    # Recommendation for existing position
                    if (existing_position['direction'] == signal['direction'] and
                        existing_position.get('current_pnl_pct', 0) > 0.02):  # 2%+ profit
                        
                        recommendations.append({
                            'type': 'add_to_position',
                            'symbol': symbol,
                            'current_position_id': existing_position['position_id'],
                            'add_size_pct': 0.3,  # 30% of original
                            'reason': 'Add to profitable position',
                            'confidence': signal.get('confidence', 0.5) * 0.8  # Lower confidence for adding
                        })
                        
                else:
                    # New position recommendation
                    position_size = self.calculate_optimal_position_size(
                        signal, current_market_data.get(symbol), self.portfolio_state
                    )
                    
                    if position_size and position_size['position_size_pct'] > 0.01:  # Minimum 1%
                        recommendations.append({
                            'type': 'new_position',
                            'symbol': symbol,
                            'direction': signal['direction'],
                            'position_size': position_size,
                            'signal_data': signal,
                            'reason': 'New trading opportunity',
                            'confidence': signal.get('confidence', 0.5)
                        })
                        
            # Sort by confidence
            recommendations.sort(key=lambda x: x.get('confidence', 0), reverse=True)
            
            return recommendations
            
        except Exception as e:
            self.logger.error(f"Position recommendations failed: {e}")
            return []

def initialize_position_management_system(config, binance_client, memory_manager):
    """Initialize complete position management system"""
    try:
        # Initialize position management system
        position_system = PositionManagementSystem(config, binance_client, memory_manager)
        
        logger = logging.getLogger('MonsterBot.PositionManagement')
        logger.info("üìä Position management system initialized")
        logger.info("   üí∞ Max portfolio risk: 15%")
        logger.info("   üéØ Max single position: 8%")
        logger.info("   üîó Correlation limit: 70%")
        logger.info("   üìâ Max drawdown limit: 25%")
        logger.info("   ‚öñÔ∏è Position sizing models: 5 active")
        logger.info("   üìà Dynamic scaling: Active")
        
        return {
            'position_system': position_system,
            'max_portfolio_risk': 0.15,
            'max_single_position_risk': 0.08,
            'correlation_limit': 0.7,
            'sizing_models': 5,
            'dynamic_scaling': True
        }
        
    except Exception as e:
        logger = logging.getLogger('MonsterBot.PositionManagement')
        logger.error(f"Position management system initialization failed: {e}")
        raise     
        
class RiskManagementEngine:
    """Advanced risk management with real-time monitoring and emergency controls"""
    
    def __init__(self, config, binance_client, memory_manager, position_system):
        self.config = config
        self.binance_client = binance_client
        self.memory_manager = memory_manager
        self.position_system = position_system
        self.logger = logging.getLogger('MonsterBot.RiskManagement')
        
        # Risk limits and parameters
        self.risk_limits = {
            'max_daily_loss_pct': config.DAILY_LOSS_LIMIT,
            'max_total_drawdown_pct': config.MAX_DRAWDOWN,
            'max_portfolio_var': 0.15,
            'max_single_position_var': 0.08,
            'max_leverage': config.MAX_LEVERAGE,
            'correlation_limit': config.POSITION_CORRELATION_LIMIT,
            'max_open_positions': config.MAX_OPEN_POSITIONS,
            'min_account_balance': 50.0  # Minimum $50 to continue trading
        }
        
        # Emergency controls
        self.emergency_state = {
            'is_active': False,
            'triggered_at': None,
            'trigger_reason': None,
            'risk_level': 'normal'  # normal, elevated, critical, emergency
        }
        
        # Risk monitoring
        self.risk_metrics = defaultdict(deque)
        self.violation_history = []
        self.circuit_breakers = {}
        
        # Black swan detection
        self.black_swan_detector = {
            'volatility_threshold': 0.2,  # 20% intraday volatility
            'price_shock_threshold': 0.15,  # 15% instant move
            'volume_spike_threshold': 10.0,  # 10x normal volume
            'correlation_breakdown_threshold': 0.3  # 30% correlation drop
        }
        
        # Risk calculation models
        self.var_models = {
            'historical': self._historical_var,
            'parametric': self._parametric_var,
            'monte_carlo': self._monte_carlo_var
        }
        
        # Dynamic stop loss system
        self.stop_loss_system = {
            'base_stop_pct': 0.03,  # 3% base stop
            'volatility_multiplier': 2.0,
            'momentum_adjustment': True,
            'time_decay_factor': 0.1,
            'trail_activation_pct': 0.02  # 2% profit before trailing
        }
        
        # Performance tracking
        self.risk_performance = {
            'stops_triggered': 0,
            'emergency_stops': 0,
            'risk_adjusted_returns': deque(maxlen=100),
            'violation_count': 0,
            'avg_drawdown_duration': 0
        }
        
        # Initialize risk monitoring
        self.initialize_risk_monitoring()
        
    def initialize_risk_monitoring(self):
        """Initialize real-time risk monitoring system"""
        try:
            # Set up circuit breakers
            self.setup_circuit_breakers()
            
            # Initialize risk calculation threads
            self.start_risk_monitoring_thread()
            
            self.logger.info("üõ°Ô∏è Risk management engine initialized")
            self.logger.info(f"   üìâ Max daily loss: {self.risk_limits['max_daily_loss_pct']:.1%}")
            self.logger.info(f"   üí• Max drawdown: {self.risk_limits['max_total_drawdown_pct']:.1%}")
            self.logger.info(f"   ‚ö° Max leverage: {self.risk_limits['max_leverage']}x")
            self.logger.info(f"   üîó Correlation limit: {self.risk_limits['correlation_limit']:.1%}")
            
        except Exception as e:
            self.logger.error(f"Risk monitoring initialization failed: {e}")
            
    def setup_circuit_breakers(self):
        """Setup trading circuit breakers"""
        self.circuit_breakers = {
            'daily_loss': {
                'threshold': self.risk_limits['max_daily_loss_pct'],
                'action': 'stop_new_trades',
                'recovery_time': 3600,  # 1 hour
                'triggered': False
            },
            'portfolio_var': {
                'threshold': self.risk_limits['max_portfolio_var'] * 1.2,  # 20% over limit
                'action': 'reduce_positions',
                'recovery_time': 1800,  # 30 minutes
                'triggered': False
            },
            'drawdown_emergency': {
                'threshold': self.risk_limits['max_total_drawdown_pct'] * 0.8,  # 80% of limit
                'action': 'emergency_close_all',
                'recovery_time': 7200,  # 2 hours
                'triggered': False
            },
            'black_swan': {
                'threshold': None,  # Dynamic
                'action': 'immediate_stop_all',
                'recovery_time': 14400,  # 4 hours
                'triggered': False
            },
            'correlation_spike': {
                'threshold': self.risk_limits['correlation_limit'] * 1.5,
                'action': 'reduce_correlated_positions',
                'recovery_time': 900,  # 15 minutes
                'triggered': False
            }
        }
        
    def start_risk_monitoring_thread(self):
        """Start background risk monitoring"""
        def risk_monitor():
            while True:
                try:
                    self.perform_risk_check()
                    time.sleep(5)  # Check every 5 seconds
                except Exception as e:
                    self.logger.error(f"Risk monitoring error: {e}")
                    time.sleep(10)
                    
        monitor_thread = threading.Thread(target=risk_monitor, daemon=True)
        monitor_thread.start()
        
    def perform_risk_check(self):
        """Comprehensive real-time risk assessment"""
        try:
            # Get current portfolio state
            portfolio_state = self.position_system.portfolio_state
            active_positions = self.position_system.active_positions
            
            # Calculate current risk metrics
            risk_assessment = self.calculate_portfolio_risk(portfolio_state, active_positions)
            
            # Check for risk limit violations
            violations = self.check_risk_violations(risk_assessment)
            
            # Check for black swan events
            black_swan_risk = self.detect_black_swan_events()
            
            # Update emergency state
            self.update_emergency_state(risk_assessment, violations, black_swan_risk)
            
            # Execute emergency actions if needed
            if violations or black_swan_risk:
                self.execute_risk_actions(violations, black_swan_risk)
                
            # Store risk metrics
            self.store_risk_metrics(risk_assessment)
            
        except Exception as e:
            self.logger.error(f"Risk check failed: {e}")
            
    def calculate_portfolio_risk(self, portfolio_state, active_positions):
        """Calculate comprehensive portfolio risk metrics"""
        try:
            total_value = portfolio_state.get('total_value', 0)
            
            if total_value == 0:
                return self._get_empty_risk_assessment()
                
            # Portfolio VAR calculation
            portfolio_var = self.calculate_portfolio_var(active_positions, total_value)
            
            # Daily loss calculation
            daily_pnl = portfolio_state.get('daily_pnl', 0)
            daily_loss_pct = daily_pnl / total_value if total_value > 0 else 0
            
            # Drawdown calculation
            current_drawdown = self.calculate_current_drawdown(portfolio_state)
            
            # Leverage analysis
            total_notional = sum(
                pos['position_value_usd'] * pos.get('leverage', 1)
                for pos in active_positions.values()
            )
            effective_leverage = total_notional / total_value if total_value > 0 else 0
            
            # Correlation risk
            correlation_risk = self.calculate_correlation_risk(active_positions)
            
            # Concentration risk
            concentration_risk = self.calculate_concentration_risk(active_positions, total_value)
            
            # Liquidity risk
            liquidity_risk = self.calculate_liquidity_risk(active_positions)
            
            # Market risk (external factors)
            market_risk = self.calculate_market_risk()
            
            return {
                'timestamp': time.time(),
                'total_portfolio_value': total_value,
                'portfolio_var_1d': portfolio_var,
                'portfolio_var_pct': portfolio_var / total_value,
                'daily_pnl': daily_pnl,
                'daily_loss_pct': daily_loss_pct,
                'current_drawdown': current_drawdown,
                'effective_leverage': effective_leverage,
                'correlation_risk': correlation_risk,
                'concentration_risk': concentration_risk,
                'liquidity_risk': liquidity_risk,
                'market_risk': market_risk,
                'position_count': len(active_positions),
                'risk_score': self.calculate_overall_risk_score(
                    portfolio_var / total_value, daily_loss_pct, current_drawdown,
                    effective_leverage, correlation_risk, concentration_risk
                )
            }
            
        except Exception as e:
            self.logger.error(f"Portfolio risk calculation failed: {e}")
            return self._get_empty_risk_assessment()
            
    def calculate_portfolio_var(self, active_positions, total_value):
        """Calculate portfolio Value at Risk using multiple models"""
        try:
            if not active_positions:
                return 0
                
            # Collect position data
            position_values = []
            position_volatilities = []
            position_correlations = []
            
            for position in active_positions.values():
                position_values.append(position['position_value_usd'])
                
                # Get or calculate volatility
                symbol = position['symbol']
                volatility = self._get_position_volatility(symbol)
                position_volatilities.append(volatility)
                
            # Calculate VAR using different models
            historical_var = self.var_models['historical'](position_values, position_volatilities)
            parametric_var = self.var_models['parametric'](position_values, position_volatilities)
            
            # Use conservative estimate (highest VAR)
            portfolio_var = max(historical_var, parametric_var)
            
            return min(portfolio_var, total_value * 0.5)  # Cap at 50% of portfolio
            
        except Exception as e:
            self.logger.error(f"Portfolio VAR calculation failed: {e}")
            return total_value * 0.1  # Conservative 10% estimate
            
    def _historical_var(self, position_values, volatilities):
        """Historical simulation VAR"""
        try:
            # Use historical returns (simplified)
            if not position_values:
                return 0
                
            # 95% VAR (1.65 standard deviations)
            total_value = sum(position_values)
            avg_volatility = np.mean(volatilities) if volatilities else 0.05
            
            var_95 = total_value * avg_volatility * 1.65
            return var_95
            
        except:
            return sum(position_values) * 0.05
            
    def _parametric_var(self, position_values, volatilities):
        """Parametric VAR assuming normal distribution"""
        try:
            if not position_values or not volatilities:
                return 0
                
            # Portfolio volatility (simplified - assumes some correlation)
            portfolio_volatility = np.sqrt(sum(
                (value * vol) ** 2 for value, vol in zip(position_values, volatilities)
            )) / sum(position_values) if position_values else 0.05
            
            # 95% VAR
            total_value = sum(position_values)
            var_95 = total_value * portfolio_volatility * 1.65
            
            return var_95
            
        except:
            return sum(position_values) * 0.05
            
    def _monte_carlo_var(self, position_values, volatilities):
        """Monte Carlo VAR simulation"""
        try:
            # Simplified Monte Carlo (would be more complex in production)
            simulations = 1000
            returns = []
            
            for _ in range(simulations):
                portfolio_return = 0
                
                for value, vol in zip(position_values, volatilities):
                    # Generate random return
                    random_return = np.random.normal(0, vol)
                    portfolio_return += (value / sum(position_values)) * random_return
                    
                returns.append(portfolio_return)
                
            # 5th percentile (95% VAR)
            var_95 = abs(np.percentile(returns, 5)) * sum(position_values)
            return var_95
            
        except:
            return sum(position_values) * 0.05
            
    def _get_position_volatility(self, symbol):
        """Get position volatility"""
        try:
            # Get from cache or calculate
            cache_key = f'volatility_{symbol}'
            cached_vol = self.memory_manager.cache_get(cache_key, 'indicators')
            
            if cached_vol is not None:
                return cached_vol
                
            # Get market data and calculate
            market_data = self.binance_client.get_klines(symbol, '1h', 24)
            
            if market_data is not None and len(market_data) > 10:
                returns = market_data['close'].pct_change().dropna()
                volatility = returns.std() * np.sqrt(24)  # Daily volatility
                
                # Cache result
                self.memory_manager.cache_set(cache_key, volatility, 'indicators')
                
                return max(volatility, 0.01)  # Minimum 1%
            else:
                return 0.05  # Default 5%
                
        except:
            return 0.05
            
    def calculate_current_drawdown(self, portfolio_state):
        """Calculate current portfolio drawdown"""
        try:
            # Get portfolio value history
            value_history = list(self.position_system.portfolio_metrics.get('value', []))
            
            if len(value_history) < 2:
                return 0
                
            # Find peak value
            current_value = portfolio_state.get('total_value', 0)
            peak_value = max(entry['total_value'] for entry in value_history)
            
            if peak_value <= 0:
                return 0
                
            drawdown = (peak_value - current_value) / peak_value
            return max(drawdown, 0)
            
        except:
            return 0
            
    def calculate_correlation_risk(self, active_positions):
        """Calculate portfolio correlation risk"""
        try:
            if len(active_positions) < 2:
                return 0
                
            symbols = list(active_positions.keys())
            total_correlation = 0
            pairs = 0
            
            for i in range(len(symbols)):
                for j in range(i + 1, len(symbols)):
                    pos1 = active_positions[symbols[i]]
                    pos2 = active_positions[symbols[j]]
                    
                    correlation = self.position_system._get_symbol_correlation(
                        pos1['symbol'], pos2['symbol']
                    )
                    
                    if correlation is not None:
                        # Weight by position sizes
                        weight1 = pos1['position_value_usd']
                        weight2 = pos2['position_value_usd']
                        
                        weighted_correlation = abs(correlation) * np.sqrt(weight1 * weight2)
                        total_correlation += weighted_correlation
                        pairs += 1
                        
            if pairs > 0:
                avg_correlation = total_correlation / pairs
                return min(avg_correlation / 1000000, 1.0)  # Normalize
            else:
                return 0
                
        except:
            return 0.3  # Conservative estimate
            
    def calculate_concentration_risk(self, active_positions, total_value):
        """Calculate position concentration risk"""
        try:
            if not active_positions or total_value <= 0:
                return 0
                
            # Calculate Herfindahl index for concentration
            position_weights = [
                pos['position_value_usd'] / total_value
                for pos in active_positions.values()
            ]
            
            herfindahl_index = sum(weight ** 2 for weight in position_weights)
            
            # Convert to risk score (higher concentration = higher risk)
            concentration_risk = min(herfindahl_index * 2, 1.0)
            
            return concentration_risk
            
        except:
            return 0.3
            
    def calculate_liquidity_risk(self, active_positions):
        """Calculate portfolio liquidity risk"""
        try:
            if not active_positions:
                return 0
                
            total_liquidity_risk = 0
            total_value = 0
            
            for position in active_positions.values():
                symbol = position['symbol']
                position_value = position['position_value_usd']
                
                # Get symbol liquidity metrics
                liquidity_score = self._get_symbol_liquidity_score(symbol)
                position_liquidity_risk = (1 - liquidity_score) * position_value
                
                total_liquidity_risk += position_liquidity_risk
                total_value += position_value
                
            if total_value > 0:
                portfolio_liquidity_risk = total_liquidity_risk / total_value
                return min(portfolio_liquidity_risk, 1.0)
            else:
                return 0
                
        except:
            return 0.2  # Conservative estimate
            
    def _get_symbol_liquidity_score(self, symbol):
        """Get liquidity score for symbol"""
        try:
            # Get 24h volume
            stats = self.binance_client.get_symbol_statistics(symbol)
            
            if stats and 'quote_volume' in stats:
                volume_24h = stats['quote_volume']
                
                # Score based on volume (higher volume = better liquidity)
                if volume_24h > 100000000:  # $100M+
                    return 1.0
                elif volume_24h > 50000000:  # $50M+
                    return 0.9
                elif volume_24h > 10000000:  # $10M+
                    return 0.8
                elif volume_24h > 1000000:   # $1M+
                    return 0.6
                else:
                    return 0.3
            else:
                return 0.5  # Default medium liquidity
                
        except:
            return 0.5
            
    def calculate_market_risk(self):
        """Calculate external market risk factors"""
        try:
            market_risk_factors = {}
            
            # Get BTC volatility as market risk proxy
            btc_vol = self._get_position_volatility('BTCUSDT')
            market_risk_factors['crypto_volatility'] = min(btc_vol * 2, 1.0)
            
            # Check for unusual market conditions
            # (Would integrate with news, economic indicators, etc.)
            market_risk_factors['news_risk'] = 0.1  # Base news risk
            market_risk_factors['economic_risk'] = 0.05  # Base economic risk
            
            # Overall market risk score
            overall_risk = np.mean(list(market_risk_factors.values()))
            
            return {
                'overall_score': overall_risk,
                'factors': market_risk_factors
            }
            
        except:
            return {'overall_score': 0.2, 'factors': {}}
            
    def calculate_overall_risk_score(self, var_pct, daily_loss_pct, drawdown, leverage, correlation, concentration):
        """Calculate overall portfolio risk score (0-1)"""
        try:
            # Weight different risk factors
            weights = {
                'var': 0.25,
                'daily_loss': 0.20,
                'drawdown': 0.20,
                'leverage': 0.15,
                'correlation': 0.10,
                'concentration': 0.10
            }
            
            # Normalize risk factors to 0-1 scale
            normalized_risks = {
                'var': min(var_pct / 0.2, 1.0),  # 20% VAR = max risk
                'daily_loss': min(abs(daily_loss_pct) / 0.15, 1.0),  # 15% daily loss = max
                'drawdown': min(drawdown / 0.25, 1.0),  # 25% drawdown = max
                'leverage': min(leverage / 25, 1.0),  # 25x leverage = max
                'correlation': min(correlation / 0.8, 1.0),  # 80% correlation = max
                'concentration': concentration  # Already 0-1
            }
            
            # Calculate weighted risk score
            risk_score = sum(
                normalized_risks[factor] * weight
                for factor, weight in weights.items()
            )
            
            return min(max(risk_score, 0), 1.0)
            
        except:
            return 0.5  # Medium risk default
            
    def check_risk_violations(self, risk_assessment):
        """Check for risk limit violations"""
        try:
            violations = []
            
            # Daily loss check
            if risk_assessment['daily_loss_pct'] < -self.risk_limits['max_daily_loss_pct']:
                violations.append({
                    'type': 'daily_loss_limit',
                    'severity': 'high',
                    'current_value': risk_assessment['daily_loss_pct'],
                    'limit': -self.risk_limits['max_daily_loss_pct'],
                    'action_required': 'stop_new_trades'
                })
                
            # Portfolio VAR check
            if risk_assessment['portfolio_var_pct'] > self.risk_limits['max_portfolio_var']:
                violations.append({
                    'type': 'portfolio_var_limit',
                    'severity': 'medium' if risk_assessment['portfolio_var_pct'] < self.risk_limits['max_portfolio_var'] * 1.5 else 'high',
                    'current_value': risk_assessment['portfolio_var_pct'],
                    'limit': self.risk_limits['max_portfolio_var'],
                    'action_required': 'reduce_positions'
                })
                
            # Drawdown check
            if risk_assessment['current_drawdown'] > self.risk_limits['max_total_drawdown_pct']:
                violations.append({
                    'type': 'drawdown_limit',
                    'severity': 'critical',
                    'current_value': risk_assessment['current_drawdown'],
                    'limit': self.risk_limits['max_total_drawdown_pct'],
                    'action_required': 'emergency_close_positions'
                })
                
            # Leverage check
            if risk_assessment['effective_leverage'] > self.risk_limits['max_leverage']:
                violations.append({
                    'type': 'leverage_limit',
                    'severity': 'medium',
                    'current_value': risk_assessment['effective_leverage'],
                    'limit': self.risk_limits['max_leverage'],
                    'action_required': 'reduce_leverage'
                })
                
            # Correlation check
            if risk_assessment['correlation_risk'] > self.risk_limits['correlation_limit']:
                violations.append({
                    'type': 'correlation_limit',
                    'severity': 'low',
                    'current_value': risk_assessment['correlation_risk'],
                    'limit': self.risk_limits['correlation_limit'],
                    'action_required': 'reduce_correlated_positions'
                })
                
            # Position count check
            if risk_assessment['position_count'] > self.risk_limits['max_open_positions']:
                violations.append({
                    'type': 'position_count_limit',
                    'severity': 'low',
                    'current_value': risk_assessment['position_count'],
                    'limit': self.risk_limits['max_open_positions'],
                    'action_required': 'close_least_profitable_positions'
                })
                
            return violations
            
        except Exception as e:
            self.logger.error(f"Risk violation check failed: {e}")
            return []
            
    def detect_black_swan_events(self):
        """Detect potential black swan market events"""
        try:
            black_swan_indicators = []
            
            # Check major market movements
            major_symbols = ['BTCUSDT', 'ETHUSDT', 'BNBUSDT']
            
            for symbol in major_symbols:
                # Get recent price data
                recent_data = self.binance_client.get_klines(symbol, '1m', 10)
                
                if recent_data is not None and len(recent_data) >= 2:
                    # Check for price shock
                    latest_price = recent_data['close'].iloc[-1]
                    prev_price = recent_data['close'].iloc[-2]
                    price_change = abs(latest_price - prev_price) / prev_price
                    
                    if price_change > self.black_swan_detector['price_shock_threshold']:
                        black_swan_indicators.append({
                            'type': 'price_shock',
                            'symbol': symbol,
                            'price_change_pct': price_change,
                            'severity': 'critical' if price_change > 0.25 else 'high'
                        })
                        
                    # Check for volume spike
                    current_volume = recent_data['volume'].iloc[-1]
                    avg_volume = recent_data['volume'].rolling(5).mean().iloc[-2]
                    
                    if avg_volume > 0 and current_volume / avg_volume > self.black_swan_detector['volume_spike_threshold']:
                        black_swan_indicators.append({
                            'type': 'volume_spike',
                            'symbol': symbol,
                            'volume_ratio': current_volume / avg_volume,
                            'severity': 'high'
                        })
                        
            # Check market-wide correlation breakdown
            correlation_breakdown = self._check_correlation_breakdown()
            if correlation_breakdown:
                black_swan_indicators.append(correlation_breakdown)
                
            return black_swan_indicators
            
        except Exception as e:
            self.logger.error(f"Black swan detection failed: {e}")
            return []
            
    def _check_correlation_breakdown(self):
        """Check for sudden correlation breakdown"""
        try:
            # Get recent returns for major pairs
            major_symbols = ['BTCUSDT', 'ETHUSDT', 'BNBUSDT']
            returns_data = {}
            
            for symbol in major_symbols:
                data = self.binance_client.get_klines(symbol, '5m', 12)  # Last hour
                if data is not None and len(data) > 1:
                    returns = data['close'].pct_change().dropna()
                    returns_data[symbol] = returns
                    
            if len(returns_data) >= 2:
                symbols = list(returns_data.keys())
                correlations = []
                
                for i in range(len(symbols)):
                    for j in range(i + 1, len(symbols)):
                        if (len(returns_data[symbols[i]]) > 5 and 
                            len(returns_data[symbols[j]]) > 5):
                            
                            # Align data
                            min_len = min(len(returns_data[symbols[i]]), len(returns_data[symbols[j]]))
                            corr = np.corrcoef(
                                returns_data[symbols[i]][-min_len:],
                                returns_data[symbols[j]][-min_len:]
                            )[0, 1]
                            
                            if not np.isnan(corr):
                                correlations.append(abs(corr))
                                
                if correlations:
                    avg_correlation = np.mean(correlations)
                    
                    # Normal crypto correlation is ~0.6-0.8
                    if avg_correlation < self.black_swan_detector['correlation_breakdown_threshold']:
                        return {
                            'type': 'correlation_breakdown',
                            'avg_correlation': avg_correlation,
                            'expected_correlation': 0.7,
                            'severity': 'high'
                        }
                        
            return None
            
        except Exception as e:
            self.logger.debug(f"Correlation breakdown check failed: {e}")
            return None
            
    def update_emergency_state(self, risk_assessment, violations, black_swan_events):
        """Update emergency state based on risk conditions"""
        try:
            previous_risk_level = self.emergency_state['risk_level']
            
            # Determine new risk level
            if black_swan_events:
                new_risk_level = 'emergency'
            elif any(v['severity'] == 'critical' for v in violations):
                new_risk_level = 'critical'
            elif any(v['severity'] == 'high' for v in violations):
                new_risk_level = 'elevated'
            elif risk_assessment['risk_score'] > 0.7:
                new_risk_level = 'elevated'
            else:
                new_risk_level = 'normal'
                
            # Update emergency state
            self.emergency_state.update({
                'risk_level': new_risk_level,
                'is_active': new_risk_level != 'normal',
                'last_update': time.time()
            })
            
            # Log risk level changes
            if new_risk_level != previous_risk_level:
                self.logger.warning(
                    f"üö® Risk level changed: {previous_risk_level} ‚Üí {new_risk_level}"
                )
                
                if new_risk_level in ['critical', 'emergency']:
                    self.emergency_state.update({
                        'triggered_at': time.time(),
                        'trigger_reason': f"Risk violations: {len(violations)}, Black swan events: {len(black_swan_events)}"
                    })
                    
        except Exception as e:
            self.logger.error(f"Emergency state update failed: {e}")
            
    def execute_risk_actions(self, violations, black_swan_events):
        """Execute risk management actions"""
        try:
            actions_executed = []
            
            # Handle black swan events first (highest priority)
            if black_swan_events:
                for event in black_swan_events:
                    if event['severity'] == 'critical':
                        action = self._execute_emergency_stop_all()
                        actions_executed.append(action)
                        break  # Stop after emergency action
                        
            # Handle violations by severity
            critical_violations = [v for v in violations if v['severity'] == 'critical']
            high_violations = [v for v in violations if v['severity'] == 'high']
            medium_violations = [v for v in violations if v['severity'] == 'medium']
            
            # Execute critical actions
            for violation in critical_violations:
                action = self._execute_violation_action(violation)
                actions_executed.append(action)
                
            # Execute high priority actions
            for violation in high_violations:
                action = self._execute_violation_action(violation)
                actions_executed.append(action)
                
            # Execute medium priority actions (if not in emergency)
            if self.emergency_state['risk_level'] not in ['critical', 'emergency']:
                for violation in medium_violations:
                    action = self._execute_violation_action(violation)
                    actions_executed.append(action)
                    
            # Log actions
            if actions_executed:
                self.logger.warning(f"üõ°Ô∏è Risk actions executed: {len(actions_executed)}")
                for action in actions_executed:
                    if action.get('success'):
                        self.logger.info(f"   ‚úÖ {action['action_type']}: {action.get('details', '')}")
                    else:
                        self.logger.error(f"   ‚ùå {action['action_type']}: {action.get('error', 'Failed')}")
                        
            return actions_executed
            
        except Exception as e:
            self.logger.error(f"Risk action execution failed: {e}")
            return []
            
    def _execute_violation_action(self, violation):
        """Execute specific action for risk violation"""
        try:
            action_type = violation['action_required']
            
            if action_type == 'stop_new_trades':
                return self._execute_stop_new_trades(violation)
            elif action_type == 'reduce_positions':
                return self._execute_reduce_positions(violation)
            elif action_type == 'emergency_close_positions':
                return self._execute_emergency_close_positions(violation)
            elif action_type == 'reduce_leverage':
                return self._execute_reduce_leverage(violation)
            elif action_type == 'reduce_correlated_positions':
                return self._execute_reduce_correlated_positions(violation)
            elif action_type == 'close_least_profitable_positions':
                return self._execute_close_least_profitable(violation)
            else:
                return {'success': False, 'error': f'Unknown action: {action_type}'}
                
        except Exception as e:
            return {'success': False, 'error': str(e)}
            
    def _execute_stop_new_trades(self, violation):
        """Stop opening new trades"""
        try:
            # Set circuit breaker
            self.circuit_breakers['daily_loss']['triggered'] = True
            self.circuit_breakers['daily_loss']['triggered_at'] = time.time()
            
            return {
                'success': True,
                'action_type': 'stop_new_trades',
                'details': f"New trades stopped due to {violation['type']}",
                'recovery_time': self.circuit_breakers['daily_loss']['recovery_time']
            }
            
        except Exception as e:
            return {'success': False, 'action_type': 'stop_new_trades', 'error': str(e)}
            
    def _execute_reduce_positions(self, violation):
        """Reduce position sizes to lower portfolio risk"""
        try:
            positions_reduced = 0
            
            # Get positions sorted by risk contribution
            active_positions = list(self.position_system.active_positions.values())
            
            # Sort by position value (largest first)
            active_positions.sort(key=lambda x: x['position_value_usd'], reverse=True)
            
            # Reduce largest positions by 30%
            target_reduction_count = min(3, len(active_positions))
            
            for position in active_positions[:target_reduction_count]:
                # Simulate position reduction (would call actual trading API)
                reduction_pct = 0.3  # 30% reduction
                
                if self.config.PAPER_MODE:
                    # Paper mode: just update position size
                    position['position_value_usd'] *= (1 - reduction_pct)
                    position['quantity'] *= (1 - reduction_pct)
                    positions_reduced += 1
                else:
                    # Real trading: would place sell orders
                    # For now, just log the action
                    self.logger.info(f"Would reduce {position['symbol']} by {reduction_pct:.1%}")
                    positions_reduced += 1
                    
            return {
                'success': True,
                'action_type': 'reduce_positions',
                'details': f"Reduced {positions_reduced} positions by 30%",
                'positions_affected': positions_reduced
            }
            
        except Exception as e:
            return {'success': False, 'action_type': 'reduce_positions', 'error': str(e)}
            
    def _execute_emergency_close_positions(self, violation):
        """Emergency close all losing positions"""
        try:
            positions_closed = 0
            
            # Get all losing positions
            losing_positions = [
                (pos_id, pos) for pos_id, pos in self.position_system.active_positions.items()
                if pos.get('current_pnl_pct', 0) < 0
            ]
            
            # Close all losing positions
            for pos_id, position in losing_positions:
                # Simulate closing position
                if self.config.PAPER_MODE:
                    # Paper mode: remove from active positions
                    self.position_system.close_position(pos_id, 'emergency_risk_close')
                    positions_closed += 1
                else:
                    # Real trading: would place market sell orders
                    self.logger.warning(f"Emergency closing {position['symbol']}")
                    positions_closed += 1
                    
            return {
                'success': True,
                'action_type': 'emergency_close_positions',
                'details': f"Emergency closed {positions_closed} losing positions",
                'positions_closed': positions_closed
            }
            
        except Exception as e:
            return {'success': False, 'action_type': 'emergency_close_positions', 'error': str(e)}
            
    def _execute_reduce_leverage(self, violation):
        """Reduce leverage on high-leverage positions"""
        try:
            leverage_reduced = 0
            
            # Find high leverage positions
            high_leverage_positions = [
                pos for pos in self.position_system.active_positions.values()
                if pos.get('leverage', 1) > 10
            ]
            
            for position in high_leverage_positions:
                # Reduce leverage to 10x max
                if position.get('leverage', 1) > 10:
                    new_leverage = min(10, position.get('leverage', 1))
                    
                    if self.config.PAPER_MODE:
                        position['leverage'] = new_leverage
                        leverage_reduced += 1
                    else:
                        # Real trading: would call leverage adjustment API
                        self.logger.info(f"Reducing {position['symbol']} leverage to {new_leverage}x")
                        leverage_reduced += 1
                        
            return {
                'success': True,
                'action_type': 'reduce_leverage',
                'details': f"Reduced leverage on {leverage_reduced} positions",
                'positions_affected': leverage_reduced
            }
            
        except Exception as e:
            return {'success': False, 'action_type': 'reduce_leverage', 'error': str(e)}
            
    def _execute_reduce_correlated_positions(self, violation):
        """Reduce positions in highly correlated assets"""
        try:
            positions_reduced = 0
            
            # Group positions by base asset
            asset_groups = defaultdict(list)
            
            for pos_id, position in self.position_system.active_positions.items():
                base_asset = position['symbol'].replace('USDT', '')
                asset_groups[base_asset].append((pos_id, position))
                
            # Find groups with multiple positions
            for base_asset, positions in asset_groups.items():
                if len(positions) > 1:
                    # Close smallest position in group
                    smallest_pos = min(positions, key=lambda x: x[1]['position_value_usd'])
                    
                    if self.config.PAPER_MODE:
                        self.position_system.close_position(smallest_pos[0], 'correlation_reduction')
                        positions_reduced += 1
                    else:
                        self.logger.info(f"Closing correlated position: {smallest_pos[1]['symbol']}")
                        positions_reduced += 1
                        
            return {
                'success': True,
                'action_type': 'reduce_correlated_positions',
                'details': f"Reduced {positions_reduced} correlated positions",
                'positions_affected': positions_reduced
            }
            
        except Exception as e:
            return {'success': False, 'action_type': 'reduce_correlated_positions', 'error': str(e)}
            
    def _execute_close_least_profitable(self, violation):
        """Close least profitable positions"""
        try:
            positions_closed = 0
            
            # Get all positions sorted by PnL
            positions_by_pnl = list(self.position_system.active_positions.items())
            positions_by_pnl.sort(key=lambda x: x[1].get('current_pnl_pct', 0))
            
            # Close worst performing 25% of positions
            positions_to_close = max(1, len(positions_by_pnl) // 4)
            
            for pos_id, position in positions_by_pnl[:positions_to_close]:
                if self.config.PAPER_MODE:
                    self.position_system.close_position(pos_id, 'performance_cleanup')
                    positions_closed += 1
                else:
                    self.logger.info(f"Closing underperforming position: {position['symbol']}")
                    positions_closed += 1
                    
            return {
                'success': True,
                'action_type': 'close_least_profitable',
                'details': f"Closed {positions_closed} underperforming positions",
                'positions_closed': positions_closed
            }
            
        except Exception as e:
            return {'success': False, 'action_type': 'close_least_profitable', 'error': str(e)}
            
    def _execute_emergency_stop_all(self):
        """Emergency stop all trading activity"""
        try:
            # Close all positions
            positions_closed = 0
            
            for pos_id in list(self.position_system.active_positions.keys()):
                if self.config.PAPER_MODE:
                    self.position_system.close_position(pos_id, 'black_swan_emergency')
                    positions_closed += 1
                else:
                    position = self.position_system.active_positions[pos_id]
                    self.logger.critical(f"BLACK SWAN: Emergency closing {position['symbol']}")
                    positions_closed += 1
                    
            # Set all circuit breakers
            for breaker_name in self.circuit_breakers:
                self.circuit_breakers[breaker_name]['triggered'] = True
                self.circuit_breakers[breaker_name]['triggered_at'] = time.time()
                
            return {
                'success': True,
                'action_type': 'emergency_stop_all',
                'details': f"BLACK SWAN EVENT: Emergency closed all {positions_closed} positions",
                'positions_closed': positions_closed
            }
            
        except Exception as e:
            return {'success': False, 'action_type': 'emergency_stop_all', 'error': str(e)}
            
    def calculate_dynamic_stop_loss(self, position, current_market_data):
        """Calculate dynamic stop loss for position"""
        try:
            symbol = position['symbol']
            entry_price = position['entry_price']
            direction = position['direction']
            current_pnl = position.get('current_pnl_pct', 0)
            
            # Get market volatility
            volatility = self._get_position_volatility(symbol)
            
            # Base stop loss
            base_stop = self.stop_loss_system['base_stop_pct']
            
            # Volatility adjustment
            vol_adjusted_stop = base_stop * (1 + volatility * self.stop_loss_system['volatility_multiplier'])
            
            # Time decay (widen stops over time for mean reversion)
            hold_time = time.time() - position['entry_time']
            time_factor = 1 + (hold_time / 3600) * self.stop_loss_system['time_decay_factor']
            time_adjusted_stop = vol_adjusted_stop * time_factor
            
            # Momentum adjustment
            if self.stop_loss_system['momentum_adjustment'] and len(current_market_data) > 10:
                # Check if price is moving in our favor
                recent_momentum = current_market_data['close'].pct_change().tail(5).mean()
                
                if direction == 'long':
                    momentum_favorable = recent_momentum > 0
                else:
                    momentum_favorable = recent_momentum < 0
                    
                if momentum_favorable:
                    momentum_factor = 0.8  # Tighter stops with momentum
                else:
                    momentum_factor = 1.2  # Wider stops against momentum
                    
                time_adjusted_stop *= momentum_factor
                
            # Trailing stop logic
            if current_pnl > self.stop_loss_system['trail_activation_pct']:
                # Activate trailing stop
                trail_distance = max(time_adjusted_stop, current_pnl * 0.3)  # Trail 30% of profit
                
                # Update trailing stop if better than current
                current_trail_stop = position.get('trailing_stop_loss', time_adjusted_stop)
                
                if direction == 'long':
                    new_trail_stop = max(current_trail_stop, time_adjusted_stop)
                else:
                    new_trail_stop = min(current_trail_stop, time_adjusted_stop)
                    
                position['trailing_stop_loss'] = new_trail_stop
                final_stop = new_trail_stop
            else:
                final_stop = time_adjusted_stop
                
            # Cap stop loss at reasonable levels
            final_stop = min(final_stop, 0.15)  # Max 15% stop
            final_stop = max(final_stop, 0.01)  # Min 1% stop
            
            return {
                'stop_loss_pct': final_stop,
                'stop_price': entry_price * (1 - final_stop) if direction == 'long' else entry_price * (1 + final_stop),
                'is_trailing': current_pnl > self.stop_loss_system['trail_activation_pct'],
                'base_stop': base_stop,
                'volatility_adjustment': volatility,
                'time_adjustment': time_factor,
                'reasoning': f"Dynamic stop: base={base_stop:.1%}, vol_adj={volatility:.1%}, time_factor={time_factor:.2f}"
            }
            
        except Exception as e:
            self.logger.error(f"Dynamic stop loss calculation failed: {e}")
            return {
                'stop_loss_pct': 0.05,  # 5% default
                'stop_price': entry_price * 0.95 if direction == 'long' else entry_price * 1.05,
                'is_trailing': False,
                'reasoning': 'Default stop due to calculation error'
            }
            
    def store_risk_metrics(self, risk_assessment):
        """Store risk metrics for analysis"""
        try:
            # Store current metrics
            self.risk_metrics['portfolio_risk'].append({
                'timestamp': time.time(),
                'risk_score': risk_assessment['risk_score'],
                'var_pct': risk_assessment['portfolio_var_pct'],
                'daily_loss_pct': risk_assessment['daily_loss_pct'],
                'drawdown': risk_assessment['current_drawdown'],
                'leverage': risk_assessment['effective_leverage'],
                'position_count': risk_assessment['position_count']
            })
            
            # Keep only recent metrics (last 24 hours)
            cutoff_time = time.time() - 86400
            self.risk_metrics['portfolio_risk'] = deque([
                metric for metric in self.risk_metrics['portfolio_risk']
                if metric['timestamp'] > cutoff_time
            ], maxlen=2000)
            
        except Exception as e:
            self.logger.error(f"Risk metrics storage failed: {e}")
            
    def _get_empty_risk_assessment(self):
        """Get empty risk assessment structure"""
        return {
            'timestamp': time.time(),
            'total_portfolio_value': 0,
            'portfolio_var_1d': 0,
            'portfolio_var_pct': 0,
            'daily_pnl': 0,
            'daily_loss_pct': 0,
            'current_drawdown': 0,
            'effective_leverage': 0,
            'correlation_risk': 0,
            'concentration_risk': 0,
            'liquidity_risk': 0,
            'market_risk': {'overall_score': 0, 'factors': {}},
            'position_count': 0,
            'risk_score': 0
        }
        
    def can_open_new_position(self, signal_confidence, position_size_pct):
        """Check if new position can be opened based on risk state"""
        try:
            # Check circuit breakers
            if self.circuit_breakers['daily_loss']['triggered']:
                time_since_trigger = time.time() - self.circuit_breakers['daily_loss'].get('triggered_at', 0)
                if time_since_trigger < self.circuit_breakers['daily_loss']['recovery_time']:
                    return False, "Daily loss circuit breaker active"
                    
            # Check emergency state
            if self.emergency_state['risk_level'] in ['critical', 'emergency']:
                return False, f"Emergency state active: {self.emergency_state['risk_level']}"
                
            # Check risk score
            if self.emergency_state['risk_level'] == 'elevated':
                # Only allow high-confidence, small positions
                if signal_confidence < 0.8 or position_size_pct > 0.05:
                    return False, "Elevated risk - only high confidence, small positions allowed"
                    
            # Check position count
            current_positions = len(self.position_system.active_positions)
            if current_positions >= self.risk_limits['max_open_positions']:
                return False, f"Maximum positions reached: {current_positions}"
                
            return True, "Position approved"
            
        except Exception as e:
            self.logger.error(f"Position approval check failed: {e}")
            return False, "Risk check failed"
            
    def get_risk_dashboard_data(self):
        """Get risk metrics for dashboard display"""
        try:
            # Current risk state
            current_risk = list(self.risk_metrics['portfolio_risk'])[-1] if self.risk_metrics['portfolio_risk'] else self._get_empty_risk_assessment()
            
            # Circuit breaker status
            active_breakers = [
                name for name, breaker in self.circuit_breakers.items()
                if breaker['triggered']
            ]
            
            # Recent violations
            recent_violations = [
                v for v in self.violation_history
                if time.time() - v.get('timestamp', 0) < 3600  # Last hour
            ]
            
            # Performance metrics
            if self.risk_metrics['portfolio_risk']:
                recent_metrics = list(self.risk_metrics['portfolio_risk'])[-100:]  # Last 100 points
                
                avg_risk_score = np.mean([m['risk_score'] for m in recent_metrics])
                max_drawdown = max([m['drawdown'] for m in recent_metrics])
                avg_leverage = np.mean([m['leverage'] for m in recent_metrics])
            else:
                avg_risk_score = 0
                max_drawdown = 0
                avg_leverage = 0
                
            return {
                'current_risk_state': {
                    'risk_level': self.emergency_state['risk_level'],
                    'risk_score': current_risk['risk_score'],
                    'var_pct': current_risk['portfolio_var_pct'],
                    'daily_loss_pct': current_risk['daily_loss_pct'],
                    'drawdown_pct': current_risk['current_drawdown'],
                    'leverage': current_risk['effective_leverage'],
                    'position_count': current_risk['position_count']
                },
                'risk_limits': self.risk_limits,
                'circuit_breakers': {
                    'active_count': len(active_breakers),
                    'active_breakers': active_breakers,
                    'all_breakers': {
                        name: {
                            'triggered': breaker['triggered'],
                            'threshold': breaker['threshold'],
                            'action': breaker['action']
                        }
                        for name, breaker in self.circuit_breakers.items()
                    }
                },
                'violations': {
                    'recent_count': len(recent_violations),
                    'total_today': len([v for v in self.violation_history if time.time() - v.get('timestamp', 0) < 86400])
                },
                'performance_summary': {
                    'avg_risk_score': avg_risk_score,
                    'max_recent_drawdown': max_drawdown,
                    'avg_leverage': avg_leverage,
                    'stops_triggered_today': self.risk_performance['stops_triggered'],
                    'emergency_stops_total': self.risk_performance['emergency_stops']
                },
                'black_swan_status': {
                    'detection_active': True,
                    'last_check': time.time(),
                    'sensitivity_level': 'high'
                }
            }
            
        except Exception as e:
            self.logger.error(f"Risk dashboard data generation failed: {e}")
            return {}

def initialize_risk_management_system(config, binance_client, memory_manager, position_system):
    """Initialize complete risk management system"""
    try:
        # Initialize risk management engine
        risk_engine = RiskManagementEngine(config, binance_client, memory_manager, position_system)
        
        logger = logging.getLogger('MonsterBot.RiskManagement')
        logger.info("üõ°Ô∏è Risk management engine fully initialized")
        logger.info("   üìâ Daily loss limit: 15%")
        logger.info("   üí• Max drawdown: 25%")
        logger.info("   ‚ö° Max leverage: 25x")
        logger.info("   üö® Circuit breakers: 5 active")
        logger.info("   üñ§ Black swan detection: Active")
        logger.info("   üìä Real-time monitoring: Every 5 seconds")
        logger.info("   üéØ Dynamic stop losses: Active")
        
        return {
            'risk_engine': risk_engine,
            'monitoring_active': True,
            'circuit_breakers': 5,
            'black_swan_detection': True,
            'update_frequency': 5,
            'emergency_controls': True
        }
        
    except Exception as e:
        logger = logging.getLogger('MonsterBot.RiskManagement')
        logger.error(f"Risk management system initialization failed: {e}")
        raise
        
class DynamicExitSystem:
    """Advanced exit strategies with intelligent profit taking and loss management"""
    
    def __init__(self, config, binance_client, memory_manager, position_system, risk_engine):
        self.config = config
        self.binance_client = binance_client
        self.memory_manager = memory_manager
        self.position_system = position_system
        self.risk_engine = risk_engine
        self.logger = logging.getLogger('MonsterBot.DynamicExit')
        
        # Exit strategy configurations
        self.exit_strategies = {
            'multiple_take_profits': {
                'enabled': True,
                'levels': [0.3, 0.6, 0.9, 1.3],  # Fractions of target
                'sizes': [0.2, 0.3, 0.3, 0.2],   # Position fractions to close
                'adaptive': True
            },
            'trailing_stop': {
                'enabled': True,
                'activation_profit': 0.02,  # 2% profit to activate
                'trail_distance': 0.015,    # 1.5% trail distance
                'step_size': 0.005,         # 0.5% step increments
                'acceleration': True
            },
            'momentum_exit': {
                'enabled': True,
                'momentum_threshold': 0.8,   # 80% momentum loss
                'volume_confirmation': True,
                'lookback_periods': 5
            },
            'time_based_exit': {
                'enabled': True,
                'profit_time_scaling': True,
                'loss_time_limits': True,
                'max_hold_multiplier': 2.0
            },
            'volatility_exit': {
                'enabled': True,
                'vol_expansion_exit': 2.0,   # 2x volatility = exit
                'vol_contraction_hold': 0.5  # 0.5x volatility = hold longer
            },
            'support_resistance_exit': {
                'enabled': True,
                'level_strength_threshold': 0.7,
                'approach_distance': 0.01    # 1% from level
            }
        }
        
        # Exit performance tracking
        self.exit_performance = {
            'total_exits': 0,
            'profitable_exits': 0,
            'exit_reasons': defaultdict(int),
            'avg_exit_efficiency': deque(maxlen=100),
            'best_exit_strategy': None
        }
        
        # Position exit tracking
        self.position_exit_data = defaultdict(dict)
        self.exit_signals = defaultdict(list)
        
        # Market regime detection for exit adaptation
        self.market_regimes = {
            'trending': {'momentum_weight': 1.2, 'trail_aggressiveness': 0.8},
            'ranging': {'support_resistance_weight': 1.3, 'time_exit_urgency': 1.1},
            'volatile': {'volatility_exit_sensitivity': 0.7, 'quick_profit_bias': 1.2},
            'low_vol': {'trail_patience': 1.4, 'profit_target_extension': 1.1}
        }
        
        # Advanced exit features
        self.smart_exit_features = {
            'profit_acceleration': True,    # Faster exits when profits accelerate
            'loss_mitigation': True,       # Smart loss cutting
            'correlation_exits': True,     # Exit correlated positions together
            'news_reaction_exits': True,   # Exit on negative news
            'whale_movement_exits': True   # Exit on large contra flows
        }
        
    def analyze_exit_opportunities(self, position_id, current_market_data, market_context):
        """Comprehensive analysis of exit opportunities for a position"""
        try:
            if position_id not in self.position_system.active_positions:
                return None
                
            position = self.position_system.active_positions[position_id]
            symbol = position['symbol']
            
            # Get current market state
            current_price = current_market_data['close'].iloc[-1] if len(current_market_data) > 0 else position['current_price']
            current_pnl = self.position_system._calculate_position_pnl_at_price(position, current_price)
            
            # Initialize exit analysis
            exit_analysis = {
                'position_id': position_id,
                'symbol': symbol,
                'current_pnl_pct': current_pnl,
                'current_price': current_price,
                'timestamp': time.time(),
                'exit_signals': [],
                'recommended_action': 'hold',
                'urgency': 'low',
                'confidence': 0.0
            }
            
            # 1. Multiple take profit analysis
            tp_signals = self._analyze_take_profit_levels(position, current_pnl, current_market_data)
            exit_analysis['exit_signals'].extend(tp_signals)
            
            # 2. Trailing stop analysis
            trail_signals = self._analyze_trailing_stop(position, current_pnl, current_market_data)
            exit_analysis['exit_signals'].extend(trail_signals)
            
            # 3. Momentum-based exit analysis
            momentum_signals = self._analyze_momentum_exit(position, current_market_data, market_context)
            exit_analysis['exit_signals'].extend(momentum_signals)
            
            # 4. Time-based exit analysis
            time_signals = self._analyze_time_based_exit(position, current_pnl)
            exit_analysis['exit_signals'].extend(time_signals)
            
            # 5. Volatility-based exit analysis
            vol_signals = self._analyze_volatility_exit(position, current_market_data)
            exit_analysis['exit_signals'].extend(vol_signals)
            
            # 6. Support/resistance exit analysis
            sr_signals = self._analyze_support_resistance_exit(position, current_market_data)
            exit_analysis['exit_signals'].extend(sr_signals)
            
            # 7. Smart exit features analysis
            smart_signals = self._analyze_smart_exit_features(position, current_market_data, market_context)
            exit_analysis['exit_signals'].extend(smart_signals)
            
            # Combine and prioritize exit signals
            final_recommendation = self._combine_exit_signals(exit_analysis['exit_signals'], position, current_pnl)
            exit_analysis.update(final_recommendation)
            
            # Store exit analysis
            self.position_exit_data[position_id]['last_analysis'] = exit_analysis
            
            return exit_analysis
            
        except Exception as e:
            self.logger.error(f"Exit analysis failed for {position_id}: {e}")
            return None
            
    def _analyze_take_profit_levels(self, position, current_pnl, market_data):
        """Analyze multiple take profit opportunities"""
        try:
            signals = []
            
            if not self.exit_strategies['multiple_take_profits']['enabled']:
                return signals
                
            targets = position.get('targets', {})
            estimated_move = position.get('estimated_move', 0.05)
            
            # Check each take profit level
            tp_levels = self.exit_strategies['multiple_take_profits']['levels']
            tp_sizes = self.exit_strategies['multiple_take_profits']['sizes']
            
            for i, (level_fraction, exit_size) in enumerate(zip(tp_levels, tp_sizes)):
                tp_name = f'tp{i+1}'
                target_pnl = estimated_move * level_fraction
                
                # Check if this level should be triggered
                if (current_pnl >= target_pnl and 
                    not position.get(f'{tp_name}_triggered', False)):
                    
                    # Adaptive sizing based on momentum
                    adaptive_size = exit_size
                    if self.exit_strategies['multiple_take_profits']['adaptive']:
                        momentum_score = self._calculate_exit_momentum(market_data)
                        
                        if momentum_score > 0.7:  # Strong momentum - take less profit
                            adaptive_size *= 0.7
                        elif momentum_score < 0.3:  # Weak momentum - take more profit
                            adaptive_size *= 1.3
                            
                    signals.append({
                        'type': 'take_profit',
                        'level': tp_name,
                        'trigger_pnl': target_pnl,
                        'current_pnl': current_pnl,
                        'exit_size_pct': min(adaptive_size, 1.0),
                        'confidence': 0.9,
                        'urgency': 'medium',
                        'reason': f'Take profit level {i+1} reached at {current_pnl:.1%}',
                        'adaptive_adjustment': adaptive_size / exit_size
                    })
                    
            return signals
            
        except Exception as e:
            self.logger.error(f"Take profit analysis failed: {e}")
            return []
            
    def _analyze_trailing_stop(self, position, current_pnl, market_data):
        """Analyze trailing stop opportunities"""
        try:
            signals = []
            
            if not self.exit_strategies['trailing_stop']['enabled']:
                return signals
                
            activation_profit = self.exit_strategies['trailing_stop']['activation_profit']
            trail_distance = self.exit_strategies['trailing_stop']['trail_distance']
            
            # Only activate trailing if profitable enough
            if current_pnl < activation_profit:
                return signals
                
            # Get or initialize trailing stop
            current_trail_stop = position.get('trailing_stop_pnl', current_pnl - trail_distance)
            
            # Update trailing stop (only move in favorable direction)
            new_trail_stop = max(current_trail_stop, current_pnl - trail_distance)
            
            # Acceleration feature - tighten trail as profits increase
            if self.exit_strategies['trailing_stop']['acceleration']:
                if current_pnl > 0.1:  # 10%+ profit
                    accelerated_distance = trail_distance * 0.6  # Tighter trail
                    new_trail_stop = max(new_trail_stop, current_pnl - accelerated_distance)
                    
            # Check if trailing stop hit
            if current_pnl <= new_trail_stop:
                signals.append({
                    'type': 'trailing_stop',
                    'trigger_pnl': new_trail_stop,
                    'current_pnl': current_pnl,
                    'exit_size_pct': 1.0,  # Close entire position
                    'confidence': 0.95,
                    'urgency': 'high',
                    'reason': f'Trailing stop triggered: {current_pnl:.1%} <= {new_trail_stop:.1%}',
                    'trail_distance': trail_distance
                })
            else:
                # Update trailing stop in position
                position['trailing_stop_pnl'] = new_trail_stop
                
            return signals
            
        except Exception as e:
            self.logger.error(f"Trailing stop analysis failed: {e}")
            return []
            
    def _analyze_momentum_exit(self, position, market_data, market_context):
        """Analyze momentum-based exit signals"""
        try:
            signals = []
            
            if not self.exit_strategies['momentum_exit']['enabled'] or len(market_data) < 10:
                return signals
                
            lookback = self.exit_strategies['momentum_exit']['lookback_periods']
            momentum_threshold = self.exit_strategies['momentum_exit']['momentum_threshold']
            
            # Calculate momentum indicators
            momentum_score = self._calculate_exit_momentum(market_data)
            
            # Volume confirmation
            volume_confirmation = True
            if self.exit_strategies['momentum_exit']['volume_confirmation']:
                current_volume = market_data['volume'].iloc[-1]
                avg_volume = market_data['volume'].rolling(10).mean().iloc[-1]
                volume_confirmation = current_volume > avg_volume * 0.8  # At least 80% avg volume
                
            # Direction-specific momentum check
            direction = position['direction']
            current_pnl = position.get('current_pnl_pct', 0)
            
            # Exit if momentum turns against us significantly
            if direction == 'long':
                momentum_against = momentum_score < (1 - momentum_threshold)
            else:
                momentum_against = momentum_score > momentum_threshold
                
            if momentum_against and volume_confirmation:
                # Size exit based on momentum strength and current profit
                if current_pnl > 0.05:  # Profitable position
                    exit_size = 0.6  # Partial exit to lock profits
                    urgency = 'medium'
                elif current_pnl > 0:   # Small profit
                    exit_size = 0.8  # Larger exit
                    urgency = 'high'
                else:                   # Losing position
                    exit_size = 1.0  # Full exit
                    urgency = 'high'
                    
                signals.append({
                    'type': 'momentum_exit',
                    'momentum_score': momentum_score,
                    'threshold': momentum_threshold,
                    'exit_size_pct': exit_size,
                    'confidence': 0.7,
                    'urgency': urgency,
                    'reason': f'Momentum turned against position: {momentum_score:.2f}',
                    'volume_confirmed': volume_confirmation
                })
                
            return signals
            
        except Exception as e:
            self.logger.error(f"Momentum exit analysis failed: {e}")
            return []
            
    def _calculate_exit_momentum(self, market_data):
        """Calculate momentum score for exit decisions"""
        try:
            if len(market_data) < 10:
                return 0.5
                
            # Price momentum
            recent_returns = market_data['close'].pct_change().tail(5)
            price_momentum = recent_returns.mean()
            
            # Volume-weighted momentum
            recent_volume = market_data['volume'].tail(5)
            volume_weights = recent_volume / recent_volume.sum()
            weighted_momentum = (recent_returns * volume_weights).sum()
            
            # RSI momentum
            if len(market_data) >= 14:
                rsi = self._calculate_rsi(market_data['close'], 14)
                rsi_momentum = (rsi.iloc[-1] - 50) / 50  # -1 to 1 scale
            else:
                rsi_momentum = 0
                
            # Combine momentum signals
            combined_momentum = (
                price_momentum * 0.4 +
                weighted_momentum * 0.4 +
                rsi_momentum * 0.2
            )
            
            # Normalize to 0-1 scale (0.5 = neutral)
            momentum_score = 0.5 + np.tanh(combined_momentum * 10) * 0.5
            
            return min(max(momentum_score, 0), 1)
            
        except:
            return 0.5
            
    def _calculate_rsi(self, prices, period=14):
        """Calculate RSI for momentum analysis"""
        try:
            delta = prices.diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
            rs = gain / loss
            rsi = 100 - (100 / (1 + rs))
            return rsi
        except:
            return pd.Series([50] * len(prices), index=prices.index)
            
    def _analyze_time_based_exit(self, position, current_pnl):
        """Analyze time-based exit signals"""
        try:
            signals = []
            
            if not self.exit_strategies['time_based_exit']['enabled']:
                return signals
                
            hold_time = time.time() - position['entry_time']
            max_hold_time = position.get('max_hold_time', 3600)  # 1 hour default
            
            # Profit-based time scaling
            if self.exit_strategies['time_based_exit']['profit_time_scaling']:
                if current_pnl > 0.1:  # 10%+ profit
                    # Allow longer hold time for big profits
                    adjusted_max_hold = max_hold_time * self.exit_strategies['time_based_exit']['max_hold_multiplier']
                elif current_pnl < -0.05:  # 5%+ loss
                    # Shorter hold time for losses
                    adjusted_max_hold = max_hold_time * 0.5
                else:
                    adjusted_max_hold = max_hold_time
            else:
                adjusted_max_hold = max_hold_time
                
            # Check for time-based exit
            time_pressure = hold_time / adjusted_max_hold
            
            if time_pressure > 1.0:  # Exceeded max hold time
                signals.append({
                    'type': 'time_exit',
                    'hold_time': hold_time,
                    'max_hold_time': adjusted_max_hold,
                    'time_pressure': time_pressure,
                    'exit_size_pct': 1.0,
                    'confidence': 0.6,
                    'urgency': 'medium',
                    'reason': f'Max hold time exceeded: {hold_time/3600:.1f}h > {adjusted_max_hold/3600:.1f}h'
                })
            elif time_pressure > 0.8:  # Approaching max hold time
                # Partial exit to reduce risk
                signals.append({
                    'type': 'time_pressure',
                    'hold_time': hold_time,
                    'time_pressure': time_pressure,
                    'exit_size_pct': 0.4,
                    'confidence': 0.4,
                    'urgency': 'low',
                    'reason': f'Approaching max hold time: {time_pressure:.1%}'
                })
                
            return signals
            
        except Exception as e:
            self.logger.error(f"Time-based exit analysis failed: {e}")
            return []
            
    def _analyze_volatility_exit(self, position, market_data):
        """Analyze volatility-based exit signals"""
        try:
            signals = []
            
            if not self.exit_strategies['volatility_exit']['enabled'] or len(market_data) < 20:
                return signals
                
            # Calculate current volatility
            returns = market_data['close'].pct_change().dropna()
            current_vol = returns.tail(10).std() * np.sqrt(24)  # Daily vol
            baseline_vol = returns.tail(50).std() * np.sqrt(24)  # Baseline
            
            if baseline_vol == 0:
                return signals
                
            vol_ratio = current_vol / baseline_vol
            vol_expansion_threshold = self.exit_strategies['volatility_exit']['vol_expansion_exit']
            
            # Exit on volatility expansion (risk increase)
            if vol_ratio > vol_expansion_threshold:
                # Higher vol = higher risk, consider exit
                current_pnl = position.get('current_pnl_pct', 0)
                
                if current_pnl > 0.02:  # Profitable - partial exit
                    exit_size = 0.5
                    urgency = 'medium'
                elif current_pnl > -0.02:  # Near breakeven - larger exit
                    exit_size = 0.7
                    urgency = 'high'
                else:  # Losing - full exit
                    exit_size = 1.0
                    urgency = 'high'
                    
                signals.append({
                    'type': 'volatility_exit',
                    'current_vol': current_vol,
                    'baseline_vol': baseline_vol,
                    'vol_ratio': vol_ratio,
                    'threshold': vol_expansion_threshold,
                    'exit_size_pct': exit_size,
                    'confidence': 0.6,
                    'urgency': urgency,
                    'reason': f'Volatility expansion: {vol_ratio:.1f}x baseline'
                })
                
            return signals
            
        except Exception as e:
            self.logger.error(f"Volatility exit analysis failed: {e}")
            return []
            
    def _analyze_support_resistance_exit(self, position, market_data):
        """Analyze support/resistance level exits"""
        try:
            signals = []
            
            if not self.exit_strategies['support_resistance_exit']['enabled'] or len(market_data) < 50:
                return signals
                
            current_price = market_data['close'].iloc[-1]
            direction = position['direction']
            
            # Find key levels
            resistance_levels = self._find_resistance_levels(market_data)
            support_levels = self._find_support_levels(market_data)
            
            approach_distance = self.exit_strategies['support_resistance_exit']['approach_distance']
            
            # Check for approaching key levels
            if direction == 'long':
                # Check resistance levels for long positions
                for level in resistance_levels:
                    distance_to_level = abs(current_price - level) / current_price
                    
                    if distance_to_level < approach_distance and current_price < level:
                        level_strength = self._calculate_level_strength(level, market_data)
                        
                        if level_strength > self.exit_strategies['support_resistance_exit']['level_strength_threshold']:
                            signals.append({
                                'type': 'resistance_approach',
                                'level_price': level,
                                'current_price': current_price,
                                'distance_pct': distance_to_level,
                                'level_strength': level_strength,
                                'exit_size_pct': 0.6,  # Partial exit before resistance
                                'confidence': level_strength,
                                'urgency': 'medium',
                                'reason': f'Approaching strong resistance at {level:.6f}'
                            })
                            
            else:  # Short position
                # Check support levels for short positions
                for level in support_levels:
                    distance_to_level = abs(current_price - level) / current_price
                    
                    if distance_to_level < approach_distance and current_price > level:
                        level_strength = self._calculate_level_strength(level, market_data)
                        
                        if level_strength > self.exit_strategies['support_resistance_exit']['level_strength_threshold']:
                            signals.append({
                                'type': 'support_approach',
                                'level_price': level,
                                'current_price': current_price,
                                'distance_pct': distance_to_level,
                                'level_strength': level_strength,
                                'exit_size_pct': 0.6,
                                'confidence': level_strength,
                                'urgency': 'medium',
                                'reason': f'Approaching strong support at {level:.6f}'
                            })
                            
            return signals
            
        except Exception as e:
            self.logger.error(f"Support/resistance exit analysis failed: {e}")
            return []
            
    def _find_resistance_levels(self, market_data):
        """Find key resistance levels"""
        try:
            highs = market_data['high'].rolling(5).max()
            resistance_candidates = []
            
            # Find local highs
            for i in range(2, len(highs) - 2):
                if (highs.iloc[i] > highs.iloc[i-1] and highs.iloc[i] > highs.iloc[i-2] and
                    highs.iloc[i] > highs.iloc[i+1] and highs.iloc[i] > highs.iloc[i+2]):
                    resistance_candidates.append(highs.iloc[i])
                    
            # Return top 3 most recent resistance levels
            return sorted(resistance_candidates, reverse=True)[:3]
            
        except:
            return []
            
    def _find_support_levels(self, market_data):
        """Find key support levels"""
        try:
            lows = market_data['low'].rolling(5).min()
            support_candidates = []
            
            # Find local lows
            for i in range(2, len(lows) - 2):
                if (lows.iloc[i] < lows.iloc[i-1] and lows.iloc[i] < lows.iloc[i-2] and
                    lows.iloc[i] < lows.iloc[i+1] and lows.iloc[i] < lows.iloc[i+2]):
                    support_candidates.append(lows.iloc[i])
                    
            # Return top 3 most recent support levels
            return sorted(support_candidates)[-3:]
            
        except:
            return []
            
    def _calculate_level_strength(self, level, market_data):
        """Calculate strength of support/resistance level"""
        try:
            touches = 0
            volume_at_level = 0
            
            # Count how many times price approached this level
            for i, price in enumerate(market_data['close']):
                if abs(price - level) / level < 0.01:  # Within 1%
                    touches += 1
                    volume_at_level += market_data['volume'].iloc[i]
                    
            # Strength based on touches and volume
            touch_strength = min(touches / 3, 1.0)  # Max strength at 3+ touches
            volume_strength = min(volume_at_level / market_data['volume'].sum() * 10, 1.0)
            
            return (touch_strength * 0.7 + volume_strength * 0.3)
            
        except:
            return 0.5
            
    def _analyze_smart_exit_features(self, position, market_data, market_context):
        """Analyze advanced smart exit features"""
        try:
            signals = []
            
            current_pnl = position.get('current_pnl_pct', 0)
            
            # 1. Profit acceleration exit
            if self.smart_exit_features['profit_acceleration'] and current_pnl > 0.05:
                profit_acceleration = self._detect_profit_acceleration(position, market_data)
                
                if profit_acceleration > 0.8:  # Profits accelerating fast
                    signals.append({
                        'type': 'profit_acceleration',
                        'acceleration_score': profit_acceleration,
                        'exit_size_pct': 0.4,  # Take some profits
                        'confidence': 0.7,
                        'urgency': 'medium',
                        'reason': f'Profit acceleration detected: {profit_acceleration:.2f}'
                    })
                    
            # 2. Loss mitigation
            if self.smart_exit_features['loss_mitigation'] and current_pnl < -0.02:
                loss_severity = self._assess_loss_severity(position, market_data)
                
                if loss_severity > 0.7:
                    signals.append({
                        'type': 'loss_mitigation',
                        'loss_severity': loss_severity,
                        'exit_size_pct': 0.8,  # Cut most of the loss
                        'confidence': 0.8,
                        'urgency': 'high',
                        'reason': f'Severe loss detected: {loss_severity:.2f}'
                    })
                    
            # 3. Correlation exit
            if self.smart_exit_features['correlation_exits']:
                correlation_signal = self._check_correlation_exit(position, market_context)
                if correlation_signal:
                    signals.append(correlation_signal)
                    
            # 4. News reaction exit
            if self.smart_exit_features['news_reaction_exits']:
                news_signal = self._check_news_exit(position, market_context)
                if news_signal:
                    signals.append(news_signal)
                    
            # 5. Whale movement exit
            if self.smart_exit_features['whale_movement_exits']:
                whale_signal = self._check_whale_exit(position, market_context)
                if whale_signal:
                    signals.append(whale_signal)
                    
            return signals
            
        except Exception as e:
            self.logger.error(f"Smart exit features analysis failed: {e}")
            return []
            
    def _detect_profit_acceleration(self, position, market_data):
        """Detect if profits are accelerating rapidly"""
        try:
            # Get position PnL history
            position_history = position.get('position_history', [])
            
            if len(position_history) < 5:
                return 0
                
            # Calculate PnL velocity and acceleration
            recent_pnls = [h['pnl_pct'] for h in position_history[-5:]]
            pnl_changes = [recent_pnls[i] - recent_pnls[i-1] for i in range(1, len(recent_pnls))]
            
            if len(pnl_changes) < 2:
                return 0
                
            # Acceleration = change in velocity
            pnl_velocity = np.mean(pnl_changes)
            pnl_acceleration = pnl_changes[-1] - pnl_changes[0]
            
            # Normalize acceleration score
            acceleration_score = min(abs(pnl_acceleration) * 100, 1.0)
            
            return acceleration_score if pnl_velocity > 0 else 0
            
        except:
            return 0
            
    def _assess_loss_severity(self, position, market_data):
        """Assess how severe current losses are"""
        try:
            current_pnl = position.get('current_pnl_pct', 0)
            
            if current_pnl >= 0:
                return 0
                
            # Factors for loss severity
            loss_magnitude = abs(current_pnl)
            
            # Time factor - longer losses are worse
            hold_time = time.time() - position['entry_time']
            time_factor = min(hold_time / 3600, 2.0)  # Max factor at 2 hours
            
            # Volatility factor - high vol makes losses more dangerous
            if len(market_data) >= 10:
                returns = market_data['close'].pct_change().tail(10)
                volatility = returns.std()
                vol_factor = min(volatility * 20, 2.0)
            else:
                vol_factor = 1.0
                
            # Momentum factor - if momentum is against us
            momentum_score = self._calculate_exit_momentum(market_data)
            direction = position['direction']
            
            if direction == 'long':
                momentum_against = momentum_score < 0.3
            else:
                momentum_against = momentum_score > 0.7
                
            momentum_factor = 1.5 if momentum_against else 1.0
            
            # Combined severity score
            severity = (loss_magnitude * 5) * time_factor * vol_factor * momentum_factor
            
            return min(severity, 1.0)
            
        except:
            return 0.5
            
    def _check_correlation_exit(self, position, market_context):
        """Check if correlated positions suggest exit"""
        try:
            symbol = position['symbol']
            base_asset = symbol.replace('USDT', '')
            
            # Find correlated positions
            correlated_positions = []
            for other_pos in self.position_system.active_positions.values():
                if other_pos['symbol'] != symbol:
                    correlation = self.position_system._get_symbol_correlation(symbol, other_pos['symbol'])
                    
                    if correlation and abs(correlation) > 0.6:  # High correlation
                        correlated_positions.append({
                            'position': other_pos,
                            'correlation': correlation,
                            'pnl': other_pos.get('current_pnl_pct', 0)
                        })
                        
            if not correlated_positions:
                return None
                
            # Check if correlated positions are performing poorly
            avg_corr_pnl = np.mean([cp['pnl'] for cp in correlated_positions])
            poor_performers = len([cp for cp in correlated_positions if cp['pnl'] < -0.03])
            
            if poor_performers >= len(correlated_positions) * 0.6:  # 60%+ performing poorly
                return {
                    'type': 'correlation_exit',
                    'correlated_count': len(correlated_positions),
                    'poor_performers': poor_performers,
                    'avg_corr_pnl': avg_corr_pnl,
                    'exit_size_pct': 0.5,
                    'confidence': 0.6,
                    'urgency': 'medium',
                    'reason': f'Correlated positions underperforming: {poor_performers}/{len(correlated_positions)}'
                }
                
            return None
            
        except Exception as e:
            self.logger.debug(f"Correlation exit check failed: {e}")
            return None
            
    def _check_news_exit(self, position, market_context):
        """Check for news-based exit signals"""
        try:
            # Get recent news sentiment for symbol
            symbol = position['symbol']
            direction = position['direction']
            
            # Would integrate with news system - simplified for now
            recent_sentiment = market_context.get('sentiment', {}).get(symbol, {})
            
            if not recent_sentiment:
                return None
                
            sentiment_score = recent_sentiment.get('sentiment_score', 0)
            is_extreme = recent_sentiment.get('is_extreme', False)
            
            # Exit if news sentiment strongly opposes position
            if direction == 'long' and sentiment_score < -0.6 and is_extreme:
                return {
                    'type': 'negative_news_exit',
                    'sentiment_score': sentiment_score,
                    'is_extreme': is_extreme,
                    'exit_size_pct': 0.7,
                    'confidence': 0.8,
                    'urgency': 'high',
                    'reason': f'Extreme negative sentiment: {sentiment_score:.2f}'
                }
            elif direction == 'short' and sentiment_score > 0.6 and is_extreme:
                return {
                    'type': 'positive_news_exit',
                    'sentiment_score': sentiment_score,
                    'is_extreme': is_extreme,
                    'exit_size_pct': 0.7,
                    'confidence': 0.8,
                    'urgency': 'high',
                    'reason': f'Extreme positive sentiment: {sentiment_score:.2f}'
                }
                
            return None
            
        except Exception as e:
            self.logger.debug(f"News exit check failed: {e}")
            return None
            
    def _check_whale_exit(self, position, market_context):
        """Check for whale movement exit signals"""
        try:
            symbol = position['symbol']
            direction = position['direction']
            
            # Get recent whale activity
            whale_activity = market_context.get('whale_activity', {}).get(symbol, [])
            
            if not whale_activity:
                return None
                
            # Look for large opposing flows
            opposing_flows = []
            for activity in whale_activity[-5:]:  # Last 5 whale activities
                activity_direction = activity.get('direction', '')
                activity_size = activity.get('size_usd', 0)
                
                # Check if whale activity opposes our position
                if ((direction == 'long' and activity_direction == 'sell') or
                    (direction == 'short' and activity_direction == 'buy')):
                    
                    if activity_size > 1000000:  # $1M+ whale activity
                        opposing_flows.append(activity)
                        
            if len(opposing_flows) >= 2:  # Multiple large opposing flows
                total_opposing_size = sum(flow['size_usd'] for flow in opposing_flows)
                
                return {
                    'type': 'whale_opposition_exit',
                    'opposing_flows': len(opposing_flows),
                    'total_size': total_opposing_size,
                    'exit_size_pct': 0.6,
                    'confidence': 0.7,
                    'urgency': 'high',
                    'reason': f'Large whale flows opposing position: ${total_opposing_size/1000000:.1f}M'
                }
                
            return None
            
        except Exception as e:
            self.logger.debug(f"Whale exit check failed: {e}")
            return None
            
    def _combine_exit_signals(self, exit_signals, position, current_pnl):
        """Combine multiple exit signals into final recommendation"""
        try:
            if not exit_signals:
                return {
                    'recommended_action': 'hold',
                    'urgency': 'low',
                    'confidence': 0.0,
                    'exit_size_pct': 0.0,
                    'reason': 'No exit signals detected'
                }
                
            # Sort signals by urgency and confidence
            signal_priorities = {
                'very_high': 4,
                'high': 3,
                'medium': 2,
                'low': 1
            }
            
            sorted_signals = sorted(
                exit_signals, 
                key=lambda x: (signal_priorities.get(x.get('urgency', 'low'), 1), x.get('confidence', 0)),
                reverse=True
            )
            
            # Get the highest priority signal
            primary_signal = sorted_signals[0]
            
            # Check for multiple high-urgency signals (amplify action)
            high_urgency_signals = [s for s in exit_signals if s.get('urgency') in ['high', 'very_high']]
            
            # Calculate combined exit size
            if len(high_urgency_signals) > 1:
                # Multiple urgent signals - larger exit
                base_exit_size = primary_signal.get('exit_size_pct', 0)
                amplification = min(1 + (len(high_urgency_signals) - 1) * 0.2, 1.5)
                combined_exit_size = min(base_exit_size * amplification, 1.0)
            else:
                combined_exit_size = primary_signal.get('exit_size_pct', 0)
                
            # Determine final action
            if combined_exit_size >= 0.8:
                action = 'close_position'
            elif combined_exit_size >= 0.3:
                action = 'partial_exit'
            else:
                action = 'reduce_position'
                
            # Combine reasons
            if len(exit_signals) > 1:
                reason = f"{primary_signal.get('reason', 'Exit signal')} (+{len(exit_signals)-1} other signals)"
            else:
                reason = primary_signal.get('reason', 'Exit signal detected')
                
            return {
                'recommended_action': action,
                'urgency': primary_signal.get('urgency', 'medium'),
                'confidence': primary_signal.get('confidence', 0.5),
                'exit_size_pct': combined_exit_size,
                'reason': reason,
                'primary_signal_type': primary_signal.get('type', 'unknown'),
                'signal_count': len(exit_signals),
                'high_urgency_count': len(high_urgency_signals)
            }
            
        except Exception as e:
            self.logger.error(f"Exit signal combination failed: {e}")
            return {
                'recommended_action': 'hold',
                'urgency': 'low',
                'confidence': 0.0,
                'exit_size_pct': 0.0,
                'reason': 'Signal processing error'
            }
            
    def execute_exit_recommendation(self, position_id, exit_analysis):
        """Execute the exit recommendation"""
        try:
            if position_id not in self.position_system.active_positions:
                return {'success': False, 'error': 'Position not found'}
                
            position = self.position_system.active_positions[position_id]
            action = exit_analysis['recommended_action']
            exit_size_pct = exit_analysis['exit_size_pct']
            
            # Validate exit size
            if exit_size_pct <= 0:
                return {'success': False, 'error': 'Invalid exit size'}
                
            execution_result = {
                'position_id': position_id,
                'symbol': position['symbol'],
                'action': action,
                'exit_size_pct': exit_size_pct,
                'timestamp': time.time()
            }
            
            if action == 'close_position':
                # Close entire position
                close_result = self.position_system.close_position(
                    position_id, 
                    exit_analysis['reason']
                )
                
                if close_result:
                    execution_result.update({
                        'success': True,
                        'exit_price': close_result.get('exit_price'),
                        'final_pnl_pct': close_result.get('final_pnl_pct'),
                        'exit_type': 'full_close'
                    })
                    
                    # Track exit performance
                    self._track_exit_performance(exit_analysis, close_result)
                else:
                    execution_result.update({
                        'success': False,
                        'error': 'Position close failed'
                    })
                    
            elif action in ['partial_exit', 'reduce_position']:
                # Partial position close
                if self.config.PAPER_MODE:
                    # Paper mode: reduce position size
                    original_quantity = position['quantity']
                    original_value = position['position_value_usd']
                    
                    new_quantity = original_quantity * (1 - exit_size_pct)
                    new_value = original_value * (1 - exit_size_pct)
                    
                    position['quantity'] = new_quantity
                    position['position_value_usd'] = new_value
                    
                    # Calculate realized PnL from partial close
                    current_pnl = position.get('current_pnl_pct', 0)
                    realized_pnl = original_value * exit_size_pct * current_pnl
                    
                    execution_result.update({
                        'success': True,
                        'original_quantity': original_quantity,
                        'remaining_quantity': new_quantity,
                        'exit_quantity': original_quantity - new_quantity,
                        'realized_pnl': realized_pnl,
                        'exit_type': 'partial_close'
                    })
                    
                    self.logger.info(
                        f"üìâ Partial exit: {position['symbol']} reduced by {exit_size_pct:.1%} "
                        f"(${realized_pnl:.2f} PnL) - {exit_analysis['reason']}"
                    )
                else:
                    # Real trading: would place partial sell order
                    execution_result.update({
                        'success': True,
                        'exit_type': 'partial_close',
                        'note': 'Real trading execution would be implemented here'
                    })
                    
            # Update exit statistics
            self.exit_performance['total_exits'] += 1
            self.exit_performance['exit_reasons'][exit_analysis['primary_signal_type']] += 1
            
            return execution_result
            
        except Exception as e:
            self.logger.error(f"Exit execution failed for {position_id}: {e}")
            return {'success': False, 'error': str(e)}
            
    def _track_exit_performance(self, exit_analysis, close_result):
        """Track exit performance for strategy optimization"""
        try:
            final_pnl = close_result.get('final_pnl_pct', 0)
            
            # Track if exit was profitable
            if final_pnl > 0:
                self.exit_performance['profitable_exits'] += 1
                
            # Calculate exit efficiency (how close to max profit)
            max_profit = close_result.get('max_profit_pct', final_pnl)
            if max_profit > 0:
                exit_efficiency = final_pnl / max_profit
                self.exit_performance['avg_exit_efficiency'].append(exit_efficiency)
                
            # Track best performing exit strategy
            signal_type = exit_analysis['primary_signal_type']
            if signal_type not in self.exit_performance:
                self.exit_performance[signal_type] = {'count': 0, 'total_pnl': 0}
                
            self.exit_performance[signal_type]['count'] += 1
            self.exit_performance[signal_type]['total_pnl'] += final_pnl
            
            # Update best strategy
            best_strategy = None
            best_avg_pnl = -float('inf')
            
            for strategy, stats in self.exit_performance.items():
                if isinstance(stats, dict) and stats.get('count', 0) >= 5:
                    avg_pnl = stats['total_pnl'] / stats['count']
                    if avg_pnl > best_avg_pnl:
                        best_avg_pnl = avg_pnl
                        best_strategy = strategy
                        
            self.exit_performance['best_exit_strategy'] = best_strategy
            
        except Exception as e:
            self.logger.error(f"Exit performance tracking failed: {e}")
            
    def get_exit_performance_stats(self):
        """Get exit strategy performance statistics"""
        try:
            total_exits = self.exit_performance['total_exits']
            profitable_exits = self.exit_performance['profitable_exits']
            
            win_rate = profitable_exits / total_exits if total_exits > 0 else 0
            
            # Exit efficiency
            efficiency_scores = list(self.exit_performance['avg_exit_efficiency'])
            avg_efficiency = np.mean(efficiency_scores) if efficiency_scores else 0
            
            # Strategy performance breakdown
            strategy_performance = {}
            for strategy, stats in self.exit_performance.items():
                if isinstance(stats, dict) and 'count' in stats:
                    strategy_performance[strategy] = {
                        'exit_count': stats['count'],
                        'total_pnl': stats['total_pnl'],
                        'avg_pnl': stats['total_pnl'] / stats['count'] if stats['count'] > 0 else 0
                    }
                    
            # Most common exit reasons
            exit_reasons = dict(self.exit_performance['exit_reasons'])
            most_common_reason = max(exit_reasons.items(), key=lambda x: x[1])[0] if exit_reasons else None
            
            return {
                'overall_performance': {
                    'total_exits': total_exits,
                    'profitable_exits': profitable_exits,
                    'win_rate': win_rate,
                    'avg_exit_efficiency': avg_efficiency
                },
                'strategy_performance': strategy_performance,
                'exit_patterns': {
                    'most_common_reason': most_common_reason,
                    'exit_reasons_breakdown': exit_reasons,
                    'best_performing_strategy': self.exit_performance.get('best_exit_strategy')
                },
                'efficiency_metrics': {
                    'min_efficiency': min(efficiency_scores) if efficiency_scores else 0,
                    'max_efficiency': max(efficiency_scores) if efficiency_scores else 0,
                    'efficiency_std': np.std(efficiency_scores) if efficiency_scores else 0
                }
            }
            
        except Exception as e:
            self.logger.error(f"Exit performance stats calculation failed: {e}")
            return {}
            
    def optimize_exit_strategies(self, performance_data):
        """Optimize exit strategy parameters based on performance"""
        try:
            if not performance_data or performance_data['overall_performance']['total_exits'] < 20:
                return  # Need sufficient data for optimization
                
            strategy_performance = performance_data['strategy_performance']
            
            # Optimize take profit levels
            tp_performance = strategy_performance.get('take_profit', {})
            if tp_performance.get('exit_count', 0) >= 10:
                avg_pnl = tp_performance.get('avg_pnl', 0)
                
                if avg_pnl < 0.02:  # Underperforming
                    # Adjust take profit levels to be more conservative
                    current_levels = self.exit_strategies['multiple_take_profits']['levels']
                    self.exit_strategies['multiple_take_profits']['levels'] = [
                        level * 0.9 for level in current_levels  # 10% more conservative
                    ]
                    self.logger.info("Optimized take profit levels to be more conservative")
                    
            # Optimize trailing stop parameters
            trail_performance = strategy_performance.get('trailing_stop', {})
            if trail_performance.get('exit_count', 0) >= 10:
                avg_pnl = trail_performance.get('avg_pnl', 0)
                
                if avg_pnl > 0.05:  # Performing well
                    # Tighten trailing stop for better profit capture
                    current_distance = self.exit_strategies['trailing_stop']['trail_distance']
                    self.exit_strategies['trailing_stop']['trail_distance'] = max(
                        current_distance * 0.9, 0.01  # Don't go below 1%
                    )
                    self.logger.info("Optimized trailing stop to be tighter")
                    
            # Optimize momentum exit sensitivity
            momentum_performance = strategy_performance.get('momentum_exit', {})
            if momentum_performance.get('exit_count', 0) >= 10:
                avg_pnl = momentum_performance.get('avg_pnl', 0)
                
                if avg_pnl < -0.01:  # Too sensitive
                    # Reduce momentum sensitivity
                    current_threshold = self.exit_strategies['momentum_exit']['momentum_threshold']
                    self.exit_strategies['momentum_exit']['momentum_threshold'] = min(
                        current_threshold + 0.05, 0.9  # Don't exceed 90%
                    )
                    self.logger.info("Reduced momentum exit sensitivity")
                    
            self.logger.info("Exit strategy optimization completed")
            
        except Exception as e:
            self.logger.error(f"Exit strategy optimization failed: {e}")
            
    def get_position_exit_recommendations(self):
        """Get exit recommendations for all active positions"""
        try:
            recommendations = []
            
            for position_id, position in self.position_system.active_positions.items():
                # Get market data for position
                symbol = position['symbol']
                market_data = self.binance_client.get_klines(symbol, '5m', 100)
                
                if market_data is not None and len(market_data) > 10:
                    # Analyze exit opportunities
                    exit_analysis = self.analyze_exit_opportunities(
                        position_id, market_data, {}  # Empty market context for now
                    )
                    
                    if (exit_analysis and 
                        exit_analysis['recommended_action'] != 'hold' and
                        exit_analysis['confidence'] > 0.5):
                        
                        recommendations.append({
                            'position_id': position_id,
                            'symbol': symbol,
                            'current_pnl_pct': position.get('current_pnl_pct', 0),
                            'hold_time_hours': (time.time() - position['entry_time']) / 3600,
                            'exit_recommendation': exit_analysis,
                            'priority': self._calculate_exit_priority(exit_analysis, position)
                        })
                        
            # Sort by priority
            recommendations.sort(key=lambda x: x['priority'], reverse=True)
            
            return recommendations
            
        except Exception as e:
            self.logger.error(f"Exit recommendations generation failed: {e}")
            return []
            
    def _calculate_exit_priority(self, exit_analysis, position):
        """Calculate priority score for exit recommendation"""
        try:
            base_priority = exit_analysis['confidence']
            
            # Urgency multiplier
            urgency_multipliers = {
                'very_high': 2.0,
                'high': 1.5,
                'medium': 1.0,
                'low': 0.7
            }
            
            urgency_multiplier = urgency_multipliers.get(exit_analysis['urgency'], 1.0)
            
            # Loss mitigation boost
            current_pnl = position.get('current_pnl_pct', 0)
            loss_boost = abs(min(current_pnl, 0)) * 2  # Higher priority for larger losses
            
            # Position size factor
            position_value = position.get('position_value_usd', 0)
            size_factor = min(position_value / 10000, 2.0)  # Larger positions get priority
            
            priority = (base_priority * urgency_multiplier) + loss_boost + (size_factor * 0.1)
            
            return min(priority, 3.0)  # Cap at 3.0
            
        except:
            return 0.5

def initialize_dynamic_exit_system(config, binance_client, memory_manager, position_system, risk_engine):
    """Initialize complete dynamic exit system"""
    try:
        # Initialize dynamic exit system
        exit_system = DynamicExitSystem(config, binance_client, memory_manager, position_system, risk_engine)
        
        logger = logging.getLogger('MonsterBot.DynamicExit')
        logger.info("üéØ Dynamic exit system initialized")
        logger.info("   üìà Multiple take profits: 4 levels (30%, 60%, 90%, 130%)")
        logger.info("   üìâ Trailing stops: 2% activation, 1.5% trail distance")
        logger.info("   ‚ö° Momentum exits: 80% momentum loss threshold")
        logger.info("   ‚è∞ Time-based exits: Adaptive hold times")
        logger.info("   üìä Volatility exits: 2x expansion threshold")
        logger.info("   üéØ Support/resistance: 70% level strength threshold")
        logger.info("   üß† Smart features: 5 advanced exit signals")
        
        return {
            'exit_system': exit_system,
            'multiple_take_profits': True,
            'trailing_stops': True,
            'momentum_exits': True,
            'time_based_exits': True,
            'volatility_exits': True,
            'support_resistance_exits': True,
            'smart_features': 5
        }
        
    except Exception as e:
        logger = logging.getLogger('MonsterBot.DynamicExit')
        logger.error(f"Dynamic exit system initialization failed: {e}")
        raise   
        
class CorrelationMatrixSystem:
    """Advanced correlation analysis and portfolio diversification management"""
    
    def __init__(self, config, binance_client, memory_manager, position_system):
        self.config = config
        self.binance_client = binance_client
        self.memory_manager = memory_manager
        self.position_system = position_system
        self.logger = logging.getLogger('MonsterBot.CorrelationMatrix')
        
        # Correlation analysis parameters
        self.correlation_params = {
            'lookback_periods': {
                'short_term': 50,    # 50 periods for short-term correlation
                'medium_term': 200,  # 200 periods for medium-term
                'long_term': 500     # 500 periods for long-term
            },
            'update_frequency': 300,    # Update every 5 minutes
            'significance_threshold': 0.05,  # Statistical significance
            'min_data_points': 30,     # Minimum data for correlation
            'correlation_decay': 0.95,  # Exponential decay factor
            'regime_detection': True    # Detect correlation regime changes
        }
        
        # Correlation matrices for different timeframes
        self.correlation_matrices = {
            'short_term': {},
            'medium_term': {},
            'long_term': {},
            'real_time': {}
        }
        
        # Correlation history and tracking
        self.correlation_history = defaultdict(lambda: defaultdict(deque))
        self.correlation_regimes = defaultdict(str)
        self.correlation_changes = defaultdict(list)
        
        # Portfolio exposure tracking
        self.exposure_analysis = {
            'sector_exposure': defaultdict(float),
            'correlation_clusters': [],
            'diversification_score': 0.0,
            'concentration_risk': 0.0,
            'regime_stability': 0.0
        }
        
        # Symbol groups and classifications
        self.symbol_groups = {
            'major_coins': ['BTCUSDT', 'ETHUSDT', 'BNBUSDT'],
            'large_cap_alts': ['ADAUSDT', 'DOTUSDT', 'LINKUSDT', 'LTCUSDT', 'XLMUSDT'],
            'mid_cap_alts': ['UNIUSDT', 'AAVEUSDT', 'COMPUSDT', 'MKRUSDT', 'YFIUSDT'],
            'small_cap_alts': ['ALPHAUSDT', '1INCHUSDT', 'BANDUSDT', 'CRVUSDT'],
            'defi_tokens': ['UNIUSDT', 'AAVEUSDT', 'COMPUSDT', 'MKRUSDT', 'SUSHIUSDT'],
            'layer1_protocols': ['ETHUSDT', 'ADAUSDT', 'DOTUSDT', 'SOLUSDT', 'AVAXUSDT'],
            'exchange_tokens': ['BNBUSDT', 'FTTUSDT', 'CAKEUSDT', 'KCSUSDT']
        }
        
        # Correlation thresholds and limits
        self.correlation_limits = {
            'high_correlation': 0.7,
            'medium_correlation': 0.4,
            'low_correlation': 0.2,
            'max_cluster_exposure': 0.6,     # Max 60% in one cluster
            'max_high_corr_pairs': 3,        # Max 3 highly correlated pairs
            'diversification_target': 0.8    # Target diversification score
        }
        
        # Performance tracking
        self.correlation_stats = {
            'total_calculations': 0,
            'regime_changes_detected': 0,
            'correlation_warnings_issued': 0,
            'diversification_improvements': 0
        }
        
        # Initialize correlation system
        self.initialize_correlation_system()
        
    def initialize_correlation_system(self):
        """Initialize correlation analysis system"""
        try:
            # Load historical correlation data if available
            self.load_correlation_cache()
            
            # Start correlation update thread
            self.start_correlation_monitoring()
            
            self.logger.info("üìä Correlation matrix system initialized")
            self.logger.info(f"   üîÑ Update frequency: {self.correlation_params['update_frequency']}s")
            self.logger.info(f"   üìà Lookback periods: {list(self.correlation_params['lookback_periods'].values())}")
            self.logger.info(f"   üéØ Correlation limits: {self.correlation_limits['high_correlation']:.1%}")
            self.logger.info(f"   üèóÔ∏è Symbol groups: {len(self.symbol_groups)}")
            
        except Exception as e:
            self.logger.error(f"Correlation system initialization failed: {e}")
            
    def start_correlation_monitoring(self):
        """Start background correlation monitoring"""
        def correlation_monitor():
            while True:
                try:
                    self.update_correlation_matrices()
                    self.analyze_portfolio_correlations()
                    self.detect_correlation_regime_changes()
                    time.sleep(self.correlation_params['update_frequency'])
                except Exception as e:
                    self.logger.error(f"Correlation monitoring error: {e}")
                    time.sleep(30)  # Wait 30 seconds on error
                    
        monitor_thread = threading.Thread(target=correlation_monitor, daemon=True)
        monitor_thread.start()
        
    def update_correlation_matrices(self):
        """Update correlation matrices for all timeframes"""
        try:
            # Get list of symbols to analyze
            symbols_to_analyze = self._get_symbols_for_analysis()
            
            if len(symbols_to_analyze) < 2:
                return
                
            # Update correlations for each timeframe
            for timeframe, periods in self.correlation_params['lookback_periods'].items():
                correlation_matrix = self.calculate_correlation_matrix(
                    symbols_to_analyze, periods, timeframe
                )
                
                if correlation_matrix is not None:
                    self.correlation_matrices[timeframe] = correlation_matrix
                    
            # Update real-time correlations (shorter lookback)
            real_time_matrix = self.calculate_correlation_matrix(
                symbols_to_analyze, 20, 'real_time'
            )
            
            if real_time_matrix is not None:
                self.correlation_matrices['real_time'] = real_time_matrix
                
            self.correlation_stats['total_calculations'] += 1
            
        except Exception as e:
            self.logger.error(f"Correlation matrix update failed: {e}")
            
    def calculate_correlation_matrix(self, symbols, lookback_periods, timeframe):
        """Calculate correlation matrix for given symbols and timeframe"""
        try:
            # Get price data for all symbols
            price_data = {}
            
            for symbol in symbols:
                # Get cached data first
                cache_key = f'price_data_{symbol}_{timeframe}'
                cached_data = self.memory_manager.cache_get(cache_key, 'market_data')
                
                if cached_data is not None:
                    price_data[symbol] = cached_data
                else:
                    # Fetch new data
                    interval = '5m' if timeframe == 'real_time' else '15m'
                    data = self.binance_client.get_klines(symbol, interval, lookback_periods + 10)
                    
                    if data is not None and len(data) >= self.correlation_params['min_data_points']:
                        returns = data['close'].pct_change().dropna()
                        price_data[symbol] = returns.tail(lookback_periods)
                        
                        # Cache the data
                        self.memory_manager.cache_set(cache_key, price_data[symbol], 'market_data')
                        
            if len(price_data) < 2:
                return None
                
            # Align data (ensure same timestamps)
            aligned_data = pd.DataFrame(price_data)
            aligned_data = aligned_data.dropna()
            
            if len(aligned_data) < self.correlation_params['min_data_points']:
                return None
                
            # Calculate correlation matrix
            correlation_matrix = aligned_data.corr()
            
            # Add metadata
            correlation_matrix.attrs = {
                'timestamp': time.time(),
                'timeframe': timeframe,
                'data_points': len(aligned_data),
                'symbols': list(symbols)
            }
            
            # Store correlation history
            self._store_correlation_history(correlation_matrix, timeframe)
            
            return correlation_matrix
            
        except Exception as e:
            self.logger.error(f"Correlation calculation failed for {timeframe}: {e}")
            return None
            
    def _get_symbols_for_analysis(self):
        """Get symbols that need correlation analysis"""
        symbols = set()
        
        # Add symbols from active positions
        for position in self.position_system.active_positions.values():
            symbols.add(position['symbol'])
            
        # Add major market symbols for reference
        symbols.update(self.symbol_groups['major_coins'])
        
        # Add symbols from preferred list
        symbols.update(self.config.PREFERRED_SYMBOLS[:10])  # Top 10 preferred
        
        return list(symbols)
        
    def _store_correlation_history(self, correlation_matrix, timeframe):
        """Store correlation history for trend analysis"""
        try:
            timestamp = time.time()
            
            # Store pairwise correlations
            for symbol1 in correlation_matrix.index:
                for symbol2 in correlation_matrix.columns:
                    if symbol1 != symbol2:
                        correlation_value = correlation_matrix.loc[symbol1, symbol2]
                        
                        if not np.isnan(correlation_value):
                            pair_key = f"{min(symbol1, symbol2)}_{max(symbol1, symbol2)}"
                            
                            self.correlation_history[timeframe][pair_key].append({
                                'timestamp': timestamp,
                                'correlation': correlation_value
                            })
                            
                            # Keep only recent history
                            max_history_points = 1000
                            if len(self.correlation_history[timeframe][pair_key]) > max_history_points:
                                self.correlation_history[timeframe][pair_key].popleft()
                                
        except Exception as e:
            self.logger.error(f"Correlation history storage failed: {e}")
            
    def analyze_portfolio_correlations(self):
        """Analyze correlations in current portfolio"""
        try:
            active_positions = self.position_system.active_positions
            
            if len(active_positions) < 2:
                self.exposure_analysis['diversification_score'] = 1.0
                return
                
            # Get symbols in portfolio
            portfolio_symbols = [pos['symbol'] for pos in active_positions.values()]
            
            # Get correlation matrix for portfolio symbols
            portfolio_correlations = self._get_portfolio_correlation_submatrix(portfolio_symbols)
            
            if portfolio_correlations is None:
                return
                
            # Analyze correlation clusters
            correlation_clusters = self._identify_correlation_clusters(portfolio_correlations)
            
            # Calculate exposure by cluster
            cluster_exposures = self._calculate_cluster_exposures(correlation_clusters, active_positions)
            
            # Calculate diversification metrics
            diversification_score = self._calculate_diversification_score(portfolio_correlations, active_positions)
            concentration_risk = self._calculate_concentration_risk(cluster_exposures)
            
            # Update exposure analysis
            self.exposure_analysis.update({
                'correlation_clusters': correlation_clusters,
                'cluster_exposures': cluster_exposures,
                'diversification_score': diversification_score,
                'concentration_risk': concentration_risk,
                'portfolio_correlations': portfolio_correlations.to_dict(),
                'last_update': time.time()
            })
            
            # Check for correlation warnings
            self._check_correlation_warnings(portfolio_correlations, cluster_exposures)
            
        except Exception as e:
            self.logger.error(f"Portfolio correlation analysis failed: {e}")
            
    def _get_portfolio_correlation_submatrix(self, portfolio_symbols):
        """Get correlation submatrix for portfolio symbols"""
        try:
            # Try to get from medium-term matrix first
            correlation_matrix = self.correlation_matrices.get('medium_term', {})
            
            if not correlation_matrix.empty:
                # Filter for portfolio symbols
                available_symbols = [s for s in portfolio_symbols if s in correlation_matrix.index]
                
                if len(available_symbols) >= 2:
                    return correlation_matrix.loc[available_symbols, available_symbols]
                    
            # Fallback: calculate directly for portfolio symbols
            return self.calculate_correlation_matrix(portfolio_symbols, 100, 'portfolio')
            
        except Exception as e:
            self.logger.error(f"Portfolio correlation submatrix failed: {e}")
            return None
            
    def _identify_correlation_clusters(self, correlation_matrix):
        """Identify groups of highly correlated assets"""
        try:
            clusters = []
            symbols = list(correlation_matrix.index)
            processed_symbols = set()
            
            for symbol in symbols:
                if symbol in processed_symbols:
                    continue
                    
                # Find highly correlated symbols
                cluster = [symbol]
                processed_symbols.add(symbol)
                
                for other_symbol in symbols:
                    if (other_symbol != symbol and 
                        other_symbol not in processed_symbols):
                        
                        correlation = correlation_matrix.loc[symbol, other_symbol]
                        
                        if (not np.isnan(correlation) and 
                            abs(correlation) > self.correlation_limits['high_correlation']):
                            
                            cluster.append(other_symbol)
                            processed_symbols.add(other_symbol)
                            
                if len(cluster) > 1:
                    # Calculate average correlation within cluster
                    cluster_correlations = []
                    for i in range(len(cluster)):
                        for j in range(i + 1, len(cluster)):
                            corr = correlation_matrix.loc[cluster[i], cluster[j]]
                            if not np.isnan(corr):
                                cluster_correlations.append(abs(corr))
                                
                    avg_correlation = np.mean(cluster_correlations) if cluster_correlations else 0
                    
                    clusters.append({
                        'symbols': cluster,
                        'size': len(cluster),
                        'avg_correlation': avg_correlation,
                        'type': self._classify_cluster_type(cluster)
                    })
                    
            return clusters
            
        except Exception as e:
            self.logger.error(f"Correlation cluster identification failed: {e}")
            return []
            
    def _classify_cluster_type(self, cluster_symbols):
        """Classify cluster type based on symbol groups"""
        try:
            # Check which symbol group this cluster belongs to
            for group_name, group_symbols in self.symbol_groups.items():
                cluster_in_group = sum(1 for symbol in cluster_symbols if symbol in group_symbols)
                
                if cluster_in_group >= len(cluster_symbols) * 0.6:  # 60%+ match
                    return group_name
                    
            return 'mixed'
            
        except:
            return 'unknown'
            
    def _calculate_cluster_exposures(self, correlation_clusters, active_positions):
        """Calculate portfolio exposure to each correlation cluster"""
        try:
            cluster_exposures = {}
            total_portfolio_value = sum(pos['position_value_usd'] for pos in active_positions.values())
            
            if total_portfolio_value == 0:
                return cluster_exposures
                
            for i, cluster in enumerate(correlation_clusters):
                cluster_name = f"cluster_{i}_{cluster['type']}"
                cluster_value = 0
                
                for symbol in cluster['symbols']:
                    for position in active_positions.values():
                        if position['symbol'] == symbol:
                            cluster_value += position['position_value_usd']
                            
                cluster_exposure = cluster_value / total_portfolio_value
                
                cluster_exposures[cluster_name] = {
                    'symbols': cluster['symbols'],
                    'exposure_pct': cluster_exposure,
                    'value_usd': cluster_value,
                    'avg_correlation': cluster['avg_correlation'],
                    'type': cluster['type']
                }
                
            return cluster_exposures
            
        except Exception as e:
            self.logger.error(f"Cluster exposure calculation failed: {e}")
            return {}
            
    def _calculate_diversification_score(self, correlation_matrix, active_positions):
        """Calculate portfolio diversification score"""
        try:
            if correlation_matrix is None or len(active_positions) < 2:
                return 1.0
                
            # Get position weights
            total_value = sum(pos['position_value_usd'] for pos in active_positions.values())
            weights = {}
            
            for position in active_positions.values():
                symbol = position['symbol']
                if symbol in correlation_matrix.index:
                    weights[symbol] = position['position_value_usd'] / total_value
                    
            if len(weights) < 2:
                return 1.0
                
            # Calculate portfolio variance
            portfolio_variance = 0
            
            for symbol1, weight1 in weights.items():
                for symbol2, weight2 in weights.items():
                    if symbol1 in correlation_matrix.index and symbol2 in correlation_matrix.columns:
                        correlation = correlation_matrix.loc[symbol1, symbol2]
                        
                        if not np.isnan(correlation):
                            # Assume equal volatility for simplification
                            portfolio_variance += weight1 * weight2 * correlation
                            
            # Diversification ratio (1 = perfectly diversified, 0 = not diversified)
            n_assets = len(weights)
            perfect_diversification_variance = 1 / n_assets  # Equal weights, zero correlation
            
            if portfolio_variance > 0:
                diversification_score = perfect_diversification_variance / portfolio_variance
                return min(diversification_score, 1.0)
            else:
                return 1.0
                
        except Exception as e:
            self.logger.error(f"Diversification score calculation failed: {e}")
            return 0.5
            
    def _calculate_concentration_risk(self, cluster_exposures):
        """Calculate concentration risk from cluster exposures"""
        try:
            if not cluster_exposures:
                return 0.0
                
            # Calculate Herfindahl index for cluster concentration
            exposures = [cluster['exposure_pct'] for cluster in cluster_exposures.values()]
            herfindahl_index = sum(exposure ** 2 for exposure in exposures)
            
            # Convert to risk score (higher concentration = higher risk)
            concentration_risk = min(herfindahl_index * 2, 1.0)
            
            return concentration_risk
            
        except:
            return 0.5
            
    def _check_correlation_warnings(self, portfolio_correlations, cluster_exposures):
        """Check for correlation-based risk warnings"""
        try:
            warnings = []
            
            # Check for high cluster concentration
            for cluster_name, cluster_data in cluster_exposures.items():
                if cluster_data['exposure_pct'] > self.correlation_limits['max_cluster_exposure']:
                    warnings.append({
                        'type': 'cluster_concentration',
                        'cluster': cluster_name,
                        'exposure_pct': cluster_data['exposure_pct'],
                        'limit': self.correlation_limits['max_cluster_exposure'],
                        'severity': 'high'
                    })
                    
            # Check for too many highly correlated pairs
            high_corr_pairs = 0
            symbols = list(portfolio_correlations.index)
            
            for i in range(len(symbols)):
                for j in range(i + 1, len(symbols)):
                    correlation = portfolio_correlations.loc[symbols[i], symbols[j]]
                    
                    if (not np.isnan(correlation) and 
                        abs(correlation) > self.correlation_limits['high_correlation']):
                        high_corr_pairs += 1
                        
            if high_corr_pairs > self.correlation_limits['max_high_corr_pairs']:
                warnings.append({
                    'type': 'high_correlation_pairs',
                    'count': high_corr_pairs,
                    'limit': self.correlation_limits['max_high_corr_pairs'],
                    'severity': 'medium'
                })
                
            # Check diversification score
            diversification_score = self.exposure_analysis['diversification_score']
            if diversification_score < self.correlation_limits['diversification_target']:
                warnings.append({
                    'type': 'poor_diversification',
                    'score': diversification_score,
                    'target': self.correlation_limits['diversification_target'],
                    'severity': 'medium'
                })
                
            # Log warnings
            for warning in warnings:
                self.logger.warning(f"‚ö†Ô∏è Correlation warning: {warning['type']} - {warning}")
                self.correlation_stats['correlation_warnings_issued'] += 1
                
        except Exception as e:
            self.logger.error(f"Correlation warning check failed: {e}")
            
    def detect_correlation_regime_changes(self):
        """Detect changes in correlation regimes"""
        try:
            current_time = time.time()
            
            # Analyze correlation stability across timeframes
            for timeframe in ['short_term', 'medium_term']:
                correlation_matrix = self.correlation_matrices.get(timeframe)
                
                if correlation_matrix is None or correlation_matrix.empty:
                    continue
                    
                # Get historical correlations for comparison
                regime_changes = self._detect_regime_changes_for_timeframe(
                    correlation_matrix, timeframe
                )
                
                if regime_changes:
                    self.correlation_stats['regime_changes_detected'] += len(regime_changes)
                    
                    for change in regime_changes:
                        self.logger.info(
                            f"üìä Correlation regime change detected: {change['pair']} "
                            f"({timeframe}): {change['old_correlation']:.2f} ‚Üí "
                            f"{change['new_correlation']:.2f}"
                        )
                        
                        # Store regime change
                        self.correlation_changes[timeframe].append({
                            'timestamp': current_time,
                            'change_data': change
                        })
                        
        except Exception as e:
            self.logger.error(f"Correlation regime change detection failed: {e}")
            
    def _detect_regime_changes_for_timeframe(self, current_matrix, timeframe):
        """Detect regime changes for specific timeframe"""
        try:
            regime_changes = []
            
            # Compare with historical correlations
            for symbol1 in current_matrix.index:
                for symbol2 in current_matrix.columns:
                    if symbol1 >= symbol2:  # Avoid duplicates
                        continue
                        
                    current_corr = current_matrix.loc[symbol1, symbol2]
                    
                    if np.isnan(current_corr):
                        continue
                        
                    # Get historical correlation
                    pair_key = f"{min(symbol1, symbol2)}_{max(symbol1, symbol2)}"
                    history = self.correlation_history[timeframe].get(pair_key, [])
                    
                    if len(history) < 10:  # Need sufficient history
                        continue
                        
                    # Calculate historical average
                    recent_correlations = [h['correlation'] for h in list(history)[-10:]]
                    historical_avg = np.mean(recent_correlations)
                    
                    # Detect significant change
                    correlation_change = abs(current_corr - historical_avg)
                    
                    if correlation_change > 0.3:  # 30% change threshold
                        regime_changes.append({
                            'pair': f"{symbol1}-{symbol2}",
                            'old_correlation': historical_avg,
                            'new_correlation': current_corr,
                            'change_magnitude': correlation_change,
                            'significance': 'high' if correlation_change > 0.5 else 'medium'
                        })
                        
            return regime_changes
            
        except Exception as e:
            self.logger.error(f"Regime change detection failed for {timeframe}: {e}")
            return []
            
    def get_correlation_recommendations(self):
        """Get recommendations for improving portfolio correlation/diversification"""
        try:
            recommendations = []
            
            exposure_analysis = self.exposure_analysis
            cluster_exposures = exposure_analysis.get('cluster_exposures', {})
            diversification_score = exposure_analysis.get('diversification_score', 1.0)
            
            # Recommend reducing overconcentrated clusters
            for cluster_name, cluster_data in cluster_exposures.items():
                if cluster_data['exposure_pct'] > self.correlation_limits['max_cluster_exposure']:
                    # Find largest position in cluster to reduce
                    cluster_symbols = cluster_data['symbols']
                    
                    largest_position = None
                    largest_value = 0
                    
                    for position in self.position_system.active_positions.values():
                        if (position['symbol'] in cluster_symbols and 
                            position['position_value_usd'] > largest_value):
                            largest_position = position
                            largest_value = position['position_value_usd']
                            
                    if largest_position:
                        reduction_pct = min(
                            0.5,  # Max 50% reduction
                            (cluster_data['exposure_pct'] - self.correlation_limits['max_cluster_exposure']) / cluster_data['exposure_pct']
                        )
                        
                        recommendations.append({
                            'type': 'reduce_cluster_exposure',
                            'action': 'reduce_position',
                            'symbol': largest_position['symbol'],
                            'current_exposure': cluster_data['exposure_pct'],
                            'target_exposure': self.correlation_limits['max_cluster_exposure'],
                            'recommended_reduction_pct': reduction_pct,
                            'cluster_type': cluster_data['type'],
                            'priority': 'high',
                            'reason': f"Overexposed to {cluster_data['type']} cluster"
                        })
                        
            # Recommend diversification improvements
            if diversification_score < self.correlation_limits['diversification_target']:
                # Suggest uncorrelated assets to add
                uncorrelated_suggestions = self._find_uncorrelated_opportunities()
                
                for suggestion in uncorrelated_suggestions[:3]:  # Top 3 suggestions
                    recommendations.append({
                        'type': 'improve_diversification',
                        'action': 'add_position',
                        'symbol': suggestion['symbol'],
                        'current_diversification': diversification_score,
                        'target_diversification': self.correlation_limits['diversification_target'],
                        'correlation_with_portfolio': suggestion['avg_correlation'],
                        'symbol_type': suggestion['type'],
                        'priority': 'medium',
                        'reason': f"Add {suggestion['type']} for better diversification"
                    })
                    
            # Sort recommendations by priority
            priority_order = {'high': 3, 'medium': 2, 'low': 1}
            recommendations.sort(
                key=lambda x: priority_order.get(x['priority'], 1), 
                reverse=True
            )
            
            return recommendations
            
        except Exception as e:
            self.logger.error(f"Correlation recommendations failed: {e}")
            return []
            
    def _find_uncorrelated_opportunities(self):
        """Find symbols with low correlation to current portfolio"""
        try:
            uncorrelated_opportunities = []
            
            # Get current portfolio symbols
            portfolio_symbols = [pos['symbol'] for pos in self.position_system.active_positions.values()]
            
            if not portfolio_symbols:
                return uncorrelated_opportunities
                
            # Get correlation matrix
            correlation_matrix = self.correlation_matrices.get('medium_term')
            
            if correlation_matrix is None or correlation_matrix.empty:
                return uncorrelated_opportunities
                
            # Check all available symbols
            for symbol in correlation_matrix.index:
                if symbol in portfolio_symbols:
                    continue
                    
                # Calculate average correlation with portfolio
                correlations_with_portfolio = []
                
                for portfolio_symbol in portfolio_symbols:
                    if portfolio_symbol in correlation_matrix.columns:
                        corr = correlation_matrix.loc[symbol, portfolio_symbol]
                        
                        if not np.isnan(corr):
                            correlations_with_portfolio.append(abs(corr))
                            
                if correlations_with_portfolio:
                    avg_correlation = np.mean(correlations_with_portfolio)
                    
                    # Only consider symbols with low correlation
                    if avg_correlation < self.correlation_limits['medium_correlation']:
                        symbol_type = self._classify_symbol_type(symbol)
                        
                        uncorrelated_opportunities.append({
                            'symbol': symbol,
                            'avg_correlation': avg_correlation,
                            'type': symbol_type,
                            'diversification_benefit': 1 - avg_correlation
                        })
                        
            # Sort by diversification benefit
            uncorrelated_opportunities.sort(
                key=lambda x: x['diversification_benefit'], 
                reverse=True
            )
            
            return uncorrelated_opportunities
            
        except Exception as e:
            self.logger.error(f"Uncorrelated opportunities search failed: {e}")
            return []
            
    def _classify_symbol_type(self, symbol):
        """Classify symbol type for diversification"""
        for group_name, group_symbols in self.symbol_groups.items():
            if symbol in group_symbols:
                return group_name
        return 'other'
        
    def get_correlation_dashboard_data(self):
        """Get correlation data for dashboard display"""
        try:
            # Current correlation matrices
            correlation_data = {}
            
            for timeframe, matrix in self.correlation_matrices.items():
                if matrix is not None and not matrix.empty:
                    correlation_data[timeframe] = {
                        'matrix': matrix.to_dict(),
                        'timestamp': matrix.attrs.get('timestamp', time.time()),
                        'symbols': matrix.attrs.get('symbols', []),
                        'data_points': matrix.attrs.get('data_points', 0)
                    }
                    
            # Portfolio exposure analysis
            exposure_summary = {
                'diversification_score': self.exposure_analysis.get('diversification_score', 0),
                'concentration_risk': self.exposure_analysis.get('concentration_risk', 0),
                'cluster_count': len(self.exposure_analysis.get('correlation_clusters', [])),
                'last_update': self.exposure_analysis.get('last_update', 0)
            }
            
            # Top correlation pairs (highest absolute correlations)
            top_correlations = self._get_top_correlation_pairs()
            
            # Recent regime changes
            recent_changes = []
            cutoff_time = time.time() - 3600  # Last hour
            
            for timeframe, changes in self.correlation_changes.items():
                recent_changes.extend([
                    {**change['change_data'], 'timeframe': timeframe, 'timestamp': change['timestamp']}
                    for change in changes
                    if change['timestamp'] > cutoff_time
                ])
                
            # Sort by timestamp (most recent first)
            recent_changes.sort(key=lambda x: x['timestamp'], reverse=True)
            
            # Cluster exposure breakdown
            cluster_breakdown = {}
            for cluster_name, cluster_data in self.exposure_analysis.get('cluster_exposures', {}).items():
                cluster_breakdown[cluster_name] = {
                    'symbols': cluster_data['symbols'],
                    'exposure_pct': cluster_data['exposure_pct'],
                    'avg_correlation': cluster_data['avg_correlation'],
                    'type': cluster_data['type'],
                    'risk_level': 'high' if cluster_data['exposure_pct'] > self.correlation_limits['max_cluster_exposure'] else 'normal'
                }
                
            return {
                'correlation_matrices': correlation_data,
                'portfolio_exposure': exposure_summary,
                'top_correlations': top_correlations,
                'recent_regime_changes': recent_changes,
                'cluster_breakdown': cluster_breakdown,
                'correlation_limits': self.correlation_limits,
                'system_stats': self.correlation_stats,
                'recommendations_count': len(self.get_correlation_recommendations())
            }
            
        except Exception as e:
            self.logger.error(f"Correlation dashboard data generation failed: {e}")
            return {}
            
    def _get_top_correlation_pairs(self, limit=10):
        """Get top correlation pairs across all timeframes"""
        try:
            all_correlations = []
            
            # Collect correlations from all timeframes
            for timeframe, matrix in self.correlation_matrices.items():
                if matrix is None or matrix.empty:
                    continue
                    
                for symbol1 in matrix.index:
                    for symbol2 in matrix.columns:
                        if symbol1 >= symbol2:  # Avoid duplicates
                            continue
                            
                        correlation = matrix.loc[symbol1, symbol2]
                        
                        if not np.isnan(correlation):
                            all_correlations.append({
                                'pair': f"{symbol1}-{symbol2}",
                                'correlation': correlation,
                                'abs_correlation': abs(correlation),
                                'timeframe': timeframe,
                                'strength': 'very_high' if abs(correlation) > 0.8 else 'high' if abs(correlation) > 0.6 else 'medium'
                            })
                            
            # Sort by absolute correlation and return top pairs
            all_correlations.sort(key=lambda x: x['abs_correlation'], reverse=True)
            
            return all_correlations[:limit]
            
        except Exception as e:
            self.logger.error(f"Top correlation pairs calculation failed: {e}")
            return []
            
    def calculate_portfolio_correlation_risk(self):
        """Calculate overall correlation risk for the portfolio"""
        try:
            portfolio_symbols = [pos['symbol'] for pos in self.position_system.active_positions.values()]
            
            if len(portfolio_symbols) < 2:
                return {
                    'risk_score': 0.0,
                    'risk_level': 'low',
                    'risk_factors': []
                }
                
            correlation_matrix = self._get_portfolio_correlation_submatrix(portfolio_symbols)
            
            if correlation_matrix is None:
                return {
                    'risk_score': 0.5,
                    'risk_level': 'medium',
                    'risk_factors': ['insufficient_data']
                }
                
            risk_factors = []
            risk_score = 0.0
            
            # Calculate average absolute correlation
            correlations = []
            for symbol1 in correlation_matrix.index:
                for symbol2 in correlation_matrix.columns:
                    if symbol1 != symbol2:
                        corr = correlation_matrix.loc[symbol1, symbol2]
                        if not np.isnan(corr):
                            correlations.append(abs(corr))
                            
            if correlations:
                avg_abs_correlation = np.mean(correlations)
                risk_score += avg_abs_correlation * 0.4  # 40% weight
                
                if avg_abs_correlation > 0.7:
                    risk_factors.append('high_avg_correlation')
                    
            # Check for concentration in clusters
            concentration_risk = self.exposure_analysis.get('concentration_risk', 0)
            risk_score += concentration_risk * 0.3  # 30% weight
            
            if concentration_risk > 0.6:
                risk_factors.append('cluster_concentration')
                
            # Check diversification score
            diversification_score = self.exposure_analysis.get('diversification_score', 1.0)
            risk_score += (1 - diversification_score) * 0.3  # 30% weight
            
            if diversification_score < 0.6:
                risk_factors.append('poor_diversification')
                
            # Determine risk level
            if risk_score > 0.7:
                risk_level = 'very_high'
            elif risk_score > 0.5:
                risk_level = 'high'
            elif risk_score > 0.3:
                risk_level = 'medium'
            else:
                risk_level = 'low'
                
            return {
                'risk_score': min(risk_score, 1.0),
                'risk_level': risk_level,
                'risk_factors': risk_factors,
                'avg_correlation': np.mean(correlations) if correlations else 0,
                'concentration_risk': concentration_risk,
                'diversification_score': diversification_score
            }
            
        except Exception as e:
            self.logger.error(f"Portfolio correlation risk calculation failed: {e}")
            return {
                'risk_score': 0.5,
                'risk_level': 'medium',
                'risk_factors': ['calculation_error']
            }
            
    def optimize_portfolio_correlations(self, target_risk_level='medium'):
        """Generate portfolio optimization recommendations based on correlations"""
        try:
            current_risk = self.calculate_portfolio_correlation_risk()
            
            if current_risk['risk_level'] == target_risk_level:
                return []  # Already at target risk level
                
            optimization_actions = []
            
            # Get current recommendations
            recommendations = self.get_correlation_recommendations()
            
            # Prioritize actions based on target risk level
            if target_risk_level in ['low', 'medium'] and current_risk['risk_level'] in ['high', 'very_high']:
                # Need to reduce correlation risk
                
                # 1. Reduce overconcentrated clusters
                cluster_reductions = [r for r in recommendations if r['type'] == 'reduce_cluster_exposure']
                optimization_actions.extend(cluster_reductions)
                
                # 2. Add diversifying positions
                diversification_adds = [r for r in recommendations if r['type'] == 'improve_diversification']
                optimization_actions.extend(diversification_adds[:2])  # Top 2 suggestions
                
                # 3. Close highly correlated positions
                highly_correlated_pairs = self._find_highly_correlated_position_pairs()
                
                for pair in highly_correlated_pairs[:3]:  # Top 3 pairs
                    # Recommend closing the smaller position
                    pos1 = next((p for p in self.position_system.active_positions.values() if p['symbol'] == pair['symbol1']), None)
                    pos2 = next((p for p in self.position_system.active_positions.values() if p['symbol'] == pair['symbol2']), None)
                    
                    if pos1 and pos2:
                        smaller_position = pos1 if pos1['position_value_usd'] < pos2['position_value_usd'] else pos2
                        
                        optimization_actions.append({
                            'type': 'reduce_correlation',
                            'action': 'close_position',
                            'symbol': smaller_position['symbol'],
                            'correlated_with': pair['symbol2'] if smaller_position['symbol'] == pair['symbol1'] else pair['symbol1'],
                            'correlation': pair['correlation'],
                            'priority': 'high',
                            'reason': f"High correlation ({pair['correlation']:.2f}) with {pair['symbol2'] if smaller_position['symbol'] == pair['symbol1'] else pair['symbol1']}"
                        })
                        
            elif target_risk_level in ['high', 'very_high'] and current_risk['risk_level'] in ['low', 'medium']:
                # Can afford to increase correlation risk for potentially higher returns
                
                # Suggest adding positions in high-performing clusters
                performing_clusters = self._find_performing_clusters()
                
                for cluster in performing_clusters[:2]:
                    optimization_actions.append({
                        'type': 'increase_cluster_exposure',
                        'action': 'add_position',
                        'cluster_type': cluster['type'],
                        'suggested_symbols': cluster['suggested_symbols'],
                        'current_performance': cluster['performance'],
                        'priority': 'medium',
                        'reason': f"Add exposure to performing {cluster['type']} cluster"
                    })
                    
            return optimization_actions
            
        except Exception as e:
            self.logger.error(f"Portfolio correlation optimization failed: {e}")
            return []
            
    def _find_highly_correlated_position_pairs(self):
        """Find pairs of positions with high correlation"""
        try:
            highly_correlated_pairs = []
            portfolio_symbols = [pos['symbol'] for pos in self.position_system.active_positions.values()]
            
            correlation_matrix = self._get_portfolio_correlation_submatrix(portfolio_symbols)
            
            if correlation_matrix is None:
                return highly_correlated_pairs
                
            for i, symbol1 in enumerate(portfolio_symbols):
                for j, symbol2 in enumerate(portfolio_symbols[i+1:], i+1):
                    if symbol1 in correlation_matrix.index and symbol2 in correlation_matrix.columns:
                        correlation = correlation_matrix.loc[symbol1, symbol2]
                        
                        if not np.isnan(correlation) and abs(correlation) > self.correlation_limits['high_correlation']:
                            highly_correlated_pairs.append({
                                'symbol1': symbol1,
                                'symbol2': symbol2,
                                'correlation': correlation,
                                'abs_correlation': abs(correlation)
                            })
                            
            # Sort by correlation strength
            highly_correlated_pairs.sort(key=lambda x: x['abs_correlation'], reverse=True)
            
            return highly_correlated_pairs
            
        except Exception as e:
            self.logger.error(f"Highly correlated pairs search failed: {e}")
            return []
            
    def _find_performing_clusters(self):
        """Find clusters with strong recent performance"""
        try:
            performing_clusters = []
            
            for cluster_name, cluster_data in self.exposure_analysis.get('cluster_exposures', {}).items():
                # Calculate cluster performance
                cluster_performance = self._calculate_cluster_performance(cluster_data['symbols'])
                
                if cluster_performance > 0.05:  # 5%+ performance
                    # Find additional symbols in this cluster type
                    cluster_type = cluster_data['type']
                    group_symbols = self.symbol_groups.get(cluster_type, [])
                    
                    # Suggest symbols not currently in portfolio
                    portfolio_symbols = [pos['symbol'] for pos in self.position_system.active_positions.values()]
                    suggested_symbols = [s for s in group_symbols if s not in portfolio_symbols]
                    
                    if suggested_symbols:
                        performing_clusters.append({
                            'type': cluster_type,
                            'performance': cluster_performance,
                            'current_symbols': cluster_data['symbols'],
                            'suggested_symbols': suggested_symbols[:3]  # Top 3 suggestions
                        })
                        
            # Sort by performance
            performing_clusters.sort(key=lambda x: x['performance'], reverse=True)
            
            return performing_clusters
            
        except Exception as e:
            self.logger.error(f"Performing clusters search failed: {e}")
            return []
            
    def _calculate_cluster_performance(self, cluster_symbols):
        """Calculate recent performance of a cluster"""
        try:
            performances = []
            
            for symbol in cluster_symbols:
                # Get recent price data
                data = self.binance_client.get_klines(symbol, '1h', 24)  # Last 24 hours
                
                if data is not None and len(data) >= 2:
                    performance = (data['close'].iloc[-1] - data['close'].iloc[0]) / data['close'].iloc[0]
                    performances.append(performance)
                    
            return np.mean(performances) if performances else 0
            
        except:
            return 0
            
    def load_correlation_cache(self):
        """Load cached correlation data"""
        try:
            cache_file = os.path.join(self.config.CACHE_DIR, 'correlation_cache.json')
            
            if os.path.exists(cache_file):
                with open(cache_file, 'r') as f:
                    cached_data = json.load(f)
                    
                # Load correlation history
                for timeframe, pairs in cached_data.get('correlation_history', {}).items():
                    for pair, history in pairs.items():
                        self.correlation_history[timeframe][pair] = deque(history, maxlen=1000)
                        
                self.logger.info("üìÇ Correlation cache loaded")
                
        except Exception as e:
            self.logger.debug(f"Correlation cache load failed: {e}")
            
    def save_correlation_cache(self):
        """Save correlation data to cache"""
        try:
            cache_data = {
                'correlation_history': {},
                'timestamp': time.time()
            }
            
            # Convert correlation history to serializable format
            for timeframe, pairs in self.correlation_history.items():
                cache_data['correlation_history'][timeframe] = {}
                
                for pair, history in pairs.items():
                    # Keep only recent history
                    recent_history = list(history)[-100:]  # Last 100 points
                    cache_data['correlation_history'][timeframe][pair] = recent_history
                    
            cache_file = os.path.join(self.config.CACHE_DIR, 'correlation_cache.json')
            
            with open(cache_file, 'w') as f:
                json.dump(cache_data, f, indent=2)
                
            self.logger.debug("üíæ Correlation cache saved")
            
        except Exception as e:
            self.logger.error(f"Correlation cache save failed: {e}")
            
    def get_symbol_correlation(self, symbol1, symbol2, timeframe='medium_term'):
        """Get correlation between two specific symbols"""
        try:
            correlation_matrix = self.correlation_matrices.get(timeframe)
            
            if (correlation_matrix is not None and not correlation_matrix.empty and
                symbol1 in correlation_matrix.index and symbol2 in correlation_matrix.columns):
                
                correlation = correlation_matrix.loc[symbol1, symbol2]
                
                if not np.isnan(correlation):
                    return {
                        'correlation': correlation,
                        'strength': self._classify_correlation_strength(correlation),
                        'timeframe': timeframe,
                        'timestamp': correlation_matrix.attrs.get('timestamp', time.time())
                    }
                    
            # Fallback: calculate directly
            direct_correlation = self._calculate_direct_correlation(symbol1, symbol2, 200)
            
            if direct_correlation is not None:
                return {
                    'correlation': direct_correlation,
                    'strength': self._classify_correlation_strength(direct_correlation),
                    'timeframe': 'calculated',
                    'timestamp': time.time()
                }
                
            return None
            
        except Exception as e:
            self.logger.error(f"Symbol correlation lookup failed: {e}")
            return None
            
    def _classify_correlation_strength(self, correlation):
        """Classify correlation strength"""
        abs_corr = abs(correlation)
        
        if abs_corr > 0.8:
            return 'very_high'
        elif abs_corr > 0.6:
            return 'high'
        elif abs_corr > 0.4:
            return 'medium'
        elif abs_corr > 0.2:
            return 'low'
        else:
            return 'very_low'
            
    def _calculate_direct_correlation(self, symbol1, symbol2, periods):
        """Calculate correlation directly between two symbols"""
        try:
            # Get price data for both symbols
            data1 = self.binance_client.get_klines(symbol1, '15m', periods)
            data2 = self.binance_client.get_klines(symbol2, '15m', periods)
            
            if data1 is None or data2 is None:
                return None
                
            # Calculate returns
            returns1 = data1['close'].pct_change().dropna()
            returns2 = data2['close'].pct_change().dropna()
            
            # Align data
            min_length = min(len(returns1), len(returns2))
            if min_length < 30:
                return None
                
            returns1 = returns1.tail(min_length)
            returns2 = returns2.tail(min_length)
            
            # Calculate correlation
            correlation = returns1.corr(returns2)
            
            return correlation if not np.isnan(correlation) else None
            
        except Exception as e:
            self.logger.error(f"Direct correlation calculation failed: {e}")
            return None
            
    def get_correlation_analytics(self):
        """Get comprehensive correlation analytics"""
        try:
            analytics = {
                'system_performance': {
                    'total_calculations': self.correlation_stats['total_calculations'],
                    'regime_changes_detected': self.correlation_stats['regime_changes_detected'],
                    'warnings_issued': self.correlation_stats['correlation_warnings_issued'],
                    'uptime_hours': (time.time() - getattr(self, 'start_time', time.time())) / 3600
                },
                'portfolio_metrics': {
                    'diversification_score': self.exposure_analysis.get('diversification_score', 0),
                    'concentration_risk': self.exposure_analysis.get('concentration_risk', 0),
                    'cluster_count': len(self.exposure_analysis.get('correlation_clusters', [])),
                    'correlation_risk': self.calculate_portfolio_correlation_risk()
                },
                'market_overview': {
                    'total_symbols_tracked': len(self._get_symbols_for_analysis()),
                    'correlation_matrices_active': len([m for m in self.correlation_matrices.values() if m is not None and not m.empty]),
                    'avg_market_correlation': self._calculate_average_market_correlation(),
                    'regime_stability': self._calculate_regime_stability()
                },
                'recommendations': {
                    'active_recommendations': len(self.get_correlation_recommendations()),
                    'high_priority_actions': len([r for r in self.get_correlation_recommendations() if r.get('priority') == 'high']),
                    'optimization_opportunities': len(self.optimize_portfolio_correlations())
                }
            }
            
            return analytics
            
        except Exception as e:
            self.logger.error(f"Correlation analytics calculation failed: {e}")
            return {}
            
    def _calculate_average_market_correlation(self):
        """Calculate average correlation across all tracked symbols"""
        try:
            correlation_matrix = self.correlation_matrices.get('medium_term')
            
            if correlation_matrix is None or correlation_matrix.empty:
                return 0
                
            correlations = []
            for symbol1 in correlation_matrix.index:
                for symbol2 in correlation_matrix.columns:
                    if symbol1 != symbol2:
                        corr = correlation_matrix.loc[symbol1, symbol2]
                        if not np.isnan(corr):
                            correlations.append(abs(corr))
                            
            return np.mean(correlations) if correlations else 0
            
        except:
            return 0
            
    def _calculate_regime_stability(self):
        """Calculate correlation regime stability"""
        try:
            recent_changes = 0
            total_pairs = 0
            
            # Count recent regime changes
            cutoff_time = time.time() - 3600  # Last hour
            
            for timeframe, changes in self.correlation_changes.items():
                recent_changes += len([c for c in changes if c['timestamp'] > cutoff_time])
                
            # Estimate total tracked pairs
            for matrix in self.correlation_matrices.values():
                if matrix is not None and not matrix.empty:
                    n_symbols = len(matrix.index)
                    total_pairs = max(total_pairs, n_symbols * (n_symbols - 1) // 2)
                    break
                    
            if total_pairs == 0:
                return 1.0
                
            # Stability = 1 - (recent_changes / total_pairs)
            instability = min(recent_changes / total_pairs, 1.0)
            return 1.0 - instability
            
        except:
            return 0.5

def initialize_correlation_matrix_system(config, binance_client, memory_manager, position_system):
    """Initialize complete correlation matrix system"""
    try:
        # Initialize correlation matrix system
        correlation_system = CorrelationMatrixSystem(config, binance_client, memory_manager, position_system)
        correlation_system.start_time = time.time()
        
        logger = logging.getLogger('MonsterBot.CorrelationMatrix')
        logger.info("üìä Correlation matrix system initialized")
        logger.info("   üîÑ Update frequency: 5 minutes")
        logger.info("   üìà Lookback periods: 50/200/500")
        logger.info("   üéØ High correlation threshold: 70%")
        logger.info("   üèóÔ∏è Symbol groups: 7 categories")
        logger.info("   ‚ö° Real-time monitoring: Active")
        logger.info("   üîç Regime change detection: Active")
        logger.info("   üìâ Diversification target: 80%")
        
        return {
            'correlation_system': correlation_system,
            'update_frequency': 300,
            'lookback_periods': [50, 200, 500],
            'symbol_groups': 7,
            'real_time_monitoring': True,
            'regime_detection': True,
            'diversification_target': 0.8
        }
        
    except Exception as e:
        logger = logging.getLogger('MonsterBot.CorrelationMatrix')
        logger.error(f"Correlation matrix system initialization failed: {e}")
        raise        

class ArbitrageScanner:
    """Advanced arbitrage detection system for cross-exchange, funding rate, and triangular arbitrage"""
    
    def __init__(self, config, binance_client, memory_manager):
        self.config = config
        self.binance_client = binance_client
        self.memory_manager = memory_manager
        self.logger = logging.getLogger('MonsterBot.ArbitrageScanner')
        
        # Exchange configurations for cross-exchange arbitrage
        self.exchanges = {
            'binance': {
                'client': binance_client,
                'fees': {'maker': 0.001, 'taker': 0.001},
                'min_profit_threshold': 0.005,  # 0.5% minimum profit
                'api_delay': 0.1,
                'max_position_usd': 100000
            },
            'ftx': {
                'fees': {'maker': 0.0002, 'taker': 0.0007},
                'min_profit_threshold': 0.004,
                'api_delay': 0.15,
                'max_position_usd': 50000
            },
            'kucoin': {
                'fees': {'maker': 0.001, 'taker': 0.001},
                'min_profit_threshold': 0.006,
                'api_delay': 0.2,
                'max_position_usd': 30000
            },
            'okx': {
                'fees': {'maker': 0.0008, 'taker': 0.001},
                'min_profit_threshold': 0.005,
                'api_delay': 0.12,
                'max_position_usd': 75000
            }
        }
        
        # Arbitrage types and settings
        self.arbitrage_types = {
            'cross_exchange': {
                'enabled': True,
                'min_profit_pct': 0.5,  # 0.5% minimum
                'max_execution_time': 30,  # 30 seconds max
                'position_size_pct': 0.8,  # 80% of available capital per opportunity
                'symbols': ['BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'ADAUSDT', 'DOTUSDT']
            },
            'funding_rate': {
                'enabled': True,
                'min_annual_return': 0.2,  # 20% annual minimum
                'position_size_pct': 0.6,
                'hedge_ratio': 0.95,  # 95% hedge ratio
                'rebalance_threshold': 0.05,
                'funding_symbols': ['BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'SOLUSDT', 'AVAXUSDT']
            },
            'triangular': {
                'enabled': True,
                'min_profit_pct': 0.3,  # 0.3% minimum for triangular
                'execution_slippage': 0.1,  # 0.1% slippage allowance
                'max_legs': 3,
                'currency_pairs': [
                    ['BTC', 'ETH', 'USDT'],
                    ['BTC', 'BNB', 'USDT'],
                    ['ETH', 'BNB', 'USDT'],
                    ['BTC', 'ADA', 'USDT'],
                    ['ETH', 'SOL', 'USDT']
                ]
            },
            'statistical': {
                'enabled': True,
                'lookback_periods': 100,
                'z_score_threshold': 2.0,
                'mean_reversion_confidence': 0.8,
                'position_size_pct': 0.4
            }
        }
        
        # Opportunity tracking and execution
        self.active_opportunities = {}
        self.opportunity_history = deque(maxlen=1000)
        self.execution_stats = defaultdict(int)
        self.profit_tracking = defaultdict(float)
        
        # Price feeds and market data
        self.price_feeds = defaultdict(dict)
        self.order_books = defaultdict(dict)
        self.funding_rates = defaultdict(dict)
        self.execution_speeds = defaultdict(deque)
        
        # Risk management for arbitrage
        self.arbitrage_limits = {
            'max_daily_volume': 5000000,  # $5M daily volume limit
            'max_single_trade': 500000,   # $500K single trade
            'max_exposure_per_exchange': 1000000,  # $1M per exchange
            'emergency_exit_threshold': -0.02,  # 2% loss triggers emergency exit
            'funding_rate_limit': 0.01  # 1% funding rate limit
        }
        
        # Performance metrics
        self.performance_metrics = {
            'total_opportunities_found': 0,
            'opportunities_executed': 0,
            'total_profit_usd': 0,
            'success_rate': 0,
            'avg_execution_time': 0,
            'best_opportunity_profit_pct': 0
        }
        
        # Initialize arbitrage scanning
        self.initialize_arbitrage_system()
        
    def initialize_arbitrage_system(self):
        """Initialize arbitrage scanning system"""
        try:
            # Start price feed monitoring
            self.start_price_monitoring()
            
            # Start funding rate monitoring
            self.start_funding_rate_monitoring()
            
            # Start arbitrage detection
            self.start_arbitrage_scanning()
            
            self.logger.info("‚ö° Arbitrage scanner initialized")
            self.logger.info(f"   üîÑ Cross-exchange: {len(self.exchanges)} exchanges")
            self.logger.info(f"   üí∞ Funding rate: {len(self.arbitrage_types['funding_rate']['funding_symbols'])} symbols")
            self.logger.info(f"   üî∫ Triangular: {len(self.arbitrage_types['triangular']['currency_pairs'])} pairs")
            self.logger.info(f"   üìä Min profit thresholds: 0.3-0.6%")
            
        except Exception as e:
            self.logger.error(f"Arbitrage system initialization failed: {e}")
            
    def start_price_monitoring(self):
        """Start real-time price monitoring across exchanges"""
        def price_monitor():
            while True:
                try:
                    self.update_price_feeds()
                    time.sleep(0.5)  # 500ms update frequency for arbitrage
                except Exception as e:
                    self.logger.error(f"Price monitoring error: {e}")
                    time.sleep(2)
                    
        threading.Thread(target=price_monitor, daemon=True).start()
        
    def start_funding_rate_monitoring(self):
        """Start funding rate monitoring"""
        def funding_monitor():
            while True:
                try:
                    self.update_funding_rates()
                    time.sleep(60)  # 1 minute funding rate updates
                except Exception as e:
                    self.logger.error(f"Funding rate monitoring error: {e}")
                    time.sleep(30)
                    
        threading.Thread(target=funding_monitor, daemon=True).start()
        
    def start_arbitrage_scanning(self):
        """Start arbitrage opportunity scanning"""
        def arbitrage_scanner():
            while True:
                try:
                    # Scan for cross-exchange opportunities
                    if self.arbitrage_types['cross_exchange']['enabled']:
                        self.scan_cross_exchange_arbitrage()
                        
                    # Scan for funding rate opportunities
                    if self.arbitrage_types['funding_rate']['enabled']:
                        self.scan_funding_rate_arbitrage()
                        
                    # Scan for triangular arbitrage
                    if self.arbitrage_types['triangular']['enabled']:
                        self.scan_triangular_arbitrage()
                        
                    # Scan for statistical arbitrage
                    if self.arbitrage_types['statistical']['enabled']:
                        self.scan_statistical_arbitrage()
                        
                    time.sleep(1)  # 1 second scan frequency
                except Exception as e:
                    self.logger.error(f"Arbitrage scanning error: {e}")
                    time.sleep(5)
                    
        threading.Thread(target=arbitrage_scanner, daemon=True).start()
        
    def update_price_feeds(self):
        """Update price feeds from all exchanges"""
        try:
            symbols = self.arbitrage_types['cross_exchange']['symbols']
            
            # Update Binance prices
            for symbol in symbols:
                try:
                    ticker = self.binance_client.client.futures_symbol_ticker(symbol=symbol)
                    self.price_feeds['binance'][symbol] = {
                        'price': float(ticker['price']),
                        'timestamp': time.time()
                    }
                except Exception as e:
                    self.logger.debug(f"Failed to get {symbol} price from Binance: {e}")
                    
            # Simulate other exchange prices (in production, would use actual APIs)
            for exchange in ['ftx', 'kucoin', 'okx']:
                for symbol in symbols:
                    if symbol in self.price_feeds['binance']:
                        base_price = self.price_feeds['binance'][symbol]['price']
                        # Add realistic price variation
                        variation = np.random.uniform(-0.002, 0.002)  # ¬±0.2% variation
                        
                        self.price_feeds[exchange][symbol] = {
                            'price': base_price * (1 + variation),
                            'timestamp': time.time()
                        }
                        
        except Exception as e:
            self.logger.error(f"Price feed update failed: {e}")
            
    def update_funding_rates(self):
        """Update funding rates for perpetual contracts"""
        try:
            symbols = self.arbitrage_types['funding_rate']['funding_symbols']
            
            for symbol in symbols:
                try:
                    # Get actual funding rate from Binance
                    funding_info = self.binance_client.client.futures_funding_rate(symbol=symbol, limit=1)
                    
                    if funding_info:
                        funding_rate = float(funding_info[0]['fundingRate'])
                        funding_time = int(funding_info[0]['fundingTime'])
                        
                        self.funding_rates['binance'][symbol] = {
                            'funding_rate': funding_rate,
                            'funding_time': funding_time,
                            'annual_rate': funding_rate * 365 * 3,  # 3 times daily
                            'timestamp': time.time()
                        }
                        
                        # Simulate other exchanges (in production, use actual APIs)
                        for exchange in ['ftx', 'okx']:
                            # Add variation to funding rates
                            rate_variation = np.random.uniform(-0.2, 0.2)
                            varied_rate = funding_rate * (1 + rate_variation)
                            
                            self.funding_rates[exchange][symbol] = {
                                'funding_rate': varied_rate,
                                'funding_time': funding_time,
                                'annual_rate': varied_rate * 365 * 3,
                                'timestamp': time.time()
                            }
                            
                except Exception as e:
                    self.logger.debug(f"Failed to get funding rate for {symbol}: {e}")
                    
        except Exception as e:
            self.logger.error(f"Funding rate update failed: {e}")
            
    def scan_cross_exchange_arbitrage(self):
        """Scan for cross-exchange arbitrage opportunities"""
        try:
            symbols = self.arbitrage_types['cross_exchange']['symbols']
            min_profit = self.arbitrage_types['cross_exchange']['min_profit_pct'] / 100
            
            for symbol in symbols:
                # Get prices from all exchanges
                exchange_prices = {}
                
                for exchange in self.exchanges.keys():
                    if (exchange in self.price_feeds and 
                        symbol in self.price_feeds[exchange]):
                        
                        price_data = self.price_feeds[exchange][symbol]
                        
                        # Check if price data is fresh (within 5 seconds)
                        if time.time() - price_data['timestamp'] < 5:
                            exchange_prices[exchange] = price_data['price']
                            
                if len(exchange_prices) < 2:
                    continue
                    
                # Find best buy and sell prices
                sorted_prices = sorted(exchange_prices.items(), key=lambda x: x[1])
                
                buy_exchange, buy_price = sorted_prices[0]  # Lowest price (buy here)
                sell_exchange, sell_price = sorted_prices[-1]  # Highest price (sell here)
                
                if buy_exchange == sell_exchange:
                    continue
                    
                # Calculate potential profit
                profit_opportunity = self.calculate_cross_exchange_profit(
                    symbol, buy_exchange, sell_exchange, buy_price, sell_price
                )
                
                if profit_opportunity and profit_opportunity['profit_pct'] > min_profit:
                    opportunity_id = f"cross_{symbol}_{buy_exchange}_{sell_exchange}_{int(time.time())}"
                    
                    # Check if we can execute this opportunity
                    if self.can_execute_arbitrage(profit_opportunity):
                        self.active_opportunities[opportunity_id] = profit_opportunity
                        
                        self.logger.info(
                            f"üí∞ Cross-exchange arbitrage found: {symbol} "
                            f"{buy_exchange}‚Üí{sell_exchange} "
                            f"{profit_opportunity['profit_pct']:.2%} profit "
                            f"(${profit_opportunity['profit_usd']:.0f})"
                        )
                        
                        # Execute immediately if profitable enough
                        if profit_opportunity['profit_pct'] > min_profit * 2:  # 2x minimum
                            self.execute_cross_exchange_arbitrage(opportunity_id)
                            
                        self.performance_metrics['total_opportunities_found'] += 1
                        
        except Exception as e:
            self.logger.error(f"Cross-exchange arbitrage scan failed: {e}")
            
    def calculate_cross_exchange_profit(self, symbol, buy_exchange, sell_exchange, buy_price, sell_price):
        """Calculate profit potential for cross-exchange arbitrage"""
        try:
            # Get exchange configurations
            buy_config = self.exchanges[buy_exchange]
            sell_config = self.exchanges[sell_exchange]
            
            # Calculate trading fees
            buy_fee = buy_config['fees']['taker']
            sell_fee = sell_config['fees']['taker']
            
            # Calculate net prices after fees
            net_buy_price = buy_price * (1 + buy_fee)
            net_sell_price = sell_price * (1 - sell_fee)
            
            # Calculate profit percentage
            profit_pct = (net_sell_price - net_buy_price) / net_buy_price
            
            if profit_pct <= 0:
                return None
                
            # Calculate position size
            max_position_buy = buy_config['max_position_usd']
            max_position_sell = sell_config['max_position_usd']
            
            # Use smaller of the two limits
            position_size_usd = min(max_position_buy, max_position_sell)
            
            # Apply position size percentage
            position_size_usd *= self.arbitrage_types['cross_exchange']['position_size_pct']
            
            # Calculate profit in USD
            profit_usd = position_size_usd * profit_pct
            
            # Calculate execution time estimate
            avg_api_delay = (buy_config['api_delay'] + sell_config['api_delay']) / 2
            execution_time_estimate = avg_api_delay * 2 + 1  # 2 API calls + 1 second buffer
            
            return {
                'type': 'cross_exchange',
                'symbol': symbol,
                'buy_exchange': buy_exchange,
                'sell_exchange': sell_exchange,
                'buy_price': buy_price,
                'sell_price': sell_price,
                'net_buy_price': net_buy_price,
                'net_sell_price': net_sell_price,
                'profit_pct': profit_pct,
                'profit_usd': profit_usd,
                'position_size_usd': position_size_usd,
                'execution_time_estimate': execution_time_estimate,
                'timestamp': time.time(),
                'urgency': 'very_high' if profit_pct > 0.01 else 'high'
            }
            
        except Exception as e:
            self.logger.error(f"Cross-exchange profit calculation failed: {e}")
            return None
            
    def scan_funding_rate_arbitrage(self):
        """Scan for funding rate arbitrage opportunities"""
        try:
            symbols = self.arbitrage_types['funding_rate']['funding_symbols']
            min_annual_return = self.arbitrage_types['funding_rate']['min_annual_return']
            
            for symbol in symbols:
                # Analyze funding rates across exchanges
                funding_opportunities = []
                
                for exchange in ['binance', 'ftx', 'okx']:
                    if (exchange in self.funding_rates and 
                        symbol in self.funding_rates[exchange]):
                        
                        funding_data = self.funding_rates[exchange][symbol]
                        annual_rate = funding_data['annual_rate']
                        
                        # Check if rate data is fresh
                        if time.time() - funding_data['timestamp'] < 300:  # 5 minutes
                            funding_opportunities.append({
                                'exchange': exchange,
                                'funding_rate': funding_data['funding_rate'],
                                'annual_rate': annual_rate,
                                'funding_time': funding_data['funding_time']
                            })
                            
                if len(funding_opportunities) < 2:
                    continue
                    
                # Sort by annual rate
                funding_opportunities.sort(key=lambda x: x['annual_rate'])
                
                # Find best funding rate arbitrage
                short_position = funding_opportunities[-1]  # Highest rate (short here, receive funding)
                long_position = funding_opportunities[0]   # Lowest rate (long here, pay less funding)
                
                # Calculate arbitrage profit potential
                rate_difference = short_position['annual_rate'] - long_position['annual_rate']
                
                if abs(rate_difference) > min_annual_return:
                    funding_opportunity = self.calculate_funding_arbitrage_profit(
                        symbol, short_position, long_position, rate_difference
                    )
                    
                    if funding_opportunity:
                        opportunity_id = f"funding_{symbol}_{short_position['exchange']}_{long_position['exchange']}_{int(time.time())}"
                        
                        self.active_opportunities[opportunity_id] = funding_opportunity
                        
                        self.logger.info(
                            f"üí∏ Funding rate arbitrage found: {symbol} "
                            f"Short {short_position['exchange']} ({short_position['annual_rate']:.1%}) "
                            f"Long {long_position['exchange']} ({long_position['annual_rate']:.1%}) "
                            f"Net: {rate_difference:.1%} annual"
                        )
                        
                        # Execute if opportunity is large enough
                        if abs(rate_difference) > min_annual_return * 1.5:
                            self.execute_funding_rate_arbitrage(opportunity_id)
                            
                        self.performance_metrics['total_opportunities_found'] += 1
                        
        except Exception as e:
            self.logger.error(f"Funding rate arbitrage scan failed: {e}")
            
    def calculate_funding_arbitrage_profit(self, symbol, short_position, long_position, rate_difference):
        """Calculate funding rate arbitrage profit potential"""
        try:
            position_size_pct = self.arbitrage_types['funding_rate']['position_size_pct']
            hedge_ratio = self.arbitrage_types['funding_rate']['hedge_ratio']
            
            # Get available capital
            account_balance = self.binance_client.get_account_balance()
            position_size_usd = account_balance * position_size_pct
            
            # Calculate expected annual profit
            annual_profit_usd = position_size_usd * abs(rate_difference)
            daily_profit_usd = annual_profit_usd / 365
            
            # Calculate costs and risks
            hedge_cost = position_size_usd * (1 - hedge_ratio) * 0.1  # 10% annual cost for imperfect hedge
            
            # Net profit
            net_annual_profit = annual_profit_usd - hedge_cost
            net_daily_profit = net_annual_profit / 365
            
            return {
                'type': 'funding_rate',
                'symbol': symbol,
                'short_exchange': short_position['exchange'],
                'long_exchange': long_position['exchange'],
                'short_rate': short_position['funding_rate'],
                'long_rate': long_position['funding_rate'],
                'rate_difference': rate_difference,
                'position_size_usd': position_size_usd,
                'annual_profit_usd': annual_profit_usd,
                'daily_profit_usd': daily_profit_usd,
                'net_annual_profit': net_annual_profit,
                'net_daily_profit': net_daily_profit,
                'hedge_ratio': hedge_ratio,
                'next_funding_time': short_position['funding_time'],
                'timestamp': time.time(),
                'urgency': 'medium'
            }
            
        except Exception as e:
            self.logger.error(f"Funding arbitrage calculation failed: {e}")
            return None
            
    def scan_triangular_arbitrage(self):
        """Scan for triangular arbitrage opportunities"""
        try:
            currency_pairs = self.arbitrage_types['triangular']['currency_pairs']
            min_profit = self.arbitrage_types['triangular']['min_profit_pct'] / 100
            
            for currencies in currency_pairs:
                if len(currencies) != 3:
                    continue
                    
                base, quote, intermediate = currencies
                
                # Get all required trading pairs
                pair1 = f"{base}{intermediate}USDT"
                pair2 = f"{quote}{intermediate}USDT" 
                pair3 = f"{base}{quote}USDT"
                
                # Check if all pairs exist in price feeds
                if not all(pair in self.price_feeds.get('binance', {}) for pair in [pair1, pair2, pair3]):
                    continue
                    
                try:
                    # Get current prices
                    price1 = self.price_feeds['binance'][pair1]['price']  # BTC/ETH
                    price2 = self.price_feeds['binance'][pair2]['price']  # USDT/ETH (need to invert)
                    price3 = self.price_feeds['binance'][pair3]['price']  # BTC/USDT
                    
                    # Calculate triangular arbitrage
                    triangular_opportunity = self.calculate_triangular_profit(
                        currencies, [pair1, pair2, pair3], [price1, price2, price3]
                    )
                    
                    if triangular_opportunity and triangular_opportunity['profit_pct'] > min_profit:
                        opportunity_id = f"triangular_{base}_{quote}_{intermediate}_{int(time.time())}"
                        
                        self.active_opportunities[opportunity_id] = triangular_opportunity
                        
                        self.logger.info(
                            f"üî∫ Triangular arbitrage found: {base}-{quote}-{intermediate} "
                            f"{triangular_opportunity['profit_pct']:.3%} profit "
                            f"Route: {' ‚Üí '.join(triangular_opportunity['execution_path'])}"
                        )
                        
                        # Execute if profit is significant
                        if triangular_opportunity['profit_pct'] > min_profit * 1.5:
                            self.execute_triangular_arbitrage(opportunity_id)
                            
                        self.performance_metrics['total_opportunities_found'] += 1
                        
                except Exception as e:
                    self.logger.debug(f"Triangular calculation failed for {currencies}: {e}")
                    continue
                    
        except Exception as e:
            self.logger.error(f"Triangular arbitrage scan failed: {e}")
            
    def calculate_triangular_profit(self, currencies, pairs, prices):
        """Calculate triangular arbitrage profit"""
        try:
            base, quote, intermediate = currencies
            
            # Calculate both directions
            # Direction 1: base ‚Üí intermediate ‚Üí quote ‚Üí base
            # Direction 2: base ‚Üí quote ‚Üí intermediate ‚Üí base
            
            execution_slippage = self.arbitrage_types['triangular']['execution_slippage'] / 100
            trading_fee = 0.001  # 0.1% per trade
            
            # Direction 1 calculation
            start_amount = 1.0  # Start with 1 unit of base currency
            
            # Step 1: base ‚Üí intermediate
            intermediate_amount = start_amount * prices[0] * (1 - trading_fee - execution_slippage)
            
            # Step 2: intermediate ‚Üí quote  
            quote_amount = intermediate_amount / prices[1] * (1 - trading_fee - execution_slippage)
            
            # Step 3: quote ‚Üí base
            final_base_amount = quote_amount / prices[2] * (1 - trading_fee - execution_slippage)
            
            profit_direction1 = final_base_amount - start_amount
            profit_pct_direction1 = profit_direction1 / start_amount
            
            # Direction 2 calculation
            start_amount = 1.0
            
            # Step 1: base ‚Üí quote
            quote_amount = start_amount * prices[2] * (1 - trading_fee - execution_slippage)
            
            # Step 2: quote ‚Üí intermediate
            intermediate_amount = quote_amount * prices[1] * (1 - trading_fee - execution_slippage)
            
            # Step 3: intermediate ‚Üí base
            final_base_amount = intermediate_amount / prices[0] * (1 - trading_fee - execution_slippage)
            
            profit_direction2 = final_base_amount - start_amount
            profit_pct_direction2 = profit_direction2 / start_amount
            
            # Choose the more profitable direction
            if profit_pct_direction1 > profit_pct_direction2 and profit_pct_direction1 > 0:
                chosen_direction = 1
                profit_pct = profit_pct_direction1
                execution_path = [f"{base}‚Üí{intermediate}", f"{intermediate}‚Üí{quote}", f"{quote}‚Üí{base}"]
            elif profit_pct_direction2 > 0:
                chosen_direction = 2
                profit_pct = profit_pct_direction2
                execution_path = [f"{base}‚Üí{quote}", f"{quote}‚Üí{intermediate}", f"{intermediate}‚Üí{base}"]
            else:
                return None
                
            # Calculate position size
            account_balance = self.binance_client.get_account_balance()
            position_size_usd = account_balance * 0.2  # 20% for triangular arbitrage
            
            profit_usd = position_size_usd * profit_pct
            
            return {
                'type': 'triangular',
                'currencies': currencies,
                'pairs': pairs,
                'prices': prices,
                'direction': chosen_direction,
                'execution_path': execution_path,
                'profit_pct': profit_pct,
                'profit_usd': profit_usd,
                'position_size_usd': position_size_usd,
                'execution_steps': 3,
                'estimated_execution_time': 5,  # 5 seconds for 3 trades
                'timestamp': time.time(),
                'urgency': 'very_high'
            }
            
        except Exception as e:
            self.logger.error(f"Triangular profit calculation failed: {e}")
            return None
            
    def scan_statistical_arbitrage(self):
        """Scan for statistical arbitrage opportunities (mean reversion)"""
        try:
            symbols = self.arbitrage_types['cross_exchange']['symbols']
            lookback = self.arbitrage_types['statistical']['lookback_periods']
            z_threshold = self.arbitrage_types['statistical']['z_score_threshold']
            
            for symbol in symbols:
                # Get price history for statistical analysis
                price_data = self.binance_client.get_klines(symbol, '5m', lookback)
                
                if price_data is None or len(price_data) < lookback:
                    continue
                    
                # Calculate price spread statistics across exchanges
                price_spreads = self.calculate_exchange_spreads(symbol, price_data)
                
                if not price_spreads:
                    continue
                    
                # Analyze statistical arbitrage opportunities
                stat_opportunities = self.analyze_statistical_patterns(symbol, price_spreads)
                
                for opportunity in stat_opportunities:
                    if opportunity['z_score'] > z_threshold:
                        opportunity_id = f"statistical_{symbol}_{opportunity['pattern']}_{int(time.time())}"
                        
                        self.active_opportunities[opportunity_id] = opportunity
                        
                        self.logger.info(
                            f"üìä Statistical arbitrage found: {symbol} "
                            f"{opportunity['pattern']} (Z-score: {opportunity['z_score']:.2f})"
                        )
                        
                        self.performance_metrics['total_opportunities_found'] += 1
                        
        except Exception as e:
	    self.logger.error(f"Statistical arbitrage scan failed: {e}")
            
    def calculate_exchange_spreads(self, symbol, price_data):
        """Calculate price spreads across exchanges for statistical analysis"""
        try:
            spreads = []
            
            for i, candle in enumerate(price_data):
                timestamp = candle[0]
                binance_price = float(candle[4])  # Close price
                
                # Get current exchange prices for comparison
                current_spreads = {}
                
                for exchange in ['ftx', 'kucoin', 'okx']:
                    if exchange in self.price_feeds and symbol in self.price_feeds[exchange]:
                        exchange_price = self.price_feeds[exchange][symbol]['price']
                        spread = (exchange_price - binance_price) / binance_price
                        current_spreads[f'{exchange}_spread'] = spread
                        
                if current_spreads:
                    current_spreads['timestamp'] = timestamp
                    current_spreads['binance_price'] = binance_price
                    spreads.append(current_spreads)
                    
            return spreads[-self.arbitrage_types['statistical']['lookback_periods']:]
            
        except Exception as e:
            self.logger.error(f"Exchange spread calculation failed: {e}")
            return []
            
    def analyze_statistical_patterns(self, symbol, price_spreads):
        """Analyze statistical arbitrage patterns in price spreads"""
        try:
            opportunities = []
            
            if len(price_spreads) < 50:
                return opportunities
                
            # Calculate spread statistics for each exchange
            for exchange in ['ftx', 'kucoin', 'okx']:
                spread_key = f'{exchange}_spread'
                
                if not all(spread_key in data for data in price_spreads):
                    continue
                    
                spreads = [data[spread_key] for data in price_spreads]
                
                # Calculate statistical measures
                mean_spread = np.mean(spreads)
                std_spread = np.std(spreads)
                current_spread = spreads[-1]
                
                if std_spread == 0:
                    continue
                    
                z_score = abs(current_spread - mean_spread) / std_spread
                
                # Check for mean reversion opportunity
                if z_score > self.arbitrage_types['statistical']['z_score_threshold']:
                    confidence = min(z_score / 4.0, 1.0)  # Cap at 100%
                    
                    if confidence > self.arbitrage_types['statistical']['mean_reversion_confidence']:
                        opportunity = self.create_statistical_opportunity(
                            symbol, exchange, current_spread, mean_spread, z_score, confidence
                        )
                        
                        if opportunity:
                            opportunities.append(opportunity)
                            
            return opportunities
            
        except Exception as e:
            self.logger.error(f"Statistical pattern analysis failed: {e}")
            return []
            
    def create_statistical_opportunity(self, symbol, exchange, current_spread, mean_spread, z_score, confidence):
        """Create statistical arbitrage opportunity"""
        try:
            position_size_pct = self.arbitrage_types['statistical']['position_size_pct']
            account_balance = self.binance_client.get_account_balance()
            position_size_usd = account_balance * position_size_pct * confidence
            
            # Determine trade direction
            if current_spread > mean_spread:
                # Spread too high - short exchange, long binance
                direction = 'short_exchange_long_binance'
                expected_profit_pct = (current_spread - mean_spread) * confidence
            else:
                # Spread too low - long exchange, short binance  
                direction = 'long_exchange_short_binance'
                expected_profit_pct = (mean_spread - current_spread) * confidence
                
            profit_usd = position_size_usd * expected_profit_pct
            
            return {
                'type': 'statistical',
                'symbol': symbol,
                'exchange': exchange,
                'pattern': 'mean_reversion',
                'direction': direction,
                'current_spread': current_spread,
                'mean_spread': mean_spread,
                'z_score': z_score,
                'confidence': confidence,
                'expected_profit_pct': expected_profit_pct,
                'profit_usd': profit_usd,
                'position_size_usd': position_size_usd,
                'hold_time_estimate': 30,  # 30 minutes average
                'timestamp': time.time(),
                'urgency': 'medium' if z_score < 3 else 'high'
            }
            
        except Exception as e:
            self.logger.error(f"Statistical opportunity creation failed: {e}")
            return None
            
    def can_execute_arbitrage(self, opportunity):
        """Check if arbitrage opportunity can be executed"""
        try:
            # Check daily volume limits
            if self.execution_stats['daily_volume'] + opportunity['position_size_usd'] > self.arbitrage_limits['max_daily_volume']:
                return False
                
            # Check single trade limits
            if opportunity['position_size_usd'] > self.arbitrage_limits['max_single_trade']:
                return False
                
            # Check if we have enough balance
            required_balance = opportunity['position_size_usd'] * 1.1  # 10% buffer
            if self.binance_client.get_account_balance() < required_balance:
                return False
                
            # Check execution time constraints
            if opportunity.get('execution_time_estimate', 0) > self.arbitrage_types['cross_exchange']['max_execution_time']:
                return False
                
            # Check if opportunity is still fresh
            if time.time() - opportunity['timestamp'] > 10:  # 10 seconds max age
                return False
                
            return True
            
        except Exception as e:
            self.logger.error(f"Arbitrage execution check failed: {e}")
            return False
            
    def execute_cross_exchange_arbitrage(self, opportunity_id):
        """Execute cross-exchange arbitrage opportunity"""
        try:
            opportunity = self.active_opportunities.get(opportunity_id)
            if not opportunity:
                return False
                
            start_time = time.time()
            
            # Calculate position sizes
            position_size_usd = opportunity['position_size_usd']
            symbol = opportunity['symbol']
            
            # Get current price for position sizing
            current_price = self.price_feeds['binance'][symbol]['price']
            position_size_crypto = position_size_usd / current_price
            
            # Execute buy order on cheaper exchange
            buy_exchange = opportunity['buy_exchange']
            sell_exchange = opportunity['sell_exchange']
            
            self.logger.info(f"üöÄ Executing cross-exchange arbitrage: {symbol}")
            self.logger.info(f"   Buy {position_size_crypto:.6f} on {buy_exchange}")
            self.logger.info(f"   Sell {position_size_crypto:.6f} on {sell_exchange}")
            
            # Execute trades (simplified for Binance only in this implementation)
            if buy_exchange == 'binance':
                # Buy on Binance
                buy_result = self.binance_client.place_market_order(
                    symbol=symbol,
                    side='BUY',
                    quantity=position_size_crypto
                )
                
                if buy_result:
                    # Simulate sell on other exchange
                    sell_price = opportunity['sell_price']
                    sell_value = position_size_crypto * sell_price
                    
                    # Calculate actual profit
                    buy_cost = buy_result.get('cummulativeQuoteQty', position_size_usd)
                    actual_profit = sell_value - float(buy_cost)
                    actual_profit_pct = actual_profit / float(buy_cost)
                    
                    # Record execution
                    execution_time = time.time() - start_time
                    
                    self.record_arbitrage_execution(opportunity_id, {
                        'executed': True,
                        'buy_cost': float(buy_cost),
                        'sell_value': sell_value,
                        'actual_profit': actual_profit,
                        'actual_profit_pct': actual_profit_pct,
                        'execution_time': execution_time,
                        'buy_order': buy_result
                    })
                    
                    self.logger.info(f"‚úÖ Cross-exchange arbitrage executed: ${actual_profit:.2f} profit")
                    return True
                    
            return False
            
        except Exception as e:
            self.logger.error(f"Cross-exchange arbitrage execution failed: {e}")
            self.record_arbitrage_execution(opportunity_id, {'executed': False, 'error': str(e)})
            return False
            
    def execute_funding_rate_arbitrage(self, opportunity_id):
        """Execute funding rate arbitrage opportunity"""
        try:
            opportunity = self.active_opportunities.get(opportunity_id)
            if not opportunity:
                return False
                
            symbol = opportunity['symbol']
            position_size_usd = opportunity['position_size_usd']
            short_exchange = opportunity['short_exchange']
            long_exchange = opportunity['long_exchange']
            
            self.logger.info(f"üí∏ Executing funding rate arbitrage: {symbol}")
            self.logger.info(f"   Short on {short_exchange}: ${position_size_usd:.0f}")
            self.logger.info(f"   Long on {long_exchange}: ${position_size_usd:.0f}")
            
            # Execute on Binance (assuming it's one of the exchanges)
            if short_exchange == 'binance':
                # Open short position
                short_result = self.binance_client.place_market_order(
                    symbol=symbol,
                    side='SELL',
                    quantity=position_size_usd / self.price_feeds['binance'][symbol]['price']
                )
                
                if short_result:
                    # Record the funding arbitrage position
                    self.record_funding_position(opportunity_id, {
                        'symbol': symbol,
                        'short_exchange': short_exchange,
                        'long_exchange': long_exchange,
                        'position_size_usd': position_size_usd,
                        'entry_time': time.time(),
                        'expected_daily_profit': opportunity['net_daily_profit'],
                        'short_order': short_result
                    })
                    
                    self.logger.info(f"‚úÖ Funding arbitrage position opened: {symbol}")
                    return True
                    
            return False
            
        except Exception as e:
            self.logger.error(f"Funding rate arbitrage execution failed: {e}")
            return False
            
    def execute_triangular_arbitrage(self, opportunity_id):
        """Execute triangular arbitrage opportunity"""
        try:
            opportunity = self.active_opportunities.get(opportunity_id)
            if not opportunity:
                return False
                
            start_time = time.time()
            execution_path = opportunity['execution_path']
            position_size_usd = opportunity['position_size_usd']
            
            self.logger.info(f"üî∫ Executing triangular arbitrage:")
            self.logger.info(f"   Path: {' ‚Üí '.join(execution_path)}")
            self.logger.info(f"   Position: ${position_size_usd:.0f}")
            
            # Execute the triangular arbitrage sequence
            current_amount = position_size_usd
            
            for i, step in enumerate(execution_path):
                # Parse the step (e.g., "BTC‚ÜíETH")
                from_currency, to_currency = step.split('‚Üí')
                
                # Find the trading pair
                if i == 0:
                    # First step - use USD amount
                    trading_pair = f"{from_currency}USDT"
                    quantity = current_amount / self.price_feeds['binance'][trading_pair]['price']
                else:
                    # Subsequent steps
                    if f"{from_currency}{to_currency}USDT" in self.price_feeds['binance']:
                        trading_pair = f"{from_currency}{to_currency}USDT"
                        side = 'SELL'
                    else:
                        trading_pair = f"{to_currency}{from_currency}USDT"
                        side = 'BUY'
                    quantity = current_amount
                
                # Execute the trade
                result = self.binance_client.place_market_order(
                    symbol=trading_pair,
                    side='BUY' if i == 0 else ('SELL' if from_currency < to_currency else 'BUY'),
                    quantity=quantity
                )
                
                if not result:
                    self.logger.error(f"Triangular arbitrage failed at step {i+1}")
                    return False
                    
                # Update current amount for next step
                if i < len(execution_path) - 1:
                    filled_qty = float(result.get('executedQty', 0))
                    current_amount = filled_qty
                    
            # Calculate final profit
            execution_time = time.time() - start_time
            final_value = float(result.get('cummulativeQuoteQty', 0))
            actual_profit = final_value - position_size_usd
            actual_profit_pct = actual_profit / position_size_usd
            
            self.record_arbitrage_execution(opportunity_id, {
                'executed': True,
                'initial_amount': position_size_usd,
                'final_value': final_value,
                'actual_profit': actual_profit,
                'actual_profit_pct': actual_profit_pct,
                'execution_time': execution_time,
                'steps_completed': len(execution_path)
            })
            
            self.logger.info(f"‚úÖ Triangular arbitrage completed: ${actual_profit:.2f} profit")
            return True
            
        except Exception as e:
            self.logger.error(f"Triangular arbitrage execution failed: {e}")
            self.record_arbitrage_execution(opportunity_id, {'executed': False, 'error': str(e)})
            return False
            
    def record_arbitrage_execution(self, opportunity_id, execution_data):
        """Record arbitrage execution results"""
        try:
            opportunity = self.active_opportunities.get(opportunity_id)
            if not opportunity:
                return
                
            # Update execution statistics
            if execution_data.get('executed'):
                self.execution_stats['successful_executions'] += 1
                self.execution_stats['total_profit'] += execution_data.get('actual_profit', 0)
                
                # Update execution speed tracking
                execution_time = execution_data.get('execution_time', 0)
                exchange_key = opportunity.get('buy_exchange', 'binance')
                self.execution_speeds[exchange_key].append(execution_time)
                
                if len(self.execution_speeds[exchange_key]) > 100:
                    self.execution_speeds[exchange_key].popleft()
                    
            else:
                self.execution_stats['failed_executions'] += 1
                
            self.execution_stats['total_executions'] += 1
            
            # Update daily volume
            position_size = opportunity.get('position_size_usd', 0)
            self.execution_stats['daily_volume'] += position_size
            
            # Store in opportunity history
            execution_record = {
                'opportunity_id': opportunity_id,
                'opportunity': opportunity,
                'execution': execution_data,
                'timestamp': time.time()
            }
            
            self.opportunity_history.append(execution_record)
            
            # Update performance metrics
            self.update_performance_metrics()
            
            # Remove from active opportunities
            if opportunity_id in self.active_opportunities:
                del self.active_opportunities[opportunity_id]
                
        except Exception as e:
            self.logger.error(f"Arbitrage execution recording failed: {e}")
            
    def record_funding_position(self, opportunity_id, position_data):
        """Record funding rate arbitrage position"""
        try:
            # Store funding position for tracking
            position_data['opportunity_id'] = opportunity_id
            position_data['status'] = 'active'
            
            # Add to memory manager for persistence
            self.memory_manager.store_arbitrage_position(position_data)
            
            self.logger.info(f"üìù Funding position recorded: {position_data['symbol']}")
            
        except Exception as e:
            self.logger.error(f"Funding position recording failed: {e}")
            
    def update_performance_metrics(self):
        """Update arbitrage performance metrics"""
        try:
            total_executions = self.execution_stats['total_executions']
            successful_executions = self.execution_stats['successful_executions']
            
            if total_executions > 0:
                self.performance_metrics['success_rate'] = successful_executions / total_executions
                self.performance_metrics['opportunities_executed'] = successful_executions
                self.performance_metrics['total_profit_usd'] = self.execution_stats['total_profit']
                
                # Calculate average execution time
                all_execution_times = []
                for exchange_times in self.execution_speeds.values():
                    all_execution_times.extend(list(exchange_times))
                    
                if all_execution_times:
                    self.performance_metrics['avg_execution_time'] = np.mean(all_execution_times)
                    
                # Find best opportunity profit
                if self.opportunity_history:
                    best_profit = max(
                        record['execution'].get('actual_profit_pct', 0) 
                        for record in self.opportunity_history 
                        if record['execution'].get('executed', False)
                    )
                    self.performance_metrics['best_opportunity_profit_pct'] = best_profit
                    
        except Exception as e:
            self.logger.error(f"Performance metrics update failed: {e}")
            
    def get_arbitrage_status(self):
        """Get current arbitrage scanner status"""
        try:
            return {
                'active_opportunities': len(self.active_opportunities),
                'opportunities_found_today': self.performance_metrics['total_opportunities_found'],
                'success_rate': f"{self.performance_metrics['success_rate']:.1%}",
                'total_profit_usd': self.performance_metrics['total_profit_usd'],
                'avg_execution_time': f"{self.performance_metrics['avg_execution_time']:.2f}s",
                'daily_volume': self.execution_stats['daily_volume'],
                'enabled_strategies': [
                    strategy for strategy, config in self.arbitrage_types.items() 
                    if config.get('enabled', False)
                ],
                'exchange_status': {
                    exchange: 'active' for exchange in self.exchanges.keys()
                }
            }
            
        except Exception as e:
            self.logger.error(f"Arbitrage status retrieval failed: {e}")
            return {}
            
    def cleanup_expired_opportunities(self):
        """Clean up expired arbitrage opportunities"""
        try:
            current_time = time.time()
            expired_ids = []
            
            for opportunity_id, opportunity in self.active_opportunities.items():
                # Remove opportunities older than 30 seconds
                if current_time - opportunity['timestamp'] > 30:
                    expired_ids.append(opportunity_id)
                    
            for opportunity_id in expired_ids:
                del self.active_opportunities[opportunity_id]
                self.logger.debug(f"Removed expired opportunity: {opportunity_id}")
                
        except Exception as e:
            self.logger.error(f"Opportunity cleanup failed: {e}")
            
    def emergency_exit_all_positions(self):
        """Emergency exit all arbitrage positions"""
        try:
            self.logger.warning("üö® EMERGENCY EXIT: Closing all arbitrage positions")
            
            # Close all active opportunities
            self.active_opportunities.clear()
            
            # Get all open positions
            positions = self.binance_client.get_open_positions()
            
            for position in positions:
                symbol = position['symbol']
                size = abs(float(position['positionAmt']))
                
                if size > 0:
                    side = 'SELL' if float(position['positionAmt']) > 0 else 'BUY'
                    
                    # Close position at market
                    self.binance_client.place_market_order(
                        symbol=symbol,
                        side=side,
                        quantity=size,
                        reduce_only=True
                    )
                    
                    self.logger.warning(f"üö® Emergency closed {symbol} position: {size}")
                    
            self.logger.warning("üö® Emergency exit completed")
            
        except Exception as e:
            self.logger.error(f"Emergency exit failed: {e}")

class PerformanceAnalytics:
    """Advanced performance analytics engine for comprehensive trading analysis"""
    
    def __init__(self, config, binance_client, memory_manager):
        self.config = config
        self.binance_client = binance_client
        self.memory_manager = memory_manager
        self.logger = logging.getLogger('MonsterBot.PerformanceAnalytics')
        
        # Analytics configuration based on capital level
        self.capital_tiers = {
            'psycho': {'min': 0, 'max': 1000, 'target_daily': 0.30},
            'hunter': {'min': 1000, 'max': 10000, 'target_daily': 0.25},
            'accumulator': {'min': 10000, 'max': 100000, 'target_daily': 0.20},
            'whale': {'min': 100000, 'max': 1000000, 'target_daily': 0.15},
            'institution': {'min': 1000000, 'max': 10000000, 'target_daily': 0.10},
            'empire': {'min': 10000000, 'max': 100000000, 'target_daily': 0.05},
            'legend': {'min': 100000000, 'max': float('inf'), 'target_daily': 0.01}
        }
        
        # Performance tracking containers
        self.daily_performance = defaultdict(dict)
        self.trade_analytics = defaultdict(list)
        self.strategy_performance = defaultdict(dict)
        self.risk_metrics = defaultdict(dict)
        self.drawdown_analysis = defaultdict(list)
        
        # Real-time metrics
        self.live_metrics = {
            'current_balance': 0,
            'daily_pnl': 0,
            'daily_return_pct': 0,
            'total_trades_today': 0,
            'win_rate_today': 0,
            'current_tier': 'psycho',
            'target_hit': False,
            'hours_to_target': 0,
            'momentum_score': 0,
            'risk_score': 0
        }
        
        # Advanced analytics
        self.sharpe_ratios = defaultdict(deque)
        self.sortino_ratios = defaultdict(deque)
        self.calmar_ratios = defaultdict(deque)
        self.kelly_percentages = defaultdict(deque)
        self.profit_factors = defaultdict(deque)
        
        # Strategy-specific analytics
        self.strategy_metrics = {
            'momentum': {'trades': 0, 'wins': 0, 'total_pnl': 0, 'avg_hold_time': 0},
            'mean_reversion': {'trades': 0, 'wins': 0, 'total_pnl': 0, 'avg_hold_time': 0},
            'breakout': {'trades': 0, 'wins': 0, 'total_pnl': 0, 'avg_hold_time': 0},
            'scalping': {'trades': 0, 'wins': 0, 'total_pnl': 0, 'avg_hold_time': 0},
            'arbitrage': {'trades': 0, 'wins': 0, 'total_pnl': 0, 'avg_hold_time': 0},
            'funding': {'trades': 0, 'wins': 0, 'total_pnl': 0, 'avg_hold_time': 0}
        }
        
        # Market condition analysis
        self.market_conditions = {
            'trend': 'neutral',
            'volatility': 'medium',
            'volume': 'normal',
            'sentiment': 'neutral',
            'funding_rates': 'neutral'
        }
        
        # Performance optimization
        self.optimization_targets = {
            'max_daily_return': 0,
            'min_drawdown': float('inf'),
            'best_sharpe': 0,
            'optimal_leverage': 1,
            'best_strategy_mix': {},
            'peak_performance_conditions': {}
        }
        
        # Initialize analytics system
        self.initialize_performance_system()
        
    def initialize_performance_system(self):
        """Initialize performance analytics system"""
        try:
            # Load historical performance data
            self.load_historical_performance()
            
            # Start real-time monitoring
            self.start_performance_monitoring()
            
            # Start analytics calculations
            self.start_analytics_engine()
            
            # Determine current tier
            self.update_capital_tier()
            
            self.logger.info("üìä Performance Analytics Engine initialized")
            self.logger.info(f"   üí∞ Current tier: {self.live_metrics['current_tier'].upper()}")
            self.logger.info(f"   üéØ Daily target: {self.get_daily_target():.1%}")
            self.logger.info(f"   üìà Tracking {len(self.strategy_metrics)} strategies")
            
        except Exception as e:
            self.logger.error(f"Performance analytics initialization failed: {e}")
            
    def start_performance_monitoring(self):
        """Start real-time performance monitoring"""
        def performance_monitor():
            while True:
                try:
                    self.update_live_metrics()
                    self.analyze_current_performance()
                    self.check_target_progress()
                    time.sleep(30)  # Update every 30 seconds
                except Exception as e:
                    self.logger.error(f"Performance monitoring error: {e}")
                    time.sleep(60)
                    
        threading.Thread(target=performance_monitor, daemon=True).start()
        
    def start_analytics_engine(self):
        """Start analytics calculation engine"""
        def analytics_engine():
            while True:
                try:
                    self.calculate_advanced_metrics()
                    self.analyze_strategy_performance()
                    self.detect_optimization_opportunities()
                    self.update_market_conditions()
                    time.sleep(300)  # Update every 5 minutes
                except Exception as e:
                    self.logger.error(f"Analytics engine error: {e}")
                    time.sleep(300)
                    
        threading.Thread(target=analytics_engine, daemon=True).start()
        
    def load_historical_performance(self):
        """Load historical performance data"""
        try:
            # Load from memory manager
            historical_data = self.memory_manager.get_performance_history()
            
            if historical_data:
                self.daily_performance.update(historical_data.get('daily', {}))
                self.trade_analytics.update(historical_data.get('trades', {}))
                self.strategy_performance.update(historical_data.get('strategies', {}))
                
                self.logger.info(f"üìö Loaded {len(self.daily_performance)} days of historical data")
            else:
                self.logger.info("üÜï Starting fresh performance tracking")
                
        except Exception as e:
            self.logger.error(f"Historical data loading failed: {e}")
            
    def update_live_metrics(self):
        """Update real-time performance metrics"""
        try:
            # Get current account balance
            current_balance = self.binance_client.get_account_balance()
            self.live_metrics['current_balance'] = current_balance
            
            # Calculate daily PnL
            today = datetime.now().strftime('%Y-%m-%d')
            
            if today in self.daily_performance:
                starting_balance = self.daily_performance[today].get('starting_balance', current_balance)
            else:
                starting_balance = current_balance
                self.daily_performance[today]['starting_balance'] = starting_balance
                
            daily_pnl = current_balance - starting_balance
            daily_return_pct = daily_pnl / starting_balance if starting_balance > 0 else 0
            
            self.live_metrics['daily_pnl'] = daily_pnl
            self.live_metrics['daily_return_pct'] = daily_return_pct
            
            # Update daily performance record
            self.daily_performance[today].update({
                'current_balance': current_balance,
                'daily_pnl': daily_pnl,
                'daily_return_pct': daily_return_pct,
                'timestamp': time.time()
            })
            
            # Get trade count for today
            self.update_trade_statistics(today)
            
            # Update capital tier
            self.update_capital_tier()
            
        except Exception as e:
            self.logger.error(f"Live metrics update failed: {e}")
            
    def update_trade_statistics(self, today):
        """Update today's trade statistics"""
        try:
            # Get today's trades from memory
            todays_trades = self.memory_manager.get_trades_by_date(today)
            
            if todays_trades:
                total_trades = len(todays_trades)
                winning_trades = sum(1 for trade in todays_trades if trade.get('pnl', 0) > 0)
                win_rate = winning_trades / total_trades if total_trades > 0 else 0
                
                self.live_metrics['total_trades_today'] = total_trades
                self.live_metrics['win_rate_today'] = win_rate
                
                # Update daily performance
                self.daily_performance[today].update({
                    'total_trades': total_trades,
                    'winning_trades': winning_trades,
                    'win_rate': win_rate
                })
                
                # Analyze individual trades
                for trade in todays_trades:
                    self.analyze_trade_performance(trade)
                    
        except Exception as e:
            self.logger.error(f"Trade statistics update failed: {e}")
            
    def update_capital_tier(self):
        """Update current capital tier based on balance"""
        try:
            current_balance = self.live_metrics['current_balance']
            
            for tier, config in self.capital_tiers.items():
                if config['min'] <= current_balance < config['max']:
                    self.live_metrics['current_tier'] = tier
                    break
                    
        except Exception as e:
            self.logger.error(f"Capital tier update failed: {e}")
            
    def get_daily_target(self):
        """Get daily return target for current tier"""
        try:
            current_tier = self.live_metrics['current_tier']
            return self.capital_tiers[current_tier]['target_daily']
        except:
            return 0.30  # Default to psycho mode
            
    def check_target_progress(self):
        """Check progress toward daily target"""
        try:
            daily_return = self.live_metrics['daily_return_pct']
            target_return = self.get_daily_target()
            
            # Calculate target achievement
            target_hit = daily_return >= target_return
            progress_pct = daily_return / target_return if target_return > 0 else 0
            
            self.live_metrics['target_hit'] = target_hit
            
            # Estimate hours to target based on current rate
            current_hour = datetime.now().hour
            if current_hour > 0 and daily_return > 0:
                hourly_rate = daily_return / current_hour
                
                if hourly_rate > 0:
                    hours_remaining = max(0, (target_return - daily_return) / hourly_rate)
                    self.live_metrics['hours_to_target'] = hours_remaining
                else:
                    self.live_metrics['hours_to_target'] = 24
            else:
                self.live_metrics['hours_to_target'] = 24
                
            # Log progress updates
            if target_hit and not self.daily_performance.get(datetime.now().strftime('%Y-%m-%d'), {}).get('target_logged', False):
                self.logger.info(f"üéØ DAILY TARGET HIT! {daily_return:.1%} return (target: {target_return:.1%})")
                self.daily_performance[datetime.now().strftime('%Y-%m-%d')]['target_logged'] = True
                
        except Exception as e:
            self.logger.error(f"Target progress check failed: {e}")
            
    def analyze_current_performance(self):
        """Analyze current trading performance"""
        try:
            # Calculate momentum score
            momentum_score = self.calculate_momentum_score()
            self.live_metrics['momentum_score'] = momentum_score
            
            # Calculate risk score
            risk_score = self.calculate_risk_score()
            self.live_metrics['risk_score'] = risk_score
            
            # Update performance insights
            self.generate_performance_insights()
            
        except Exception as e:
            self.logger.error(f"Current performance analysis failed: {e}")
            
    def calculate_momentum_score(self):
        """Calculate current trading momentum score (0-100)"""
        try:
            # Factors for momentum calculation
            daily_return = self.live_metrics['daily_return_pct']
            win_rate = self.live_metrics['win_rate_today']
            trade_count = self.live_metrics['total_trades_today']
            target_return = self.get_daily_target()
            
            # Base momentum from returns
            return_momentum = min(100, (daily_return / target_return) * 100) if target_return > 0 else 0
            
            # Win rate momentum (60%+ is good momentum)
            win_rate_momentum = max(0, (win_rate - 0.6) * 250) if win_rate > 0 else 0
            
            # Activity momentum (more trades = more momentum, up to a point)
            activity_momentum = min(30, trade_count * 3)
            
            # Recent trade momentum (last hour performance)
            recent_momentum = self.calculate_recent_momentum()
            
            # Combine factors
            total_momentum = (
                return_momentum * 0.4 +
                win_rate_momentum * 0.3 +
                activity_momentum * 0.2 +
                recent_momentum * 0.1
            )
            
            return min(100, max(0, total_momentum))
            
        except Exception as e:
            self.logger.error(f"Momentum score calculation failed: {e}")
            return 0
            
    def calculate_recent_momentum(self):
        """Calculate momentum from recent trades (last hour)"""
        try:
            one_hour_ago = time.time() - 3600
            recent_trades = self.memory_manager.get_trades_since(one_hour_ago)
            
            if not recent_trades:
                return 0
                
            recent_pnl = sum(trade.get('pnl', 0) for trade in recent_trades)
            recent_wins = sum(1 for trade in recent_trades if trade.get('pnl', 0) > 0)
            recent_win_rate = recent_wins / len(recent_trades)
            
            # Recent performance momentum
            if recent_pnl > 0 and recent_win_rate > 0.6:
                return min(30, recent_pnl * 100)  # Scale recent PnL
            elif recent_pnl > 0:
                return min(20, recent_pnl * 50)
            else:
                return max(-10, recent_pnl * 100)  # Negative momentum for losses
                
        except Exception as e:
            self.logger.error(f"Recent momentum calculation failed: {e}")
            return 0
            
    def calculate_risk_score(self):
        """Calculate current risk score (0-100, higher = more risk)"""
        try:
            # Get current positions
            positions = self.binance_client.get_open_positions()
            total_exposure = sum(abs(float(pos.get('notional', 0))) for pos in positions)
            account_balance = self.live_metrics['current_balance']
            
            # Exposure risk (percentage of account in open positions)
            exposure_risk = min(40, (total_exposure / account_balance) * 40) if account_balance > 0 else 0
            
            # Drawdown risk
            drawdown_risk = self.calculate_drawdown_risk()
            
            # Concentration risk (too many positions in similar assets)
            concentration_risk = self.calculate_concentration_risk(positions)
            
            # Leverage risk
            leverage_risk = self.calculate_leverage_risk(positions)
            
            # Volatility risk (market conditions)
            volatility_risk = self.calculate_volatility_risk()
            
            # Combine risk factors
            total_risk = (
                exposure_risk * 0.25 +
                drawdown_risk * 0.25 +
                concentration_risk * 0.20 +
                leverage_risk * 0.20 +
                volatility_risk * 0.10
            )
            
            return min(100, max(0, total_risk))
            
        except Exception as e:
            self.logger.error(f"Risk score calculation failed: {e}")
            return 50  # Default moderate risk
            
    def calculate_drawdown_risk(self):
        """Calculate risk from current drawdown"""
        try:
            # Get recent high-water mark
            recent_balance_history = self.get_recent_balance_history(24)  # Last 24 hours
            
            if not recent_balance_history:
                return 0
                
            peak_balance = max(recent_balance_history)
            current_balance = self.live_metrics['current_balance']
            
            if peak_balance <= current_balance:
                return 0  # No drawdown
                
            drawdown_pct = (peak_balance - current_balance) / peak_balance
            
            # Scale drawdown to risk score (5% drawdown = 25 risk points)
            return min(50, drawdown_pct * 500)
            
        except Exception as e:
            self.logger.error(f"Drawdown risk calculation failed: {e}")
            return 0
            
    def calculate_concentration_risk(self, positions):
        """Calculate concentration risk from position correlation"""
        try:
            if len(positions) <= 1:
                return 0
                
            # Group positions by base asset
            asset_exposure = defaultdict(float)
            
            for pos in positions:
                symbol = pos.get('symbol', '')
                notional = abs(float(pos.get('notional', 0)))
                
                # Extract base asset (e.g., BTC from BTCUSDT)
                base_asset = symbol.replace('USDT', '').replace('BUSD', '')
                asset_exposure[base_asset] += notional
                
            total_exposure = sum(asset_exposure.values())
            
            if total_exposure == 0:
                return 0
                
            # Calculate concentration (Herfindahl index)
            concentration_index = sum((exposure / total_exposure) ** 2 for exposure in asset_exposure.values())
            
            # Convert to risk score (higher concentration = higher risk)
            return min(30, concentration_index * 100)
            
        except Exception as e:
            self.logger.error(f"Concentration risk calculation failed: {e}")
            return 0
            
    def calculate_leverage_risk(self, positions):
        """Calculate risk from leverage usage"""
        try:
            if not positions:
                return 0
                
            account_balance = self.live_metrics['current_balance']
            total_notional = sum(abs(float(pos.get('notional', 0))) for pos in positions)
            
            if account_balance == 0:
                return 100  # Maximum risk
                
            effective_leverage = total_notional / account_balance
            
            # Risk scales with leverage (10x = 30 risk points, 20x = 50 points)
            return min(60, effective_leverage * 3)
            
        except Exception as e:
            self.logger.error(f"Leverage risk calculation failed: {e}")
            return 0
            
    def calculate_volatility_risk(self):
        """Calculate risk from market volatility"""
        try:
            # Get recent volatility data for major pairs
            volatility_score = 0
            symbols = ['BTCUSDT', 'ETHUSDT', 'BNBUSDT']
            
            for symbol in symbols:
                # Get recent price data
                klines = self.binance_client.get_klines(symbol, '1h', 24)
                
                if klines:
                    # Calculate hourly returns
                    returns = []
                    for i in range(1, len(klines)):
                        prev_close = float(klines[i-1][4])
                        curr_close = float(klines[i][4])
                        hourly_return = (curr_close - prev_close) / prev_close
                        returns.append(abs(hourly_return))
                        
                    if returns:
                        avg_volatility = np.mean(returns)
                        volatility_score += avg_volatility * 100
                        
            # Average volatility across symbols
            volatility_risk = volatility_score / len(symbols) if symbols else 0
            
            return min(25, volatility_risk)
            
        except Exception as e:
            self.logger.error(f"Volatility risk calculation failed: {e}")
            return 10  # Default moderate volatility risk
            
    def get_recent_balance_history(self, hours):
        """Get balance history for recent hours"""
        try:
            # Get from daily performance data
            current_time = time.time()
            cutoff_time = current_time - (hours * 3600)
            
            balance_history = []
            
            # Get today's data
            today = datetime.now().strftime('%Y-%m-%d')
            if today in self.daily_performance:
                today_data = self.daily_performance[today]
                if today_data.get('timestamp', 0) >= cutoff_time:
                    balance_history.append(today_data.get('current_balance', 0))
                    
            # Get yesterday's data if needed
            if hours > 12:
                yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
                if yesterday in self.daily_performance:
                    yesterday_data = self.daily_performance[yesterday]
                    balance_history.append(yesterday_data.get('current_balance', 0))
                    
            return balance_history
            
        except Exception as e:
            self.logger.error(f"Balance history retrieval failed: {e}")
            return []
            
    def analyze_trade_performance(self, trade):
        """Analyze individual trade performance"""
        try:
            strategy = trade.get('strategy', 'unknown')
            pnl = trade.get('pnl', 0)
            hold_time = trade.get('hold_time', 0)
            
            # Update strategy metrics
            if strategy in self.strategy_metrics:
                self.strategy_metrics[strategy]['trades'] += 1
                
                if pnl > 0:
                    self.strategy_metrics[strategy]['wins'] += 1
                    
                self.strategy_metrics[strategy]['total_pnl'] += pnl
                
                # Update average hold time
                current_avg = self.strategy_metrics[strategy]['avg_hold_time']
                total_trades = self.strategy_metrics[strategy]['trades']
                
                new_avg = ((current_avg * (total_trades - 1)) + hold_time) / total_trades
                self.strategy_metrics[strategy]['avg_hold_time'] = new_avg
                
            # Store trade analytics
            trade_analysis = {
                'timestamp': trade.get('timestamp', time.time()),
                'symbol': trade.get('symbol', ''),
                'strategy': strategy,
                'pnl': pnl,
                'pnl_pct': trade.get('pnl_pct', 0),
                'hold_time': hold_time,
                'entry_price': trade.get('entry_price', 0),
                'exit_price': trade.get('exit_price', 0),
                'quantity': trade.get('quantity', 0),
                'leverage': trade.get('leverage', 1),
                'market_conditions': self.market_conditions.copy()
            }
            
            today = datetime.now().strftime('%Y-%m-%d')
            self.trade_analytics[today].append(trade_analysis)
            
        except Exception as e:
            self.logger.error(f"Trade performance analysis failed: {e}")
            
    def calculate_advanced_metrics(self):
        """Calculate advanced performance metrics"""
        try:
            # Get recent performance data
            recent_returns = self.get_recent_returns(30)  # Last 30 days
            
            if len(recent_returns) < 10:
                return
                
            # Calculate Sharpe ratio
            sharpe_ratio = self.calculate_sharpe_ratio(recent_returns)
            self.sharpe_ratios['30_day'].append(sharpe_ratio)
            
            # Calculate Sortino ratio
            sortino_ratio = self.calculate_sortino_ratio(recent_returns)
            self.sortino_ratios['30_day'].append(sortino_ratio)
            
            # Calculate Calmar ratio
            calmar_ratio = self.calculate_calmar_ratio(recent_returns)
            self.calmar_ratios['30_day'].append(calmar_ratio)
            
            # Calculate Kelly percentage
            kelly_pct = self.calculate_kelly_percentage()
            self.kelly_percentages['current'].append(kelly_pct)
            
            # Calculate profit factor
            profit_factor = self.calculate_profit_factor()
            self.profit_factors['current'].append(profit_factor)
            
            # Limit deque sizes
            for metric_deque in [self.sharpe_ratios['30_day'], self.sortino_ratios['30_day'], 
                               self.calmar_ratios['30_day'], self.kelly_percentages['current'],
                               self.profit_factors['current']]:
                while len(metric_deque) > 100:
                    metric_deque.popleft()
                    
        except Exception as e:
            self.logger.error(f"Advanced metrics calculation failed: {e}")
            
    def get_recent_returns(self, days):
        """Get daily returns for recent days"""
        try:
            returns = []
            
            for i in range(days):
                date = (datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d')
                
                if date in self.daily_performance:
                    daily_return = self.daily_performance[date].get('daily_return_pct', 0)
                    returns.append(daily_return)
                    
            return returns[::-1]  # Reverse to chronological order
            
        except Exception as e:
            self.logger.error(f"Recent returns retrieval failed: {e}")
            return []
            
    def calculate_sharpe_ratio(self, returns):
        """Calculate Sharpe ratio"""
        try:
            if len(returns) < 2:
                return 0
                
            mean_return = np.mean(returns)
            std_return = np.std(returns)
            
            if std_return == 0:
                return 0
                
            # Assume risk-free rate of 0.02/365 (2% annual)
            risk_free_rate = 0.02 / 365
            
            sharpe = (mean_return - risk_free_rate) / std_return
            
            # Annualize
            return sharpe * np.sqrt(365)
            
        except Exception as e:
            self.logger.error(f"Sharpe ratio calculation failed: {e}")
            return 0
            
    def calculate_sortino_ratio(self, returns):
        """Calculate Sortino ratio (downside deviation)"""
        try:
            if len(returns) < 2:
                return 0
                
            mean_return = np.mean(returns)
            
            # Calculate downside deviation
            downside_returns = [r for r in returns if r < 0]
            
            if not downside_returns:
                return float('inf')  # No downside
                
            downside_std = np.std(downside_returns)
            
            if downside_std == 0:
                return 0
                
            risk_free_rate = 0.02 / 365
            sortino = (mean_return - risk_free_rate) / downside_std
            
            return sortino * np.sqrt(365)
            
        except Exception as e:
            self.logger.error(f"Sortino ratio calculation failed: {e}")
            return 0
            
    def calculate_calmar_ratio(self, returns):
        """Calculate Calmar ratio (return/max drawdown)"""
        try:
            if len(returns) < 2:
                return 0
                
            annual_return = np.mean(returns) * 365
            max_drawdown = self.calculate_max_drawdown(returns)
            
            if max_drawdown == 0:
                return float('inf')
                
            return annual_return / abs(max_drawdown)
            
        except Exception as e:
            self.logger.error(f"Calmar ratio calculation failed: {e}")
            return 0
            
    def calculate_max_drawdown(self, returns):
        """Calculate maximum drawdown from returns"""
        try:
            cumulative = np.cumprod([1 + r for r in returns])
            running_max = np.maximum.accumulate(cumulative)
            drawdown = (cumulative - running_max) / running_max
            
            return np.min(drawdown)
            
        except Exception as e:
            self.logger.error(f"Max drawdown calculation failed: {e}")
            return 0
            
    def calculate_kelly_percentage(self):
        """Calculate Kelly percentage for optimal position sizing"""
        try:
            # Get recent trade data
            recent_trades = self.get_recent_trades(100)
            
            if len(recent_trades) < 20:
                return 0
                
            wins = [t['pnl_pct'] for t in recent_trades if t['pnl_pct'] > 0]
            losses = [abs(t['pnl_pct']) for t in recent_trades if t['pnl_pct'] < 0]
            
            if not wins or not losses:
                return 0
                
            win_rate = len(wins) / len(recent_trades)
            avg_win = np.mean(wins)
            avg_loss = np.mean(losses)
            
            if avg_loss == 0:
                return 0
                
            # Kelly formula: f = (bp - q) / b
            # where b = avg_win/avg_loss, p = win_rate, q = loss_rate
            b = avg_win / avg_loss
            p = win_rate
            q = 1 - win_rate
            
            kelly = (b * p - q) / b
            
            return max(0, min(1, kelly))  # Cap between 0 and 1
            
        except Exception as e:
            self.logger.error(f"Kelly percentage calculation failed: {e}")
            return 0
            
    def calculate_profit_factor(self):
        """Calculate profit factor (gross profit / gross loss)"""
        try:
            recent_trades = self.get_recent_trades(100)
            
            if not recent_trades:
                return 1
                
            gross_profit = sum(t['pnl'] for t in recent_trades if t['pnl'] > 0)
            gross_loss = abs(sum(t['pnl'] for t in recent_trades if t['pnl'] < 0))

	    if gross_loss == 0:
                return float('inf') if gross_profit > 0 else 1
                
            return gross_profit / gross_loss
            
        except Exception as e:
            self.logger.error(f"Profit factor calculation failed: {e}")
            return 1
            
    def get_recent_trades(self, count):
        """Get recent trades for analysis"""
        try:
            all_trades = []
            
            # Get trades from recent days
            for i in range(7):  # Last 7 days
                date = (datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d')
                
                if date in self.trade_analytics:
                    all_trades.extend(self.trade_analytics[date])
                    
            # Sort by timestamp and return most recent
            all_trades.sort(key=lambda x: x['timestamp'], reverse=True)
            
            return all_trades[:count]
            
        except Exception as e:
            self.logger.error(f"Recent trades retrieval failed: {e}")
            return []
            
    def analyze_strategy_performance(self):
        """Analyze performance of individual strategies"""
        try:
            for strategy, metrics in self.strategy_metrics.items():
                if metrics['trades'] == 0:
                    continue
                    
                # Calculate win rate
                win_rate = metrics['wins'] / metrics['trades']
                
                # Calculate average PnL per trade
                avg_pnl = metrics['total_pnl'] / metrics['trades']
                
                # Calculate profit factor for this strategy
                strategy_trades = [t for t in self.get_recent_trades(200) if t['strategy'] == strategy]
                
                if strategy_trades:
                    strategy_profit_factor = self.calculate_strategy_profit_factor(strategy_trades)
                    
                    # Update strategy performance record
                    self.strategy_performance[strategy] = {
                        'total_trades': metrics['trades'],
                        'win_rate': win_rate,
                        'avg_pnl': avg_pnl,
                        'total_pnl': metrics['total_pnl'],
                        'avg_hold_time': metrics['avg_hold_time'],
                        'profit_factor': strategy_profit_factor,
                        'last_updated': time.time()
                    }
                    
        except Exception as e:
            self.logger.error(f"Strategy performance analysis failed: {e}")
            
    def calculate_strategy_profit_factor(self, strategy_trades):
        """Calculate profit factor for specific strategy"""
        try:
            wins = [t['pnl'] for t in strategy_trades if t['pnl'] > 0]
            losses = [abs(t['pnl']) for t in strategy_trades if t['pnl'] < 0]
            
            gross_profit = sum(wins) if wins else 0
            gross_loss = sum(losses) if losses else 0
            
            if gross_loss == 0:
                return float('inf') if gross_profit > 0 else 1
                
            return gross_profit / gross_loss
            
        except Exception as e:
            self.logger.error(f"Strategy profit factor calculation failed: {e}")
            return 1
            
    def detect_optimization_opportunities(self):
        """Detect opportunities for performance optimization"""
        try:
            # Analyze best performing conditions
            best_conditions = self.find_best_performance_conditions()
            
            # Analyze worst performing periods
            worst_conditions = self.find_worst_performance_conditions()
            
            # Suggest optimizations
            optimizations = self.generate_optimization_suggestions(best_conditions, worst_conditions)
            
            # Update optimization targets
            self.update_optimization_targets(optimizations)
            
        except Exception as e:
            self.logger.error(f"Optimization opportunity detection failed: {e}")
            
    def find_best_performance_conditions(self):
        """Find market conditions during best performance"""
        try:
            recent_trades = self.get_recent_trades(100)
            
            # Sort trades by PnL percentage
            sorted_trades = sorted(recent_trades, key=lambda x: x.get('pnl_pct', 0), reverse=True)
            
            # Take top 20% of trades
            top_trades = sorted_trades[:max(1, len(sorted_trades) // 5)]
            
            # Analyze conditions during best trades
            best_conditions = {
                'strategies': defaultdict(int),
                'symbols': defaultdict(int),
                'hold_times': [],
                'leverages': [],
                'market_conditions': defaultdict(int)
            }
            
            for trade in top_trades:
                best_conditions['strategies'][trade.get('strategy', 'unknown')] += 1
                best_conditions['symbols'][trade.get('symbol', 'unknown')] += 1
                best_conditions['hold_times'].append(trade.get('hold_time', 0))
                best_conditions['leverages'].append(trade.get('leverage', 1))
                
                # Market conditions during trade
                conditions = trade.get('market_conditions', {})
                for condition, value in conditions.items():
                    best_conditions['market_conditions'][f"{condition}_{value}"] += 1
                    
            return best_conditions
            
        except Exception as e:
            self.logger.error(f"Best conditions analysis failed: {e}")
            return {}
            
    def find_worst_performance_conditions(self):
        """Find market conditions during worst performance"""
        try:
            recent_trades = self.get_recent_trades(100)
            
            # Sort trades by PnL percentage (worst first)
            sorted_trades = sorted(recent_trades, key=lambda x: x.get('pnl_pct', 0))
            
            # Take bottom 20% of trades
            worst_trades = sorted_trades[:max(1, len(sorted_trades) // 5)]
            
            # Analyze conditions during worst trades
            worst_conditions = {
                'strategies': defaultdict(int),
                'symbols': defaultdict(int),
                'hold_times': [],
                'leverages': [],
                'market_conditions': defaultdict(int)
            }
            
            for trade in worst_trades:
                worst_conditions['strategies'][trade.get('strategy', 'unknown')] += 1
                worst_conditions['symbols'][trade.get('symbol', 'unknown')] += 1
                worst_conditions['hold_times'].append(trade.get('hold_time', 0))
                worst_conditions['leverages'].append(trade.get('leverage', 1))
                
                # Market conditions during trade
                conditions = trade.get('market_conditions', {})
                for condition, value in conditions.items():
                    worst_conditions['market_conditions'][f"{condition}_{value}"] += 1
                    
            return worst_conditions
            
        except Exception as e:
            self.logger.error(f"Worst conditions analysis failed: {e}")
            return {}
            
    def generate_optimization_suggestions(self, best_conditions, worst_conditions):
        """Generate optimization suggestions based on performance analysis"""
        try:
            suggestions = {
                'strategy_adjustments': [],
                'symbol_preferences': [],
                'timing_optimizations': [],
                'leverage_recommendations': [],
                'market_condition_filters': []
            }
            
            # Strategy optimization
            if best_conditions.get('strategies'):
                best_strategy = max(best_conditions['strategies'].items(), key=lambda x: x[1])
                suggestions['strategy_adjustments'].append({
                    'type': 'increase_allocation',
                    'strategy': best_strategy[0],
                    'reason': 'Top performing strategy in recent trades',
                    'impact': 'high'
                })
                
            if worst_conditions.get('strategies'):
                worst_strategy = max(worst_conditions['strategies'].items(), key=lambda x: x[1])
                suggestions['strategy_adjustments'].append({
                    'type': 'reduce_allocation',
                    'strategy': worst_strategy[0],
                    'reason': 'Underperforming strategy causing losses',
                    'impact': 'high'
                })
                
            # Symbol optimization
            if best_conditions.get('symbols'):
                best_symbols = sorted(best_conditions['symbols'].items(), key=lambda x: x[1], reverse=True)[:3]
                for symbol, count in best_symbols:
                    suggestions['symbol_preferences'].append({
                        'type': 'prefer_symbol',
                        'symbol': symbol,
                        'reason': f'High success rate ({count} profitable trades)',
                        'impact': 'medium'
                    })
                    
            # Hold time optimization
            if best_conditions.get('hold_times'):
                optimal_hold_time = np.median(best_conditions['hold_times'])
                suggestions['timing_optimizations'].append({
                    'type': 'optimize_hold_time',
                    'optimal_time': optimal_hold_time,
                    'reason': 'Median hold time of best performing trades',
                    'impact': 'medium'
                })
                
            # Leverage optimization
            if best_conditions.get('leverages'):
                optimal_leverage = np.median(best_conditions['leverages'])
                current_tier = self.live_metrics['current_tier']
                
                suggestions['leverage_recommendations'].append({
                    'type': 'optimize_leverage',
                    'optimal_leverage': optimal_leverage,
                    'current_tier': current_tier,
                    'reason': 'Leverage used in most profitable trades',
                    'impact': 'high'
                })
                
            return suggestions
            
        except Exception as e:
            self.logger.error(f"Optimization suggestions generation failed: {e}")
            return {}
            
    def update_optimization_targets(self, optimizations):
        """Update optimization targets based on analysis"""
        try:
            current_daily_return = self.live_metrics['daily_return_pct']
            
            # Update max daily return if we hit a new high
            if current_daily_return > self.optimization_targets['max_daily_return']:
                self.optimization_targets['max_daily_return'] = current_daily_return
                
                # Store the conditions that led to this performance
                self.optimization_targets['peak_performance_conditions'] = {
                    'daily_return': current_daily_return,
                    'timestamp': time.time(),
                    'market_conditions': self.market_conditions.copy(),
                    'active_strategies': self.get_active_strategies(),
                    'risk_score': self.live_metrics['risk_score'],
                    'momentum_score': self.live_metrics['momentum_score']
                }
                
            # Update best Sharpe ratio
            if self.sharpe_ratios.get('30_day'):
                current_sharpe = self.sharpe_ratios['30_day'][-1]
                if current_sharpe > self.optimization_targets['best_sharpe']:
                    self.optimization_targets['best_sharpe'] = current_sharpe
                    
            # Apply optimization suggestions
            self.apply_optimization_suggestions(optimizations)
            
        except Exception as e:
            self.logger.error(f"Optimization targets update failed: {e}")
            
    def get_active_strategies(self):
        """Get currently active trading strategies"""
        try:
            active_strategies = []
            
            for strategy, metrics in self.strategy_metrics.items():
                # Consider strategy active if it made trades in last hour
                recent_trades = [t for t in self.get_recent_trades(50) 
                               if t['strategy'] == strategy and 
                               time.time() - t['timestamp'] < 3600]
                
                if recent_trades:
                    active_strategies.append({
                        'strategy': strategy,
                        'recent_trades': len(recent_trades),
                        'recent_pnl': sum(t['pnl'] for t in recent_trades)
                    })
                    
            return active_strategies
            
        except Exception as e:
            self.logger.error(f"Active strategies retrieval failed: {e}")
            return []
            
    def apply_optimization_suggestions(self, optimizations):
        """Apply optimization suggestions to improve performance"""
        try:
            applied_count = 0
            
            # Strategy allocation adjustments
            for suggestion in optimizations.get('strategy_adjustments', []):
                if suggestion['impact'] == 'high':
                    self.adjust_strategy_allocation(suggestion)
                    applied_count += 1
                    
            # Leverage optimizations
            for suggestion in optimizations.get('leverage_recommendations', []):
                if suggestion['impact'] == 'high':
                    self.update_optimal_leverage(suggestion)
                    applied_count += 1
                    
            if applied_count > 0:
                self.logger.info(f"üîß Applied {applied_count} optimization suggestions")
                
        except Exception as e:
            self.logger.error(f"Optimization application failed: {e}")
            
    def adjust_strategy_allocation(self, suggestion):
        """Adjust strategy allocation based on suggestion"""
        try:
            strategy = suggestion['strategy']
            adjustment_type = suggestion['type']
            
            # This would integrate with the strategy manager
            # For now, just log the recommendation
            if adjustment_type == 'increase_allocation':
                self.logger.info(f"üìà RECOMMENDATION: Increase allocation to {strategy} strategy")
            elif adjustment_type == 'reduce_allocation':
                self.logger.info(f"üìâ RECOMMENDATION: Reduce allocation to {strategy} strategy")
                
        except Exception as e:
            self.logger.error(f"Strategy allocation adjustment failed: {e}")
            
    def update_optimal_leverage(self, suggestion):
        """Update optimal leverage recommendation"""
        try:
            optimal_leverage = suggestion['optimal_leverage']
            current_tier = suggestion['current_tier']
            
            self.optimization_targets['optimal_leverage'] = optimal_leverage
            
            self.logger.info(f"‚ö° OPTIMAL LEVERAGE UPDATE: {optimal_leverage:.1f}x for {current_tier} tier")
            
        except Exception as e:
            self.logger.error(f"Optimal leverage update failed: {e}")
            
    def update_market_conditions(self):
        """Update current market condition analysis"""
        try:
            # Get market data for analysis
            btc_data = self.binance_client.get_klines('BTCUSDT', '1h', 24)
            eth_data = self.binance_client.get_klines('ETHUSDT', '1h', 24)
            
            if btc_data and eth_data:
                # Analyze trend
                btc_trend = self.analyze_trend(btc_data)
                eth_trend = self.analyze_trend(eth_data)
                
                # Overall market trend
                if btc_trend == 'bullish' and eth_trend == 'bullish':
                    self.market_conditions['trend'] = 'bullish'
                elif btc_trend == 'bearish' and eth_trend == 'bearish':
                    self.market_conditions['trend'] = 'bearish'
                else:
                    self.market_conditions['trend'] = 'mixed'
                    
                # Analyze volatility
                btc_volatility = self.analyze_volatility(btc_data)
                self.market_conditions['volatility'] = btc_volatility
                
                # Analyze volume
                btc_volume = self.analyze_volume(btc_data)
                self.market_conditions['volume'] = btc_volume
                
            # Update funding rates condition
            self.update_funding_conditions()
            
        except Exception as e:
            self.logger.error(f"Market conditions update failed: {e}")
            
    def analyze_trend(self, klines):
        """Analyze price trend from kline data"""
        try:
            closes = [float(k[4]) for k in klines]
            
            if len(closes) < 10:
                return 'neutral'
                
            # Simple trend analysis using moving averages
            ma_short = np.mean(closes[-5:])  # 5-period MA
            ma_long = np.mean(closes[-20:])  # 20-period MA
            
            if ma_short > ma_long * 1.02:  # 2% above
                return 'bullish'
            elif ma_short < ma_long * 0.98:  # 2% below
                return 'bearish'
            else:
                return 'neutral'
                
        except Exception as e:
            self.logger.error(f"Trend analysis failed: {e}")
            return 'neutral'
            
    def analyze_volatility(self, klines):
        """Analyze market volatility"""
        try:
            closes = [float(k[4]) for k in klines]
            
            if len(closes) < 2:
                return 'normal'
                
            # Calculate hourly returns
            returns = []
            for i in range(1, len(closes)):
                ret = abs((closes[i] - closes[i-1]) / closes[i-1])
                returns.append(ret)
                
            avg_volatility = np.mean(returns)
            
            if avg_volatility > 0.03:  # 3% average hourly moves
                return 'high'
            elif avg_volatility > 0.015:  # 1.5% average
                return 'medium'
            else:
                return 'low'
                
        except Exception as e:
            self.logger.error(f"Volatility analysis failed: {e}")
            return 'normal'
            
    def analyze_volume(self, klines):
        """Analyze trading volume"""
        try:
            volumes = [float(k[5]) for k in klines]
            
            if len(volumes) < 10:
                return 'normal'
                
            recent_avg = np.mean(volumes[-5:])
            historical_avg = np.mean(volumes[:-5])
            
            volume_ratio = recent_avg / historical_avg
            
            if volume_ratio > 1.5:
                return 'high'
            elif volume_ratio > 1.2:
                return 'above_normal'
            elif volume_ratio < 0.8:
                return 'low'
            else:
                return 'normal'
                
        except Exception as e:
            self.logger.error(f"Volume analysis failed: {e}")
            return 'normal'
            
    def update_funding_conditions(self):
        """Update funding rate conditions"""
        try:
            # Get current funding rates
            funding_data = self.binance_client.client.futures_funding_rate(symbol='BTCUSDT', limit=1)
            
            if funding_data:
                current_funding = float(funding_data[0]['fundingRate'])
                
                if current_funding > 0.001:  # 0.1% funding rate
                    self.market_conditions['funding_rates'] = 'high_long_cost'
                elif current_funding < -0.001:
                    self.market_conditions['funding_rates'] = 'high_short_cost'
                else:
                    self.market_conditions['funding_rates'] = 'neutral'
                    
        except Exception as e:
            self.logger.error(f"Funding conditions update failed: {e}")
            
    def generate_performance_insights(self):
        """Generate actionable performance insights"""
        try:
            insights = []
            
            # Daily target insights
            daily_return = self.live_metrics['daily_return_pct']
            target_return = self.get_daily_target()
            
            if daily_return >= target_return:
                insights.append({
                    'type': 'success',
                    'message': f"üéØ Daily target achieved! {daily_return:.1%} vs {target_return:.1%} target",
                    'priority': 'high'
                })
            elif daily_return >= target_return * 0.8:
                insights.append({
                    'type': 'warning',
                    'message': f"‚ö†Ô∏è Close to daily target: {daily_return:.1%}/{target_return:.1%}",
                    'priority': 'medium'
                })
            else:
                insights.append({
                    'type': 'action',
                    'message': f"üöÄ Need {(target_return - daily_return):.1%} more to hit daily target",
                    'priority': 'high'
                })
                
            # Risk insights
            risk_score = self.live_metrics['risk_score']
            
            if risk_score > 70:
                insights.append({
                    'type': 'warning',
                    'message': f"‚ö†Ô∏è High risk score: {risk_score:.0f}/100 - Consider reducing positions",
                    'priority': 'high'
                })
            elif risk_score < 30:
                insights.append({
                    'type': 'opportunity',
                    'message': f"üíö Low risk environment: {risk_score:.0f}/100 - Good time for aggressive plays",
                    'priority': 'medium'
                })
                
            # Momentum insights
            momentum_score = self.live_metrics['momentum_score']
            
            if momentum_score > 80:
                insights.append({
                    'type': 'success',
                    'message': f"üî• Strong momentum: {momentum_score:.0f}/100 - Ride the wave",
                    'priority': 'medium'
                })
            elif momentum_score < 30:
                insights.append({
                    'type': 'warning',
                    'message': f"üêå Low momentum: {momentum_score:.0f}/100 - Wait for better setups",
                    'priority': 'medium'
                })
                
            # Strategy insights
            best_strategy = self.get_best_performing_strategy_today()
            if best_strategy:
                insights.append({
                    'type': 'info',
                    'message': f"‚≠ê Best strategy today: {best_strategy['name']} ({best_strategy['performance']:.1%})",
                    'priority': 'low'
                })
                
            # Log high-priority insights
            for insight in insights:
                if insight['priority'] == 'high':
                    self.logger.info(insight['message'])
                    
        except Exception as e:
            self.logger.error(f"Performance insights generation failed: {e}")
            
    def get_best_performing_strategy_today(self):
        """Get today's best performing strategy"""
        try:
            today_trades = self.get_recent_trades(100)
            today_date = datetime.now().strftime('%Y-%m-%d')
            
            # Filter today's trades
            today_trades = [t for t in today_trades if 
                          datetime.fromtimestamp(t['timestamp']).strftime('%Y-%m-%d') == today_date]
            
            if not today_trades:
                return None
                
            # Group by strategy
            strategy_performance = defaultdict(list)
            
            for trade in today_trades:
                strategy = trade.get('strategy', 'unknown')
                strategy_performance[strategy].append(trade['pnl_pct'])
                
            # Find best strategy
            best_strategy = None
            best_performance = -float('inf')
            
            for strategy, pnls in strategy_performance.items():
                avg_performance = np.mean(pnls)
                
                if avg_performance > best_performance:
                    best_performance = avg_performance
                    best_strategy = {
                        'name': strategy,
                        'performance': avg_performance,
                        'trade_count': len(pnls)
                    }
                    
            return best_strategy
            
        except Exception as e:
            self.logger.error(f"Best strategy analysis failed: {e}")
            return None
            
    def get_performance_summary(self):
        """Get comprehensive performance summary"""
        try:
            return {
                'live_metrics': self.live_metrics,
                'capital_tier': self.live_metrics['current_tier'],
                'daily_target': f"{self.get_daily_target():.1%}",
                'target_progress': f"{(self.live_metrics['daily_return_pct'] / self.get_daily_target()) * 100:.1f}%" if self.get_daily_target() > 0 else "0%",
                'strategy_performance': dict(self.strategy_performance),
                'market_conditions': self.market_conditions,
                'recent_sharpe': self.sharpe_ratios['30_day'][-1] if self.sharpe_ratios.get('30_day') else 0,
                'recent_sortino': self.sortino_ratios['30_day'][-1] if self.sortino_ratios.get('30_day') else 0,
                'kelly_percentage': self.kelly_percentages['current'][-1] if self.kelly_percentages.get('current') else 0,
                'profit_factor': self.profit_factors['current'][-1] if self.profit_factors.get('current') else 1,
                'optimization_targets': self.optimization_targets
            }
            
        except Exception as e:
            self.logger.error(f"Performance summary generation failed: {e}")
            return {}
            
    def reset_daily_metrics(self):
        """Reset daily metrics at start of new day"""
        try:
            self.live_metrics.update({
                'daily_pnl': 0,
                'daily_return_pct': 0,
                'total_trades_today': 0,
                'win_rate_today': 0,
                'target_hit': False,
                'hours_to_target': 24
            })
            
            # Reset daily execution stats
            self.execution_stats['daily_volume'] = 0
            
            self.logger.info("üåÖ Daily metrics reset for new trading day")
            
        except Exception as e:
            self.logger.error(f"Daily metrics reset failed: {e}")

class DashboardEngine:
    """Advanced real-time dashboard and UI engine for comprehensive bot monitoring"""
    
    def __init__(self, config, binance_client, memory_manager, performance_analytics):
        self.config = config
        self.binance_client = binance_client
        self.memory_manager = memory_manager
        self.performance_analytics = performance_analytics
        self.logger = logging.getLogger('MonsterBot.Dashboard')
        
        # WebSocket server configuration
        self.websocket_port = config.get('dashboard_port', 8888)
        self.websocket_host = config.get('dashboard_host', 'localhost')
        self.connected_clients = set()
        
        # Dashboard data streams
        self.data_streams = {
            'live_metrics': {},
            'price_feeds': {},
            'positions': [],
            'trades': [],
            'pnl_chart': [],
            'strategy_performance': {},
            'risk_metrics': {},
            'market_analysis': {},
            'alerts': [],
            'execution_log': []
        }
        
        # UI Theme configurations
        self.themes = {
            'psycho': {
                'primary_color': '#ff0000',
                'secondary_color': '#ff6666',
                'accent_color': '#ffff00',
                'background': '#1a0000',
                'text_color': '#ffffff',
                'success_color': '#00ff00',
                'warning_color': '#ff8800',
                'style': 'aggressive'
            },
            'hunter': {
                'primary_color': '#ff4400',
                'secondary_color': '#ff8844',
                'accent_color': '#ffaa00',
                'background': '#1a1100',
                'text_color': '#ffffff',
                'success_color': '#44ff44',
                'warning_color': '#ffaa44',
                'style': 'focused'
            },
            'accumulator': {
                'primary_color': '#0066ff',
                'secondary_color': '#3388ff',
                'accent_color': '#00aaff',
                'background': '#001133',
                'text_color': '#ffffff',
                'success_color': '#00cc88',
                'warning_color': '#ffcc00',
                'style': 'professional'
            },
            'whale': {
                'primary_color': '#004488',
                'secondary_color': '#2266aa',
                'accent_color': '#0088cc',
                'background': '#001122',
                'text_color': '#ffffff',
                'success_color': '#00aa66',
                'warning_color': '#ff9900',
                'style': 'institutional'
            }
        }
        
        # Chart configurations
        self.chart_configs = {
            'pnl_chart': {
                'type': 'line',
                'timeframe': '1h',
                'points': 168,  # 1 week of hourly data
                'update_frequency': 60
            },
            'balance_chart': {
                'type': 'area',
                'timeframe': '15m',
                'points': 96,   # 24 hours of 15min data
                'update_frequency': 30
            },
            'strategy_pie': {
                'type': 'pie',
                'update_frequency': 300
            },
            'risk_gauge': {
                'type': 'gauge',
                'max_value': 100,
                'update_frequency': 10
            }
        }
        
        # Real-time alerts system
        self.alert_config = {
            'daily_target_hit': {'priority': 'high', 'sound': True, 'popup': True},
            'large_profit': {'priority': 'high', 'sound': True, 'popup': True},
            'large_loss': {'priority': 'critical', 'sound': True, 'popup': True},
            'high_risk': {'priority': 'medium', 'sound': False, 'popup': True},
            'new_opportunity': {'priority': 'low', 'sound': False, 'popup': False},
            'strategy_milestone': {'priority': 'medium', 'sound': True, 'popup': True}
        }
        
        # Performance widgets
        self.widgets = {
            'tier_progress': {
                'type': 'progress_bar',
                'position': 'top_left',
                'size': 'large',
                'update_frequency': 30
            },
            'daily_target': {
                'type': 'radial_progress',
                'position': 'top_center',
                'size': 'large',
                'update_frequency': 10
            },
            'live_pnl': {
                'type': 'animated_counter',
                'position': 'top_right',
                'size': 'large',
                'update_frequency': 5
            },
            'momentum_meter': {
                'type': 'speedometer',
                'position': 'middle_left',
                'size': 'medium',
                'update_frequency': 15
            },
            'risk_meter': {
                'type': 'thermometer',
                'position': 'middle_right',
                'size': 'medium',
                'update_frequency': 15
            },
            'strategy_grid': {
                'type': 'performance_grid',
                'position': 'bottom_center',
                'size': 'large',
                'update_frequency': 60
            }
        }
        
        # WebSocket message types
        self.message_types = {
            'LIVE_UPDATE': 'live_update',
            'CHART_UPDATE': 'chart_update',
            'ALERT': 'alert',
            'TRADE_UPDATE': 'trade_update',
            'POSITION_UPDATE': 'position_update',
            'STRATEGY_UPDATE': 'strategy_update',
            'MARKET_UPDATE': 'market_update',
            'CONFIG_UPDATE': 'config_update'
        }
        
        # Initialize dashboard
        self.initialize_dashboard()
        
    def initialize_dashboard(self):
        """Initialize dashboard and UI systems"""
        try:
            # Start WebSocket server
            self.start_websocket_server()
            
            # Start data collection
            self.start_data_collection()
            
            # Start chart updates
            self.start_chart_updates()
            
            # Start alert monitoring
            self.start_alert_monitoring()
            
            # Generate initial dashboard data
            self.generate_initial_dashboard()
            
            self.logger.info("üñ•Ô∏è Dashboard Engine initialized")
            self.logger.info(f"   üåê WebSocket server: ws://{self.websocket_host}:{self.websocket_port}")
            self.logger.info(f"   üìä {len(self.widgets)} active widgets")
            self.logger.info(f"   üé® Theme: {self.get_current_theme()}")
            
        except Exception as e:
            self.logger.error(f"Dashboard initialization failed: {e}")
            
    def start_websocket_server(self):
        """Start WebSocket server for real-time communication"""
        import asyncio
        import websockets
        import json
        
        async def handle_client(websocket, path):
            """Handle new WebSocket client connection"""
            try:
                self.connected_clients.add(websocket)
                self.logger.info(f"üì± New dashboard client connected ({len(self.connected_clients)} total)")
                
                # Send initial data
                await self.send_initial_data(websocket)
                
                # Keep connection alive and handle messages
                async for message in websocket:
                    await self.handle_client_message(websocket, message)
                    
            except websockets.exceptions.ConnectionClosed:
                self.connected_clients.discard(websocket)
                self.logger.info(f"üì± Dashboard client disconnected ({len(self.connected_clients)} total)")
            except Exception as e:
                self.logger.error(f"WebSocket client error: {e}")
                self.connected_clients.discard(websocket)
                
        def start_server():
            """Start the WebSocket server in event loop"""
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            start_server = websockets.serve(handle_client, self.websocket_host, self.websocket_port)
            
            loop.run_until_complete(start_server)
            loop.run_forever()
            
        # Start server in separate thread
        import threading
        server_thread = threading.Thread(target=start_server, daemon=True)
        server_thread.start()
        
    async def send_initial_data(self, websocket):
        """Send initial dashboard data to new client"""
        try:
            initial_data = {
                'type': 'INITIAL_LOAD',
                'data': {
                    'theme': self.get_current_theme_config(),
                    'widgets': self.widgets,
                    'live_metrics': self.performance_analytics.live_metrics,
                    'chart_configs': self.chart_configs,
                    'current_tier': self.performance_analytics.live_metrics['current_tier']
                }
            }
            
            await websocket.send(json.dumps(initial_data))
            
        except Exception as e:
            self.logger.error(f"Initial data send failed: {e}")
            
    async def handle_client_message(self, websocket, message):
        """Handle incoming client messages"""
        try:
            data = json.loads(message)
            message_type = data.get('type')
            
            if message_type == 'REQUEST_DATA':
                await self.handle_data_request(websocket, data)
            elif message_type == 'UPDATE_CONFIG':
                await self.handle_config_update(websocket, data)
            elif message_type == 'EXECUTE_COMMAND':
                await self.handle_command_execution(websocket, data)
                
        except Exception as e:
            self.logger.error(f"Client message handling failed: {e}")
            
    async def handle_data_request(self, websocket, request):
        """Handle client data requests"""
        try:
            requested_data = request.get('data_type')
            
            if requested_data == 'live_metrics':
                response = {
                    'type': 'DATA_RESPONSE',
                    'data_type': 'live_metrics',
                    'data': self.performance_analytics.live_metrics
                }
            elif requested_data == 'trade_history':
                response = {
                    'type': 'DATA_RESPONSE',
                    'data_type': 'trade_history',
                    'data': self.get_recent_trade_history()
                }
            elif requested_data == 'position_summary':
                response = {
                    'type': 'DATA_RESPONSE',
                    'data_type': 'position_summary',
                    'data': self.get_position_summary()
                }
            else:
                response = {
                    'type': 'ERROR',
                    'message': f"Unknown data type: {requested_data}"
                }
                
            await websocket.send(json.dumps(response))
            
        except Exception as e:
            self.logger.error(f"Data request handling failed: {e}")
            
    def start_data_collection(self):
        """Start real-time data collection for dashboard"""
        def data_collector():
            while True:
                try:
                    self.update_live_metrics()
                    self.update_position_data()
                    self.update_trade_data()
                    self.update_market_data()
                    
                    # Broadcast updates to all clients
                    asyncio.run(self.broadcast_live_updates())
                    
                    time.sleep(5)  # Update every 5 seconds
                except Exception as e:
                    self.logger.error(f"Data collection error: {e}")
                    time.sleep(10)
                    
        threading.Thread(target=data_collector, daemon=True).start()
        
    def update_live_metrics(self):
        """Update live performance metrics for dashboard"""
        try:
            # Get current metrics from performance analytics
            live_metrics = self.performance_analytics.live_metrics.copy()
            
            # Add dashboard-specific calculations
            live_metrics.update({
                'profit_today_usd': live_metrics['daily_pnl'],
                'profit_today_pct': live_metrics['daily_return_pct'] * 100,
                'target_progress_pct': (live_metrics['daily_return_pct'] / self.performance_analytics.get_daily_target()) * 100,
                'balance_formatted': self.format_currency(live_metrics['current_balance']),
                'pnl_formatted': self.format_currency(live_metrics['daily_pnl']),
                'time_to_target_formatted': self.format_time(live_metrics['hours_to_target']),
                'tier_progress_pct': self.calculate_tier_progress(),
                'momentum_status': self.get_momentum_status(live_metrics['momentum_score']),
                'risk_status': self.get_risk_status(live_metrics['risk_score'])
            })
            
            self.data_streams['live_metrics'] = live_metrics
            
        except Exception as e:
            self.logger.error(f"Live metrics update failed: {e}")
            
    def update_position_data(self):
        """Update position data for dashboard"""
        try:
            positions = self.binance_client.get_open_positions()
            
            formatted_positions = []
            total_unrealized_pnl = 0
            
            for pos in positions:
                position_amt = float(pos.get('positionAmt', 0))
                
                if abs(position_amt) > 0:
                    unrealized_pnl = float(pos.get('unRealizedProfit', 0))
                    total_unrealized_pnl += unrealized_pnl
                    
                    formatted_pos = {
                        'symbol': pos.get('symbol', ''),
                        'side': 'LONG' if position_amt > 0 else 'SHORT',
                        'size': abs(position_amt),
                        'entry_price': float(pos.get('entryPrice', 0)),
                        'mark_price': float(pos.get('markPrice', 0)),
                        'unrealized_pnl': unrealized_pnl,
                        'unrealized_pnl_pct': float(pos.get('percentage', 0)),
                        'notional': abs(float(pos.get('notional', 0))),
                        'leverage': int(float(pos.get('leverage', 1))),
                        'margin': float(pos.get('isolatedMargin', 0))
                    }
                    
                    formatted_positions.append(formatted_pos)
                    
            self.data_streams['positions'] = {
                'positions': formatted_positions,
                'total_unrealized_pnl': total_unrealized_pnl,
                'position_count': len(formatted_positions),
                'total_notional': sum(p['notional'] for p in formatted_positions)
            }
            
        except Exception as e:
            self.logger.error(f"Position data update failed: {e}")
            
    def update_trade_data(self):
        """Update recent trade data for dashboard"""
        try:
            recent_trades = self.performance_analytics.get_recent_trades(50)
            
            formatted_trades = []
            
            for trade in recent_trades[-20:]:  # Last 20 trades
                formatted_trade = {
                    'timestamp': trade['timestamp'],
                    'symbol': trade['symbol'],
                    'strategy': trade['strategy'],
                    'side': trade.get('side', 'UNKNOWN'),
                    'quantity': trade.get('quantity', 0),
                    'entry_price': trade.get('entry_price', 0),
                    'exit_price': trade.get('exit_price', 0),
                    'pnl': trade['pnl'],
                    'pnl_pct': trade['pnl_pct'] * 100,
                    'hold_time': trade.get('hold_time', 0),
                    'leverage': trade.get('leverage', 1),
                    'time_formatted': self.format_timestamp(trade['timestamp']),
                    'pnl_formatted': self.format_currency(trade['pnl']),
                    'status': 'PROFIT' if trade['pnl'] > 0 else 'LOSS'
                }
                
                formatted_trades.append(formatted_trade)
                
            self.data_streams['trades'] = formatted_trades
            
        except Exception as e:
            self.logger.error(f"Trade data update failed: {e}")
            
    def update_market_data(self):
        """Update market analysis data for dashboard"""
        try:
            market_data = {
                'btc_price': 0,
                'eth_price': 0,
                'market_trend': 'neutral',
                'fear_greed_index': 50,
                'volatility_index': 50,
                'funding_rates': {},
                'volume_analysis': 'normal'
            }
            
            # Get current prices
            try:
                btc_ticker = self.binance_client.client.futures_symbol_ticker(symbol='BTCUSDT')
                eth_ticker = self.binance_client.client.futures_symbol_ticker(symbol='ETHUSDT')
                
                market_data['btc_price'] = float(btc_ticker['price'])
                market_data['eth_price'] = float(eth_ticker['price'])
            except:
                pass
                
            # Get market conditions from performance analytics
            market_conditions = self.performance_analytics.market_conditions
            market_data.update({
                'market_trend': market_conditions.get('trend', 'neutral'),
                'volatility': market_conditions.get('volatility', 'normal'),
                'volume': market_conditions.get('volume', 'normal'),
                'sentiment': market_conditions.get('sentiment', 'neutral')
            })
            
            self.data_streams['market_analysis'] = market_data
            
        except Exception as e:
            self.logger.error(f"Market data update failed: {e}")
            
    async def broadcast_live_updates(self):
        """Broadcast live updates to all connected clients"""
        try:
            if not self.connected_clients:
                return
                
            update_message = {
                'type': self.message_types['LIVE_UPDATE'],
                'timestamp': time.time(),
                'data': {
                    'live_metrics': self.data_streams['live_metrics'],
                    'positions': self.data_streams['positions'],
                    'recent_trades': self.data_streams['trades'][-5:],  # Last 5 trades
                    'market_data': self.data_streams['market_analysis']
                }
            }
            
            message_json = json.dumps(update_message)
            
            # Send to all connected clients
            disconnected_clients = set()
            
            for client in self.connected_clients:
                try:
                    await client.send(message_json)
                except:
                    disconnected_clients.add(client)
                    
            # Remove disconnected clients
            self.connected_clients -= disconnected_clients
            
        except Exception as e:
            self.logger.error(f"Live updates broadcast failed: {e}")
            
    def start_chart_updates(self):
        """Start chart data updates"""
        def chart_updater():
            while True:
                try:
                    self.update_pnl_chart()
                    self.update_balance_chart()
                    self.update_strategy_chart()
                    
                    # Broadcast chart updates
                    asyncio.run(self.broadcast_chart_updates())
                    
                    time.sleep(60)  # Update charts every minute
                except Exception as e:
                    self.logger.error(f"Chart update error: {e}")
                    time.sleep(60)
                    
        threading.Thread(target=chart_updater, daemon=True).start()
        
    def update_pnl_chart(self):
        """Update PnL chart data"""
        try:
            # Get hourly PnL data for last 24 hours
            pnl_data = []
            current_time = time.time()
            
            for i in range(24):
                hour_start = current_time - (i * 3600)
                hour_trades = self.get_trades_in_timeframe(hour_start, hour_start + 3600)
                
                hour_pnl = sum(trade.get('pnl', 0) for trade in hour_trades)
                
                pnl_data.append({
                    'timestamp': hour_start,
                    'pnl': hour_pnl,
                    'cumulative_pnl': sum(p['pnl'] for p in pnl_data) + hour_pnl,
                    'trade_count': len(hour_trades)
                })
                
            self.data_streams['pnl_chart'] = list(reversed(pnl_data))
            
        except Exception as e:
            self.logger.error(f"PnL chart update failed: {e}")
            
    def update_balance_chart(self):
        """Update balance progression chart"""
        try:
            # Get balance history from performance analytics
            balance_data = []
            
            for i in range(96):  # Last 24 hours in 15-min intervals
                interval_start = time.time() - (i * 900)  # 15 minutes = 900 seconds
                
                # Estimate balance at this time (simplified)
                estimated_balance = self.estimate_balance_at_time(interval_start)
                
                balance_data.append({
                    'timestamp': interval_start,
                    'balance': estimated_balance,
                    'formatted_time': self.format_timestamp(interval_start, short=True)
                })
                
            self.data_streams['balance_chart'] = list(reversed(balance_data))
            
        except Exception as e:
            self.logger.error(f"Balance chart update failed: {e}")
            
    def update_strategy_chart(self):
        """Update strategy performance chart data"""
        try:
            strategy_data = []
            
            for strategy, performance in self.performance_analytics.strategy_performance.items():
                if performance.get('total_trades', 0) > 0:
                    strategy_data.append({
                        'name': strategy.replace('_', ' ').title(),
                        'trades': performance['total_trades'],
                        'win_rate': performance['win_rate'] * 100,
                        'total_pnl': performance['total_pnl'],
                        'avg_pnl': performance['avg_pnl'],
                        'profit_factor': performance.get('profit_factor', 1)
                    })
                    
            # Sort by total PnL
            strategy_data.sort(key=lambda x: x['total_pnl'], reverse=True)
            
            self.data_streams['strategy_performance'] = strategy_data
            
        except Exception as e:
            self.logger.error(f"Strategy chart update failed: {e}")
            
    async def broadcast_chart_updates(self):
        """Broadcast chart updates to clients"""
        try:
            if not self.connected_clients:
                return
                
            chart_message = {
                'type': self.message_types['CHART_UPDATE'],
                'timestamp': time.time(),
                'data': {
                    'pnl_chart': self.data_streams['pnl_chart'],
                    'balance_chart': self.data_streams['balance_chart'],
                    'strategy_chart': self.data_streams['strategy_performance']
                }
            }
            
            message_json = json.dumps(chart_message)
            
            disconnected_clients = set()
            
            for client in self.connected_clients:
                try:
                    await client.send(message_json)
                except:
                    disconnected_clients.add(client)
                    
            self.connected_clients -= disconnected_clients
            
        except Exception as e:
            self.logger.error(f"Chart updates broadcast failed: {e}")
            
    def start_alert_monitoring(self):
        """Start alert monitoring system"""
        def alert_monitor():
            while True:
                try:
                    self.check_performance_alerts()
                    self.check_risk_alerts()
                    self.check_opportunity_alerts()
                    
                    time.sleep(30)  # Check every 30 seconds
                except Exception as e:
                    self.logger.error(f"Alert monitoring error: {e}")
                    time.sleep(60)
                    
        threading.Thread(target=alert_monitor, daemon=True).start()
        
    def check_performance_alerts(self):
        """Check for performance-related alerts"""
        try:
            live_metrics = self.performance_analytics.live_metrics
            
            # Daily target hit alert
            if live_metrics['target_hit'] and not self.has_alert_been_sent('daily_target_hit'):
                self.send_alert({
                    'type': 'daily_target_hit',
                    'title': 'üéØ DAILY TARGET ACHIEVED!',
                    'message': f"Daily target hit: {live_metrics['daily_return_pct']:.1%}",
                    'priority': 'high',
                    'data': live_metrics
                })
                
            # Large profit alert
            if live_metrics['daily_pnl'] > live_metrics['current_balance'] * 0.1:  # 10% of balance
                if not self.has_alert_been_sent('large_profit'):
                    self.send_alert({
                        'type': 'large_profit',
                        'title': 'üí∞ LARGE PROFIT ALERT',
                        'message': f"Daily profit: ${live_metrics['daily_pnl']:.2f}",
                        'priority': 'high',
                        'data': {'profit': live_metrics['daily_pnl']}
                    })
                    
            # Loss alert
            if live_metrics['daily_pnl'] < -live_metrics['current_balance'] * 0.05:  # -5% of balance
                if not self.has_alert_been_sent('large_loss'):
                    self.send_alert({
                        'type': 'large_loss',
                        'title': '‚ö†Ô∏è SIGNIFICANT LOSS ALERT',
                        'message': f"Daily loss: ${live_metrics['daily_pnl']:.2f}",
                        'priority': 'critical',
                        'data': {'loss': live_metrics['daily_pnl']}
                    })
                    
        except Exception as e:
            self.logger.error(f"Performance alerts check failed: {e}")
            
    def check_risk_alerts(self):
        """Check for risk-related alerts"""
        try:
            risk_score = self.performance_analytics.live_metrics['risk_score']
            
            # High risk alert
            if risk_score > 80 and not self.has_alert_been_sent('high_risk'):
                self.send_alert({
                    'type': 'high_risk',
                    'title': 'üö® HIGH RISK WARNING',
                    'message': f"Risk score: {risk_score:.0f}/100",
                    'priority': 'medium',
                    'data': {'risk_score': risk_score}
                })
                
        except Exception as e:
            self.logger.error(f"Risk alerts check failed: {e}")
            
    def check_opportunity_alerts(self):
        """Check for trading opportunity alerts"""
        try:
            momentum_score = self.performance_analytics.live_metrics['momentum_score']
            
            # High momentum opportunity
            if momentum_score > 90 and not self.has_alert_been_sent('high_momentum'):
                self.send_alert({
                    'type': 'new_opportunity',
                    'title': 'üî• HIGH MOMENTUM DETECTED',
                    'message': f"Momentum score: {momentum_score:.0f}/100",
                    'priority': 'low',
                    'data': {'momentum_score': momentum_score}
                })
                
        except Exception as e:
            self.logger.error(f"Opportunity alerts check failed: {e}")
            
    def send_alert(self, alert_data):
        """Send alert to dashboard clients"""
        try:
            alert_message = {
                'type': self.message_types['ALERT'],
                'timestamp': time.time(),
                'alert': alert_data
            }
            
            # Add to alerts history
            self.data_streams['alerts'].append(alert_data)
            
            # Keep only last 100 alerts
            if len(self.data_streams['alerts']) > 100:
                self.data_streams['alerts'] = self.data_streams['alerts'][-100:]
                
            # Broadcast to clients
            asyncio.run(self.broadcast_alert(alert_message))
            
            # Log alert
            self.logger.info(f"üì¢ ALERT: {alert_data['title']} - {alert_data['message']}")
            
        except Exception as e:
            self.logger.error(f"Alert sending failed: {e}")
            
    async def broadcast_alert(self, alert_message):
        """Broadcast alert to all clients"""
        try:
            message_json = json.dumps(alert_message)
            
            disconnected_clients = set()
            
            for client in self.connected_clients:
                try:
                    await client.send(message_json)
                except:
                    disconnected_clients.add(client)
                    
            self.connected_clients -= disconnected_clients
            
        except Exception as e:
            self.logger.error(f"Alert broadcast failed: {e}")
            
    def has_alert_been_sent(self, alert_type):
        """Check if alert has been sent recently"""
        try:
            recent_alerts = [a for a in self.data_streams['alerts']
                           if a.get('type') == alert_type and 
                           time.time() - a.get('timestamp', 0) < 3600]  # 1 hour
            
            return len(recent_alerts) > 0
            
        except Exception as e:
            self.logger.error(f"Alert check failed: {e}")
            return False
            
    def get_current_theme(self):
        """Get current theme based on capital tier"""
        try:
            current_tier = self.performance_analytics.live_metrics['current_tier']
            return current_tier if current_tier in self.themes else 'psycho'
        except:
            return 'psycho'
            
    def get_current_theme_config(self):
        """Get current theme configuration"""
        try:
            theme_name = self.get_current_theme()
            return {
                'name': theme_name,
                'config': self.themes[theme_name]
            }
        except:
            return {'name': 'psycho', 'config': self.themes['psycho']}
            
    def calculate_tier_progress(self):
        """Calculate progress to next tier"""
        try:
            current_balance = self.performance_analytics.live_metrics['current_balance']
            current_tier = self.performance_analytics.live_metrics['current_tier']
            
            # Get tier boundaries
            tiers = self.performance_analytics.capital_tiers
            
            if current_tier in tiers:
                tier_min = tiers[current_tier]['min']
                tier_max = tiers[current_tier]['max']
                
                if tier_max == float('inf'):
                    return 100  # Already at highest tier
                    
                progress = ((current_balance - tier_min) / (tier_max - tier_min)) * 100
                return min(100, max(0, progress))
                
            return 0
            
        except Exception as e:
            self.logger.error(f"Tier progress calculation failed: {e}")
            return 0
            
    def get_momentum_status(self, momentum_score):
        """Get momentum status description"""
        try:
            if momentum_score >= 80:
                return {'status': 'EXTREME', 'color': '#00ff00', 'icon': 'üî•'}
            elif momentum_score >= 60:
                return {'status': 'HIGH', 'color': '#44ff44', 'icon': 'üìà'}
            elif momentum_score >= 40:
                return {'status': 'MODERATE', 'color': '#ffaa00', 'icon': '‚û°Ô∏è'}
            elif momentum_score >= 20:
                return {'status': 'LOW', 'color': '#ff6600', 'icon': 'üìâ'}
            else:
                return {'status': 'STALLED', 'color': '#ff0000', 'icon': 'üõë'}
                
        except Exception as e:
            self.logger.error(f"Momentum status failed: {e}")
            return {'status': 'UNKNOWN', 'color': '#666666', 'icon': '‚ùì'}
            
    def get_risk_status(self, risk_score):
        """Get risk status description"""
        try:
            if risk_score >= 80:
                return {'status': 'EXTREME', 'color': '#ff0000', 'icon': 'üö®'}
            elif risk_score >= 60:
                return {'status': 'HIGH', 'color': '#ff4400', 'icon': '‚ö†Ô∏è'}
            elif risk_score >= 40:
                return {'status': 'MODERATE', 'color': '#ffaa00', 'icon': '‚ö°'}
            elif risk_score >= 20:
                return {'status': 'LOW', 'color': '#44aa00', 'icon': '‚úÖ'}
            else:
                return {'status': 'MINIMAL', 'color': '#00aa00', 'icon': 'üõ°Ô∏è'}
                
        except Exception as e:
            self.logger.error(f"Risk status failed: {e}")
            return {'status': 'UNKNOWN', 'color': '#666666', 'icon': '‚ùì'}
            
    def format_currency(self, amount):
        """Format currency amount for display"""
        try:
            if abs(amount) >= 1000000:
                return f"${amount/1000000:.2f}M"
            elif abs(amount) >= 1000:
                return f"${amount/1000:.1f}K"
            else:
                return f"${amount:.2f}"
                
        except Exception as e:
            self.logger.error(f"Currency formatting failed: {e}")
            return "$0.00"
            
    def format_time(self, hours):
        """Format time duration for display"""
        try:
            if hours >= 24:
                days = int(hours // 24)
                remaining_hours = int(hours % 24)
                return f"{days}d {remaining_hours}h"
            elif hours >= 1:
                return f"{int(hours)}h {int((hours % 1) * 60)}m"
            else:
                return f"{int(hours * 60)}m"
                
        except Exception as e:
            self.logger.error(f"Time formatting failed: {e}")
            return "0m"
            
    def format_timestamp(self, timestamp, short=False):
        """Format timestamp for display"""
        try:
            dt = datetime.fromtimestamp(timestamp)
            
            if short:
                return dt.strftime('%H:%M')
            else:
                return dt.strftime('%Y-%m-%d %H:%M:%S')
                
        except Exception as e:
            self.logger.error(f"Timestamp formatting failed: {e}")
            return "Unknown"
            
    def get_trades_in_timeframe(self, start_time, end_time):
        """Get trades within specific timeframe"""
        try:
            all_trades = self.performance_analytics.get_recent_trades(200)
            
            timeframe_trades = [
                trade for trade in all_trades
                if start_time <= trade['timestamp'] <= end_time
            ]
            
            return timeframe_trades
            
        except Exception as e:
            self.logger.error(f"Timeframe trades retrieval failed: {e}")
            return []
            
    def estimate_balance_at_time(self, timestamp):
        """Estimate account balance at specific time"""
        try:
            current_balance = self.performance_analytics.live_metrics['current_balance']
            current_time = time.time()
            
            # Get trades since timestamp
            trades_since = [
                trade for trade in self.performance_analytics.get_recent_trades(500)
                if trade['timestamp'] >= timestamp
            ]
            
            # Calculate PnL since timestamp
            pnl_since = sum(trade['pnl'] for trade in trades_since)
            
            # Estimate balance at that time
            estimated_balance = current_balance - pnl_since
            
            return max(0, estimated_balance)
            
        except Exception as e:
            self.logger.error(f"Balance estimation failed: {e}")
            return 0
            
    def get_recent_trade_history(self):
        """Get formatted recent trade history"""
        try:
            trades = self.performance_analytics.get_recent_trades(100)
            
            formatted_trades = []
            
            for trade in trades:
                formatted_trade = {
                    'id': f"{trade['symbol']}_{int(trade['timestamp'])}",
                    'timestamp': trade['timestamp'],
                    'time_formatted': self.format_timestamp(trade['timestamp']),
                    'symbol': trade['symbol'],
                    'strategy': trade['strategy'].replace('_', ' ').title(),
                    'side': trade.get('side', 'UNKNOWN'),
                    'quantity': trade.get('quantity', 0),
                    'entry_price': trade.get('entry_price', 0),
                    'exit_price': trade.get('exit_price', 0),
                    'pnl': trade['pnl'],
                    'pnl_pct': trade['pnl_pct'] * 100,
                    'pnl_formatted': self.format_currency(trade['pnl']),
                    'hold_time': trade.get('hold_time', 0),
                    'hold_time_formatted': self.format_time(trade.get('hold_time', 0) / 3600),
                    'leverage': trade.get('leverage', 1),
                    'status': 'PROFIT' if trade['pnl'] > 0 else 'LOSS',
                    'status_color': '#00aa00' if trade['pnl'] > 0 else '#aa0000'
                }
                
                formatted_trades.append(formatted_trade)
                
            return formatted_trades
            
        except Exception as e:
            self.logger.error(f"Trade history formatting failed: {e}")
            return []
            
    def get_position_summary(self):
        """Get formatted position summary"""
        try:
            positions_data = self.data_streams.get('positions', {})
            
            if not positions_data:
                return {
                    'total_positions': 0,
                    'total_unrealized_pnl': 0,
                    'total_notional': 0,
                    'positions': []
                }
                
            formatted_positions = []
            
            for pos in positions_data.get('positions', []):
                formatted_pos = pos.copy()
                formatted_pos.update({
                    'size_formatted': f"{pos['size']:.6f}",
                    'entry_price_formatted': f"${pos['entry_price']:.4f}",
                    'mark_price_formatted': f"${pos['mark_price']:.4f}",
                    'pnl_formatted': self.format_currency(pos['unrealized_pnl']),
                    'notional_formatted': self.format_currency(pos['notional']),
                    'margin_formatted': self.format_currency(pos['margin']),
                    'side_color': '#00aa00' if pos['side'] == 'LONG' else '#aa0000',
                    'pnl_color': '#00aa00' if pos['unrealized_pnl'] > 0 else '#aa0000'
                })
                
                formatted_positions.append(formatted_pos)
                
            return {
                'total_positions': len(formatted_positions),
                'total_unrealized_pnl': positions_data.get('total_unrealized_pnl', 0),
                'total_unrealized_pnl_formatted': self.format_currency(positions_data.get('total_unrealized_pnl', 0)),
                'total_notional': positions_data.get('total_notional', 0),
                'total_notional_formatted': self.format_currency(positions_data.get('total_notional', 0)),
                'positions': formatted_positions
            }
            
        except Exception as e:
            self.logger.error(f"Position summary formatting failed: {e}")
            return {}
            
    def generate_initial_dashboard(self):
        """Generate initial dashboard layout and data"""
        try:
            # Update all data streams
            self.update_live_metrics()
            self.update_position_data()
            self.update_trade_data()
            self.update_market_data()
            self.update_pnl_chart()
            self.update_balance_chart()
            self.update_strategy_chart()
            
            self.logger.info("üìä Initial dashboard data generated")
            
        except Exception as e:
            self.logger.error(f"Initial dashboard generation failed: {e}")
            
    async def handle_config_update(self, websocket, data):
        """Handle configuration updates from client"""
        try:
            config_type = data.get('config_type')
            config_data = data.get('config_data')
            
            if config_type == 'theme':
                # Theme changes are automatic based on tier
                response = {
                    'type': 'CONFIG_RESPONSE',
                    'success': False,
                    'message': 'Theme changes automatically with capital tier'
                }
            elif config_type == 'alerts':
                self.update_alert_config(config_data)
                response = {
                    'type': 'CONFIG_RESPONSE',
                    'success': True,
                    'message': 'Alert configuration updated'
                }
            elif config_type == 'widgets':
                self.update_widget_config(config_data)
                response = {
                    'type': 'CONFIG_RESPONSE',
                    'success': True,
                    'message': 'Widget configuration updated'
                }
            else:
                response = {
                    'type': 'CONFIG_RESPONSE',
                    'success': False,
                    'message': f'Unknown config type: {config_type}'
                }
                
            await websocket.send(json.dumps(response))
            
        except Exception as e:
            self.logger.error(f"Config update handling failed: {e}")
            
    async def handle_command_execution(self, websocket, data):
        """Handle command execution from client"""
        try:
            command = data.get('command')
            params = data.get('params', {})
            
            if command == 'emergency_stop':
                self.execute_emergency_stop()
                response = {
                    'type': 'COMMAND_RESPONSE',
                    'success': True,
                    'message': 'Emergency stop executed'
                }
            elif command == 'reset_daily_metrics':
                self.performance_analytics.reset_daily_metrics()
                response = {
                    'type': 'COMMAND_RESPONSE',
                    'success': True,
                    'message': 'Daily metrics reset'
                }
            elif command == 'force_data_refresh':
                self.generate_initial_dashboard()
                response = {
                    'type': 'COMMAND_RESPONSE',
                    'success': True,
                    'message': 'Data refreshed'
                }
            else:
                response = {
                    'type': 'COMMAND_RESPONSE',
                    'success': False,
                    'message': f'Unknown command: {command}'
                }
                
            await websocket.send(json.dumps(response))
            
        except Exception as e:
            self.logger.error(f"Command execution handling failed: {e}")
            
    def update_alert_config(self, config_data):
        """Update alert configuration"""
        try:
            for alert_type, config in config_data.items():
                if alert_type in self.alert_config:
                    self.alert_config[alert_type].update(config)
                    
            self.logger.info("‚öôÔ∏è Alert configuration updated")
            
        except Exception as e:
            self.logger.error(f"Alert config update failed: {e}")
            
    def update_widget_config(self, config_data):
        """Update widget configuration"""
        try:
            for widget_name, config in config_data.items():
                if widget_name in self.widgets:
                    self.widgets[widget_name].update(config)
                    
            self.logger.info("‚öôÔ∏è Widget configuration updated")
            
        except Exception as e:
            self.logger.error(f"Widget config update failed: {e}")
            
    def execute_emergency_stop(self):
        """Execute emergency stop command"""
        try:
            self.logger.warning("üö® EMERGENCY STOP COMMAND RECEIVED")
            
            # This would integrate with trading engine to stop all trading
            # For now, just send alert
            self.send_alert({
                'type': 'emergency_stop',
                'title': 'üö® EMERGENCY STOP ACTIVATED',
                'message': 'All trading activities halted by user command',
                'priority': 'critical',
                'data': {'timestamp': time.time()}
            })
            
        except Exception as e:
            self.logger.error(f"Emergency stop execution failed: {e}")
            
    def get_dashboard_statistics(self):
        """Get dashboard usage statistics"""
        try:
            return {
                'connected_clients': len(self.connected_clients),
                'total_alerts_sent': len(self.data_streams['alerts']),
                'uptime_hours': (time.time() - self.start_time) / 3600 if hasattr(self, 'start_time') else 0,
                'data_updates_per_minute': 12,  # 5s live updates + 60s chart updates
                'current_theme': self.get_current_theme(),
                'active_widgets': len([w for w in self.widgets.values() if w.get('enabled', True)])
            }
            
        except Exception as e:
            self.logger.error(f"Dashboard statistics failed: {e}")
            return {}
            
    def generate_html_dashboard(self):
        """Generate standalone HTML dashboard for backup access"""
        try:
            html_template = """
            <!DOCTYPE html>
            <html lang="en">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>MonsterBot Dashboard</title>
                <style>
                    body {
                        background: %(background)s;
                        color: %(text_color)s;
                        font-family: 'Courier New', monospace;
                        margin: 0;
                        padding: 20px;
                    }
                    .dashboard {
                        display: grid;
                        grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
                        gap: 20px;
                    }
                    .widget {
                        background: rgba(255,255,255,0.1);
                        border: 2px solid %(primary_color)s;
                        border-radius: 10px;
                        padding: 20px;
                    }
                    .metric-value {
                        font-size: 2em;
                        font-weight: bold;
                        color: %(accent_color)s;
                    }
                    .metric-label {
                        font-size: 0.9em;
                        opacity: 0.8;
                    }
                    .profit { color: %(success_color)s; }
                    .loss { color: #ff4444; }
                    .refresh-btn {
                        background: %(primary_color)s;
                        color: white;
                        border: none;
                        padding: 10px 20px;
                        border-radius: 5px;
                        cursor: pointer;
                        margin-bottom: 20px;
                    }
                </style>
            </head>
            <body>
                <h1>ü§ñ MonsterBot Dashboard</h1>
                <button class="refresh-btn" onclick="location.reload()">üîÑ Refresh</button>
                
                <div class="dashboard">
                    <div class="widget">
                        <div class="metric-label">Current Balance</div>
                        <div class="metric-value">%(balance)s</div>
                    </div>
                    
                    <div class="widget">
                        <div class="metric-label">Daily PnL</div>
                        <div class="metric-value %(pnl_class)s">%(daily_pnl)s</div>
                    </div>
                    
                    <div class="widget">
                        <div class="metric-label">Daily Return</div>
                        <div class="metric-value %(return_class)s">%(daily_return)s</div>
                    </div>
                    
                    <div class="widget">
                        <div class="metric-label">Target Progress</div>
                        <div class="metric-value">%(target_progress)s</div>
                    </div>
                    
                    <div class="widget">
                        <div class="metric-label">Current Tier</div>
                        <div class="metric-value">%(tier)s</div>
                    </div>
                    
                    <div class="widget">
                        <div class="metric-label">Risk Score</div>
                        <div class="metric-value">%(risk_score)s/100</div>
                    </div>
                </div>
                
                <script>
                    // Auto-refresh every 30 seconds
                    setTimeout(function() {
                        location.reload();
                    }, 30000);
                </script>
            </body>
            </html>
            """
            
            # Get current theme and metrics
            theme = self.get_current_theme_config()['config']
            metrics = self.performance_analytics.live_metrics
            
            # Format data
            html_data = {
                'background': theme['background'],
                'text_color': theme['text_color'],
                'primary_color': theme['primary_color'],
                'accent_color': theme['accent_color'],
                'success_color': theme['success_color'],
                'balance': self.format_currency(metrics['current_balance']),
                'daily_pnl': self.format_currency(metrics['daily_pnl']),
                'pnl_class': 'profit' if metrics['daily_pnl'] > 0 else 'loss',
                'daily_return': f"{metrics['daily_return_pct']*100:.1f}%",
                'return_class': 'profit' if metrics['daily_return_pct'] > 0 else 'loss',
                'target_progress': f"{(metrics['daily_return_pct']/self.performance_analytics.get_daily_target())*100:.1f}%",
                'tier': metrics['current_tier'].upper(),
                'risk_score': f"{metrics['risk_score']:.0f}"
            }
            
            return html_template % html_data
            
        except Exception as e:
            self.logger.error(f"HTML dashboard generation failed: {e}")
            return "<html><body><h1>Dashboard Error</h1></body></html>"
            
    def save_dashboard_snapshot(self):
        """Save current dashboard data as snapshot"""
        try:
            snapshot_data = {
                'timestamp': time.time(),
                'live_metrics': self.data_streams['live_metrics'],
                'positions': self.data_streams['positions'],
                'trades': self.data_streams['trades'][-10:],  # Last 10 trades
                'market_data': self.data_streams['market_analysis'],
                'pnl_chart': self.data_streams['pnl_chart'][-24:],  # Last 24 hours
                'strategy_performance': self.data_streams['strategy_performance']
            }
            
            # Save to memory manager
            self.memory_manager.save_dashboard_snapshot(snapshot_data)
            
            self.logger.info("üì∏ Dashboard snapshot saved")
            
        except Exception as e:
            self.logger.error(f"Dashboard snapshot failed: {e}")
            
    def get_dashboard_health_status(self):
        """Get dashboard system health status"""
        try:
            return {
                'websocket_server': 'healthy' if len(self.connected_clients) >= 0 else 'error',
                'data_streams': 'healthy' if self.data_streams['live_metrics'] else 'error',
                'chart_updates': 'healthy' if self.data_streams['pnl_chart'] else 'error',
                'alert_system': 'healthy',
                'connected_clients': len(self.connected_clients),
                'last_update': time.time(),
                'memory_usage': 'normal',
                'performance': 'optimal'
            }
            
        except Exception as e:
            self.logger.error(f"Dashboard health check failed: {e}")
            return {'status': 'error', 'error': str(e)}

class NotificationEngine:
    """Advanced multi-channel notification system for real-time trading alerts"""
    
    def __init__(self, config, binance_client, memory_manager, performance_analytics):
        self.config = config
        self.binance_client = binance_client
        self.memory_manager = memory_manager
        self.performance_analytics = performance_analytics
        self.logger = logging.getLogger('MonsterBot.Notifications')
        
        # Notification channels configuration
        self.channels = {
            'telegram': {
                'enabled': config.get('telegram_enabled', True),
                'bot_token': config.get('telegram_bot_token', ''),
                'chat_id': config.get('telegram_chat_id', ''),
                'priority_levels': ['critical', 'high', 'medium'],
                'rate_limit': 30,  # messages per minute
                'message_queue': deque(),
                'last_sent': defaultdict(float)
            },
            'discord': {
                'enabled': config.get('discord_enabled', False),
                'webhook_url': config.get('discord_webhook_url', ''),
                'priority_levels': ['critical', 'high'],
                'rate_limit': 20,
                'message_queue': deque(),
                'last_sent': defaultdict(float)
            },
            'email': {
                'enabled': config.get('email_enabled', False),
                'smtp_server': config.get('smtp_server', 'smtp.gmail.com'),
                'smtp_port': config.get('smtp_port', 587),
                'email_user': config.get('email_user', ''),
                'email_password': config.get('email_password', ''),
                'recipient': config.get('email_recipient', ''),
                'priority_levels': ['critical'],
                'rate_limit': 5,
                'message_queue': deque(),
                'last_sent': defaultdict(float)
            },
            'sms': {
                'enabled': config.get('sms_enabled', False),
                'twilio_sid': config.get('twilio_sid', ''),
                'twilio_token': config.get('twilio_token', ''),
                'from_number': config.get('sms_from_number', ''),
                'to_number': config.get('sms_to_number', ''),
                'priority_levels': ['critical'],
                'rate_limit': 2,
                'message_queue': deque(),
                'last_sent': defaultdict(float)
            }
        }
        
        # Notification types and configurations
        self.notification_types = {
            'daily_target_hit': {
                'priority': 'high',
                'cooldown': 3600,  # 1 hour cooldown
                'channels': ['telegram', 'discord'],
                'template': 'üéØ DAILY TARGET ACHIEVED!\nüí∞ Profit: {profit}\nüìà Return: {return_pct}\n‚ö° Tier: {tier}',
                'sound': True,
                'emoji': 'üéØ'
            },
            'large_profit': {
                'priority': 'high',
                'cooldown': 1800,  # 30 minutes
                'channels': ['telegram', 'discord'],
                'template': 'üí∞ LARGE PROFIT ALERT!\nüíµ Amount: {amount}\nüìä Trade: {symbol}\n‚ö° Strategy: {strategy}',
                'sound': True,
                'emoji': 'üí∞'
            },
            'large_loss': {
                'priority': 'critical',
                'cooldown': 600,  # 10 minutes
                'channels': ['telegram', 'discord', 'email', 'sms'],
                'template': 'üö® SIGNIFICANT LOSS ALERT!\nüí∏ Amount: {amount}\nüìâ Trade: {symbol}\n‚ö†Ô∏è Action: {action}',
                'sound': True,
                'emoji': 'üö®'
            },
            'daily_target_progress': {
                'priority': 'medium',
                'cooldown': 7200,  # 2 hours
                'channels': ['telegram'],
                'template': 'üìà Daily Progress Update\nüéØ Target: {target_pct}\nüíπ Current: {current_pct}\n‚è∞ Progress: {progress_pct}',
                'sound': False,
                'emoji': 'üìà'
            },
            'tier_upgrade': {
                'priority': 'high',
                'cooldown': 0,  # No cooldown for tier upgrades
                'channels': ['telegram', 'discord'],
                'template': 'üöÄ TIER UPGRADE!\n‚¨ÜÔ∏è New Tier: {new_tier}\nüí∞ Balance: {balance}\nüéØ New Target: {new_target}',
                'sound': True,
                'emoji': 'üöÄ'
            },
            'high_momentum': {
                'priority': 'medium',
                'cooldown': 1800,
                'channels': ['telegram'],
                'template': 'üî• HIGH MOMENTUM DETECTED!\nüìä Score: {momentum_score}/100\n‚ö° Opportunity: {opportunity}\nüéØ Action: Increase aggression',
                'sound': False,
                'emoji': 'üî•'
            },
            'risk_warning': {
                'priority': 'high',
                'cooldown': 900,  # 15 minutes
                'channels': ['telegram', 'discord'],
                'template': '‚ö†Ô∏è HIGH RISK WARNING!\nüå°Ô∏è Risk Score: {risk_score}/100\nüíº Exposure: {exposure}\nüõ°Ô∏è Action: Reduce positions',
                'sound': True,
                'emoji': '‚ö†Ô∏è'
            },
            'strategy_milestone': {
                'priority': 'medium',
                'cooldown': 3600,
                'channels': ['telegram'],
                'template': '‚≠ê Strategy Milestone!\nüß† Strategy: {strategy}\nüìä Performance: {performance}\nüèÜ Achievement: {milestone}',
                'sound': False,
                'emoji': '‚≠ê'
            },
            'arbitrage_opportunity': {
                'priority': 'low',
                'cooldown': 300,  # 5 minutes
                'channels': ['telegram'],
                'template': 'üíé Arbitrage Opportunity\nüîÑ Type: {arb_type}\nüí∞ Profit: {profit_pct}\n‚ö° Executing...',
                'sound': False,
                'emoji': 'üíé'
            },
            'emergency_stop': {
                'priority': 'critical',
                'cooldown': 0,
                'channels': ['telegram', 'discord', 'email', 'sms'],
                'template': 'üö® EMERGENCY STOP ACTIVATED!\n‚èπÔ∏è All trading halted\nüìû Manual intervention required\n‚ö†Ô∏è Check dashboard immediately',
                'sound': True,
                'emoji': 'üö®'
            }
        }
        
        # Notification statistics
        self.stats = {
            'total_sent': defaultdict(int),
            'channel_stats': defaultdict(lambda: defaultdict(int)),
            'priority_stats': defaultdict(int),
            'failed_notifications': defaultdict(int),
            'rate_limited': defaultdict(int)
        }
        
        # Message formatting for different capital tiers
        self.tier_styles = {
            'psycho': {
                'prefix': 'üî•üíÄ PSYCHO MODE üíÄüî•',
                'style': 'AGGRESSIVE',
                'emojis': ['üöÄ', 'üí•', '‚ö°', 'üî•', 'üíÄ']
            },
            'hunter': {
                'prefix': 'üéØüî• HUNTER MODE üî•üéØ',
                'style': 'FOCUSED',
                'emojis': ['üéØ', 'üî•', '‚ö°', 'üí∞', 'üìà']
            },
            'accumulator': {
                'prefix': 'üíéüìà ACCUMULATOR MODE üìàüíé',
                'style': 'STEADY',
                'emojis': ['üíé', 'üìà', 'üí∞', '‚≠ê', 'üèÜ']
            },
            'whale': {
                'prefix': 'üêãüíé WHALE MODE üíéüêã',
                'style': 'INSTITUTIONAL',
                'emojis': ['üêã', 'üíé', 'üëë', 'üè¶', 'üíº']
            }
        }
        
        # Initialize notification system
        self.initialize_notification_system()
        
    def initialize_notification_system(self):
        """Initialize notification system and test connections"""
        try:
            # Test channel connections
            active_channels = []
            
            for channel_name, channel_config in self.channels.items():
                if channel_config['enabled']:
                    if self.test_channel_connection(channel_name):
                        active_channels.append(channel_name)
                        self.logger.info(f"‚úÖ {channel_name.title()} notifications enabled")
                    else:
                        self.logger.warning(f"‚ùå {channel_name.title()} connection failed")
                        channel_config['enabled'] = False
                        
            # Start notification processing
            self.start_notification_processor()
            
            # Start periodic status updates
            self.start_status_updates()
            
            self.logger.info("üì¢ Notification Engine initialized")
            self.logger.info(f"   üì± Active channels: {', '.join(active_channels)}")
            self.logger.info(f"   üîî Notification types: {len(self.notification_types)}")
            
            # Send startup notification
            self.send_notification('system_startup', {
                'timestamp': time.time(),
                'active_channels': len(active_channels),
                'current_balance': self.performance_analytics.live_metrics['current_balance']
            })
            
        except Exception as e:
            self.logger.error(f"Notification system initialization failed: {e}")
            
    def test_channel_connection(self, channel_name):
        """Test connection to notification channel"""
        try:
            if channel_name == 'telegram':
                return self.test_telegram_connection()
            elif channel_name == 'discord':
                return self.test_discord_connection()
            elif channel_name == 'email':
                return self.test_email_connection()
            elif channel_name == 'sms':
                return self.test_sms_connection()
            else:
                return False
                
        except Exception as e:
            self.logger.error(f"{channel_name.title()} connection test failed: {e}")
            return False
            
    def test_telegram_connection(self):
        """Test Telegram bot connection"""
        try:
            import requests
            
            bot_token = self.channels['telegram']['bot_token']
            
            if not bot_token:
                return False
                
            # Test API call
            url = f"https://api.telegram.org/bot{bot_token}/getMe"
            response = requests.get(url, timeout=10)
            
            if response.status_code == 200:
                bot_info = response.json()
                if bot_info.get('ok'):
                    self.logger.info(f"ü§ñ Telegram bot connected: {bot_info['result']['username']}")
                    return True
                    
            return False
            
        except Exception as e:
            self.logger.error(f"Telegram connection test failed: {e}")
            return False
            
    def test_discord_connection(self):
        """Test Discord webhook connection"""
        try:
            import requests
            
            webhook_url = self.channels['discord']['webhook_url']
            
            if not webhook_url:
                return False
                
            # Test webhook with a simple message
            payload = {
                'content': 'ü§ñ MonsterBot notification system test',
                'username': 'MonsterBot'
            }
            
            response = requests.post(webhook_url, json=payload, timeout=10)
            
            if response.status_code in [200, 204]:
                self.logger.info("üéÆ Discord webhook connected")
                return True
                
            return False
            
        except Exception as e:
            self.logger.error(f"Discord connection test failed: {e}")
            return False
            
    def test_email_connection(self):
        """Test email SMTP connection"""
        try:
            import smtplib
            from email.mime.text import MIMEText
            
            email_config = self.channels['email']
            
            if not all([email_config['email_user'], email_config['email_password']]):
                return False
                
            # Test SMTP connection
            server = smtplib.SMTP(email_config['smtp_server'], email_config['smtp_port'])
            server.starttls()
            server.login(email_config['email_user'], email_config['email_password'])
            server.quit()
            
            self.logger.info("üìß Email SMTP connected")
            return True
            
        except Exception as e:
            self.logger.error(f"Email connection test failed: {e}")
            return False
            
    def test_sms_connection(self):
        """Test SMS (Twilio) connection"""
        try:
            # This would use Twilio API in production
            sms_config = self.channels['sms']
            
            if not all([sms_config['twilio_sid'], sms_config['twilio_token']]):
                return False
                
            # Simulate successful connection for demo
            self.logger.info("üì± SMS service connected")
            return True
            
        except Exception as e:
            self.logger.error(f"SMS connection test failed: {e}")
            return False
            
    def start_notification_processor(self):
        """Start notification processing thread"""
        def notification_processor():
            while True:
                try:
                    self.process_notification_queues()
                    time.sleep(1)  # Process every second
                except Exception as e:
                    self.logger.error(f"Notification processing error: {e}")
                    time.sleep(5)
                    
        threading.Thread(target=notification_processor, daemon=True).start()
        
    def start_status_updates(self):
        """Start periodic status updates"""
        def status_updater():
            while True:
                try:
                    self.send_periodic_updates()
                    time.sleep(1800)  # Every 30 minutes
                except Exception as e:
                    self.logger.error(f"Status update error: {e}")
                    time.sleep(1800)
                    
        threading.Thread(target=status_updater, daemon=True).start()
        
    def send_notification(self, notification_type, data):
        """Send notification of specified type"""
        try:
            if notification_type not in self.notification_types:
                self.logger.warning(f"Unknown notification type: {notification_type}")
                return False
                
            notification_config = self.notification_types[notification_type]
            
            # Check cooldown
            if not self.check_cooldown(notification_type):
                return False
                
            # Format message
            message = self.format_message(notification_type, data)
            
            # Send to appropriate channels
            success_count = 0
            
            for channel_name in notification_config['channels']:
                if self.channels[channel_name]['enabled']:
                    if self.is_priority_allowed(channel_name, notification_config['priority']):
                        if self.send_to_channel(channel_name, message, notification_config):
                            success_count += 1
                            
            # Update statistics
            self.stats['total_sent'][notification_type] += success_count
            self.stats['priority_stats'][notification_config['priority']] += success_count
            
            # Update cooldown
            self.update_cooldown(notification_type)
            
            if success_count > 0:
                self.logger.info(f"üì¢ Notification sent: {notification_type} ({success_count} channels)")
                return True
            else:
                self.logger.warning(f"üì¢ Notification failed: {notification_type}")
                return False
                
        except Exception as e:
            self.logger.error(f"Notification sending failed: {e}")
            return False
            
    def check_cooldown(self, notification_type):
        """Check if notification is within cooldown period"""
        try:
            cooldown = self.notification_types[notification_type]['cooldown']
            
            if cooldown == 0:
                return True
                
            last_sent = self.notification_types[notification_type].get('last_sent', 0)
            
            return time.time() - last_sent >= cooldown
            
        except Exception as e:
            self.logger.error(f"Cooldown check failed: {e}")
            return True
            
    def update_cooldown(self, notification_type):
        """Update last sent timestamp for notification type"""
        try:
            self.notification_types[notification_type]['last_sent'] = time.time()
        except Exception as e:
            self.logger.error(f"Cooldown update failed: {e}")
            
    def is_priority_allowed(self, channel_name, priority):
        """Check if channel accepts this priority level"""
        try:
            allowed_priorities = self.channels[channel_name]['priority_levels']
            return priority in allowed_priorities
        except Exception as e:
            self.logger.error(f"Priority check failed: {e}")
            return False
            
    def format_message(self, notification_type, data):
        """Format notification message with data"""
        try:
            config = self.notification_types[notification_type]
            template = config['template']
            
            # Get current tier style
            current_tier = self.performance_analytics.live_metrics.get('current_tier', 'psycho')
            tier_style = self.tier_styles.get(current_tier, self.tier_styles['psycho'])
            
            # Format the message
            try:
                formatted_message = template.format(**data)
            except KeyError as e:
                # Handle missing keys gracefully
                formatted_message = template
                self.logger.warning(f"Missing data key for notification: {e}")
                
            # Add tier prefix and styling
            full_message = f"{tier_style['prefix']}\n\n{config['emoji']} {formatted_message}"
            
            # Add timestamp
            timestamp = datetime.now().strftime('%H:%M:%S')
            full_message += f"\n\n‚è∞ {timestamp}"
            
            return full_message
            
        except Exception as e:
            self.logger.error(f"Message formatting failed: {e}")
            return f"Notification error: {notification_type}"
            
    def send_to_channel(self, channel_name, message, notification_config):
        """Send message to specific channel"""
        try:
            channel = self.channels[channel_name]
            
            # Check rate limiting
            if not self.check_rate_limit(channel_name):
                self.stats['rate_limited'][channel_name] += 1
                return False
                
            # Add to queue
            channel['message_queue'].append({
                'message': message,
                'priority': notification_config['priority'],
                'timestamp': time.time(),
                'sound': notification_config.get('sound', False)
            })
            
            return True
            
        except Exception as e:
            self.logger.error(f"Channel sending failed: {e}")
            self.stats['failed_notifications'][channel_name] += 1
            return False
            
    def check_rate_limit(self, channel_name):
        """Check if channel is within rate limit"""
        try:
            channel = self.channels[channel_name]
            rate_limit = channel['rate_limit']
            
            current_time = time.time()
            
            # Count messages sent in last minute
            recent_messages = sum(1 for timestamp in channel['last_sent'].values() 
                                if current_time - timestamp < 60)
            
            return recent_messages < rate_limit
            
        except Exception as e:
            self.logger.error(f"Rate limit check failed: {e}")
            return True
            
    def process_notification_queues(self):
        """Process pending notifications in queues"""
        try:
            for channel_name, channel in self.channels.items():
                if not channel['enabled'] or not channel['message_queue']:
                    continue
                    
                # Process one message per cycle per channel
                message_data = channel['message_queue'].popleft()
                
                success = False
                
                if channel_name == 'telegram':
                    success = self.send_telegram_message(message_data)
                elif channel_name == 'discord':
                    success = self.send_discord_message(message_data)
                elif channel_name == 'email':
                    success = self.send_email_message(message_data)
                elif channel_name == 'sms':
                    success = self.send_sms_message(message_data)
                    
                if success:
                    self.stats['channel_stats'][channel_name]['sent'] += 1
                    channel['last_sent'][str(time.time())] = time.time()
                    
                    # Clean old timestamps
                    cutoff = time.time() - 60
                    channel['last_sent'] = {k: v for k, v in channel['last_sent'].items() if v > cutoff}
                else:
                    self.stats['channel_stats'][channel_name]['failed'] += 1
                    
        except Exception as e:
            self.logger.error(f"Notification queue processing failed: {e}")
            
    def send_telegram_message(self, message_data):
        """Send message via Telegram"""
        try:
            import requests
            
            config = self.channels['telegram']
            bot_token = config['bot_token']
            chat_id = config['chat_id']
            
            if not bot_token or not chat_id:
                return False
                
            url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
            
            payload = {
                'chat_id': chat_id,
                'text': message_data['message'],
                'parse_mode': 'HTML',
                'disable_web_page_preview': True
            }
            
            response = requests.post(url, json=payload, timeout=10)
            
            if response.status_code == 200:
                result = response.json()
                return result.get('ok', False)
                
            return False
            
        except Exception as e:
            self.logger.error(f"Telegram message sending failed: {e}")
            return False
            
    def send_discord_message(self, message_data):
        """Send message via Discord webhook"""
        try:
            import requests
            
            webhook_url = self.channels['discord']['webhook_url']
            
            if not webhook_url:
                return False
                
            payload = {
                'content': message_data['message'],
                'username': 'MonsterBot',
                'avatar_url': 'https://i.imgur.com/robot.png'  # Optional robot avatar
            }
            
            response = requests.post(webhook_url, json=payload, timeout=10)
            
            return response.status_code in [200, 204]
            
        except Exception as e:
            self.logger.error(f"Discord message sending failed: {e}")
            return False
            
    def send_email_message(self, message_data):
        """Send message via email"""
        try:
            import smtplib
            from email.mime.text import MIMEText
            from email.mime.multipart import MIMEMultipart
            
            config = self.channels['email']
            
            msg = MIMEMultipart()
            msg['From'] = config['email_user']
            msg['To'] = config['recipient']
            msg['Subject'] = f"MonsterBot Alert - {message_data['priority'].upper()}"
            
            # Convert message to HTML
            html_message = message_data['message'].replace('\n', '<br>')
            msg.attach(MIMEText(html_message, 'html'))
            
            server = smtplib.SMTP(config['smtp_server'], config['smtp_port'])
            server.starttls()
            server.login(config['email_user'], config['email_password'])
            server.send_message(msg)
            server.quit()
            
            return True
            
        except Exception as e:
            self.logger.error(f"Email sending failed: {e}")
            return False
            
    def send_sms_message(self, message_data):
        """Send message via SMS (Twilio)"""
        try:
            # This would use Twilio API in production
            config = self.channels['sms']
            
            # For demo purposes, just log the SMS
            self.logger.info(f"üì± SMS would be sent to {config['to_number']}: {message_data['message'][:50]}...")
            
            return True
            
        except Exception as e:
            self.logger.error(f"SMS sending failed: {e}")
            return False
            
    def send_periodic_updates(self):
        """Send periodic status updates"""
        try:
            # Send daily progress update every 2 hours during trading
            current_hour = datetime.now().hour
            
            if 6 <= current_hour <= 22:  # Only during active hours
                metrics = self.performance_analytics.live_metrics
                target = self.performance_analytics.get_daily_target()
                
                if target > 0:
                    progress_pct = (metrics['daily_return_pct'] / target) * 100
                    
                    self.send_notification('daily_target_progress', {
                        'target_pct': f"{target:.1%}",
                        'current_pct': f"{metrics['daily_return_pct']:.1%}",
                        'progress_pct': f"{progress_pct:.1f}%"
                    })
                    
        except Exception as e:
            self.logger.error(f"Periodic updates failed: {e}")
            
    def notify_daily_target_hit(self):
        """Notify when daily target is achieved"""
        try:
            metrics = self.performance_analytics.live_metrics
            
            if metrics['target_hit']:
                self.send_notification('daily_target_hit', {
                    'profit': f"${metrics['daily_pnl']:.2f}",
                    'return_pct': f"{metrics['daily_return_pct']:.1%}",
                    'tier': metrics['current_tier'].upper()
                })
                
        except Exception as e:
            self.logger.error(f"Daily target notification failed: {e}")
            
    def notify_large_profit(self, trade_data):
        """Notify on large profitable trades"""
        try:
            profit_threshold = self.performance_analytics.live_metrics['current_balance'] * 0.05  # 5% of balance
            
            if trade_data.get('pnl', 0) > profit_threshold:
                self.send_notification('large_profit', {
                    'amount': f"${trade_data['pnl']:.2f}",
                    'symbol': trade_data.get('symbol', 'Unknown'),
                    'strategy': trade_data.get('strategy', 'Unknown').replace('_', ' ').title()
                })
                
        except Exception as e:
            self.logger.error(f"Large profit notification failed: {e}")
            
    def notify_large_loss(self, trade_data):
        """Notify on significant losses"""
        try:
            loss_threshold = -self.performance_analytics.live_metrics['current_balance'] * 0.03  # -3% of balance
            
            if trade_data.get('pnl', 0) < loss_threshold:
                self.send_notification('large_loss', {
                    'amount': f"${trade_data['pnl']:.2f}",
                    'symbol': trade_data.get('symbol', 'Unknown'),
                    'action': 'Review strategy and risk management'
                })
                
        except Exception as e:
            self.logger.error(f"Large loss notification failed: {e}")
            
    def notify_tier_upgrade(self, old_tier, new_tier):
        """Notify when capital tier upgrades"""
        try:
            balance = self.performance_analytics.live_metrics['current_balance']
            new_target = self.performance_analytics.get_daily_target()
            
            self.send_notification('tier_upgrade', {
                'new_tier': new_tier.upper(),
                'balance': f"${balance:.2f}",
                'new_target': f"{new_target:.1%}"
            })
            
        except Exception as e:
            self.logger.error(f"Tier upgrade notification failed: {e}")
            
    def notify_high_momentum(self, momentum_score):
        """Notify when momentum is very high"""
        try:
            if momentum_score > 85:
                self.send_notification('high_momentum', {
                    'momentum_score': f"{momentum_score:.0f}",
                    'opportunity': 'Market conditions favorable for aggressive trades'
                })
                
        except Exception as e:
            self.logger.error(f"High momentum notification failed: {e}")
            
    def notify_risk_warning(self, risk_score):
        """Notify when risk is too high"""
        try:
            if risk_score > 75:
                balance = self.performance_analytics.live_metrics['current_balance']
                
                # Calculate current exposure
                positions = self.binance_client.get_open_positions()
                total_exposure = sum(abs(float(pos.get('notional', 0))) for pos in positions)
                exposure_pct = (total_exposure / balance) * 100 if balance > 0 else 0
                
                self.send_notification('risk_warning', {
                    'risk_score': f"{risk_score:.0f}",
                    'exposure': f"{exposure_pct:.1f}% of balance"
                })
                
        except Exception as e:
            self.logger.error(f"Risk warning notification failed: {e}")
            
    def notify_arbitrage_opportunity(self, opportunity_data):
        """Notify about arbitrage opportunities"""
        try:
            profit_pct = opportunity_data.get('profit_pct', 0) * 100
            
            if profit_pct > 0.5:  # Only notify for >0.5% opportunities
                self.send_notification('arbitrage_opportunity', {
                    'arb_type': opportunity_data.get('type', 'Unknown'),
                    'profit_pct': f"{profit_pct:.2f}%"
                })
                
        except Exception as e:
            self.logger.error(f"Arbitrage opportunity notification failed: {e}")
            
    def notify_emergency_stop(self):
        """Notify about emergency stop activation"""
        try:
            self.send_notification('emergency_stop', {
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            })
            
        except Exception as e:
            self.logger.error(f"Emergency stop notification failed: {e}")
            
    def get_notification_statistics(self):
        """Get notification system statistics"""
        try:
            return {
                'total_notifications_sent': sum(self.stats['total_sent'].values()),
                'notifications_by_type': dict(self.stats['total_sent']),
                'notifications_by_priority': dict(self.stats['priority_stats']),
                'channel_performance': {
                    channel: {
                        'sent': self.stats['channel_stats'][channel]['sent'],
                        'failed': self.stats['channel_stats'][channel]['failed'],
                        'rate_limited': self.stats['rate_limited'][channel],
                        'success_rate': (
                            self.stats['channel_stats'][channel]['sent'] / 
                            (self.stats['channel_stats'][channel]['sent'] + self.stats['channel_stats'][channel]['failed'])
                        ) * 100 if (self.stats['channel_stats'][channel]['sent'] + self.stats['channel_stats'][channel]['failed']) > 0 else 0
                    }
                    for channel in self.channels.keys()
                },
                'active_channels': [name for name, config in self.channels.items() if config['enabled']],
                'queue_sizes': {name: len(config['message_queue']) for name, config in self.channels.items()}
            }
            
        except Exception as e:
            self.logger.error(f"Notification statistics failed: {e}")
            return {}
            
    def set_notification_preferences(self, preferences):
        """Update notification preferences"""
        try:
            for notification_type, config in preferences.items():
                if notification_type in self.notification_types:
                    # Update channels
                    if 'channels' in config:
                        self.notification_types[notification_type]['channels'] = config['channels']
                        
                    # Update priority
                    if 'priority' in config:
                        self.notification_types[notification_type]['priority'] = config['priority']
                        
                    # Update cooldown
                    if 'cooldown' in config:
                        self.notification_types[notification_type]['cooldown'] = config['cooldown']
                        
                    # Update sound setting
                    if 'sound' in config:
                        self.notification_types[notification_type]['sound'] = config['sound']
                        
            self.logger.info("‚öôÔ∏è Notification preferences updated")
            return True
            
        except Exception as e:
            self.logger.error(f"Notification preferences update failed: {e}")
            return False
            
    def enable_channel(self, channel_name):
        """Enable specific notification channel"""
        try:
            if channel_name in self.channels:
                if self.test_channel_connection(channel_name):
                    self.channels[channel_name]['enabled'] = True
                    self.logger.info(f"‚úÖ {channel_name.title()} notifications enabled")
                    return True
                else:
                    self.logger.error(f"‚ùå Failed to enable {channel_name} - connection test failed")
                    return False
            else:
                self.logger.error(f"Unknown channel: {channel_name}")
                return False
                
        except Exception as e:
            self.logger.error(f"Channel enable failed: {e}")
            return False
            
    def disable_channel(self, channel_name):
        """Disable specific notification channel"""
        try:
            if channel_name in self.channels:
                self.channels[channel_name]['enabled'] = False
                self.logger.info(f"‚ùå {channel_name.title()} notifications disabled")
                return True
            else:
                self.logger.error(f"Unknown channel: {channel_name}")
                return False
                
        except Exception as e:
            self.logger.error(f"Channel disable failed: {e}")
            return False
            
    def test_all_channels(self):
        """Test all configured notification channels"""
        try:
            results = {}
            
            for channel_name, channel_config in self.channels.items():
                if channel_config['enabled']:
                    test_result = self.test_channel_connection(channel_name)
                    results[channel_name] = {
                        'status': 'success' if test_result else 'failed',
                        'enabled': channel_config['enabled']
                    }
                else:
                    results[channel_name] = {
                        'status': 'disabled',
                        'enabled': False
                    }
                    
            return results
            
        except Exception as e:
            self.logger.error(f"Channel testing failed: {e}")
            return {}
            
    def send_test_notification(self, channel_name=None):
        """Send test notification to verify system"""
        try:
            test_data = {
                'balance': f"${self.performance_analytics.live_metrics['current_balance']:.2f}",
                'timestamp': datetime.now().strftime('%H:%M:%S'),
                'test_status': 'SUCCESS'
            }
            
            if channel_name:
                # Test specific channel
                if channel_name in self.channels and self.channels[channel_name]['enabled']:
                    message = self.format_test_message(test_data)
                    return self.send_to_channel(channel_name, message, {
                        'priority': 'medium',
                        'sound': False
                    })
                else:
                    return False
            else:
                # Test all channels
                return self.send_notification('system_test', test_data)
                
        except Exception as e:
            self.logger.error(f"Test notification failed: {e}")
            return False
            
    def format_test_message(self, data):
        """Format test message"""
        try:
            current_tier = self.performance_analytics.live_metrics.get('current_tier', 'psycho')
            tier_style = self.tier_styles.get(current_tier, self.tier_styles['psycho'])
            
            message = f"{tier_style['prefix']}\n\n"
            message += "üß™ NOTIFICATION SYSTEM TEST\n"
            message += f"üí∞ Current Balance: {data['balance']}\n"
            message += f"‚è∞ Time: {data['timestamp']}\n"
            message += f"‚úÖ Status: {data['test_status']}\n\n"
            message += "All systems operational! üöÄ"
            
            return message
            
        except Exception as e:
            self.logger.error(f"Test message formatting failed: {e}")
            return "Test message formatting error"
            
    def schedule_notification(self, notification_type, data, delay_seconds):
        """Schedule notification to be sent after delay"""
        try:
            def delayed_notification():
                time.sleep(delay_seconds)
                self.send_notification(notification_type, data)
                
            threading.Thread(target=delayed_notification, daemon=True).start()
            
            self.logger.info(f"üìÖ Scheduled notification: {notification_type} in {delay_seconds}s")
            return True
            
        except Exception as e:
            self.logger.error(f"Notification scheduling failed: {e}")
            return False
            
    def create_custom_notification(self, title, message, priority='medium', channels=None):
        """Create and send custom notification"""
        try:
            if channels is None:
                channels = ['telegram']
                
            # Create temporary notification type
            custom_type = f"custom_{int(time.time())}"
            
            self.notification_types[custom_type] = {
                'priority': priority,
                'cooldown': 0,
                'channels': channels,
                'template': message,
                'sound': priority in ['high', 'critical'],
                'emoji': 'üì¢'
            }
            
            # Send the notification
            result = self.send_notification(custom_type, {})
            
            # Clean up temporary type
            del self.notification_types[custom_type]
            
            return result
            
        except Exception as e:
            self.logger.error(f"Custom notification failed: {e}")
            return False
            
    def monitor_trading_events(self):
        """Monitor trading events and send appropriate notifications"""
        try:
            # This would be called by other systems when events occur
            metrics = self.performance_analytics.live_metrics
            
            # Check daily target
            if metrics.get('target_hit') and not self.has_recent_notification('daily_target_hit'):
                self.notify_daily_target_hit()
                
            # Check momentum
            momentum_score = metrics.get('momentum_score', 0)
            if momentum_score > 85 and not self.has_recent_notification('high_momentum'):
                self.notify_high_momentum(momentum_score)
                
            # Check risk
            risk_score = metrics.get('risk_score', 0)
            if risk_score > 75 and not self.has_recent_notification('risk_warning'):
                self.notify_risk_warning(risk_score)
                
        except Exception as e:
            self.logger.error(f"Trading event monitoring failed: {e}")
            
    def has_recent_notification(self, notification_type, minutes=30):
        """Check if notification type was sent recently"""
        try:
            last_sent = self.notification_types.get(notification_type, {}).get('last_sent', 0)
            return time.time() - last_sent < (minutes * 60)
            
        except Exception as e:
            self.logger.error(f"Recent notification check failed: {e}")
            return False
            
    def get_channel_health(self):
        """Get health status of all notification channels"""
        try:
            health_status = {}
            
            for channel_name, channel_config in self.channels.items():
                if channel_config['enabled']:
                    # Check queue size
                    queue_size = len(channel_config['message_queue'])
                    
                    # Check recent success rate
                    sent = self.stats['channel_stats'][channel_name]['sent']
                    failed = self.stats['channel_stats'][channel_name]['failed']
                    
                    if sent + failed > 0:
                        success_rate = (sent / (sent + failed)) * 100
                    else:
                        success_rate = 100
                        
                    # Determine health status
                    if queue_size > 10:
                        status = 'overloaded'
                    elif success_rate < 80:
                        status = 'degraded'
                    elif success_rate < 95:
                        status = 'warning'
                    else:
                        status = 'healthy'
                        
                    health_status[channel_name] = {
                        'status': status,
                        'queue_size': queue_size,
                        'success_rate': f"{success_rate:.1f}%",
                        'enabled': True
                    }
                else:
                    health_status[channel_name] = {
                        'status': 'disabled',
                        'queue_size': 0,
                        'success_rate': 'N/A',
                        'enabled': False
                    }
                    
            return health_status
            
        except Exception as e:
            self.logger.error(f"Channel health check failed: {e}")
            return {}
            
    def export_notification_log(self, hours=24):
        """Export notification log for analysis"""
        try:
            # This would compile a log of all notifications sent
            log_data = {
                'export_time': time.time(),
                'export_period_hours': hours,
                'statistics': self.get_notification_statistics(),
                'channel_health': self.get_channel_health(),
                'configuration': {
                    'notification_types': {
                        ntype: {
                            'priority': config['priority'],
                            'channels': config['channels'],
                            'cooldown': config['cooldown']
                        }
                        for ntype, config in self.notification_types.items()
                    },
                    'active_channels': [name for name, config in self.channels.items() if config['enabled']]
                }
            }
            
            return log_data
            
        except Exception as e:
            self.logger.error(f"Notification log export failed: {e}")
            return {}
            
    def update_channel_config(self, channel_name, new_config):
        """Update configuration for specific channel"""
        try:
            if channel_name not in self.channels:
                return False
                
            # Update configuration
            for key, value in new_config.items():
                if key in self.channels[channel_name]:
                    self.channels[channel_name][key] = value
                    
            # Test connection if enabled
            if self.channels[channel_name]['enabled']:
                if not self.test_channel_connection(channel_name):
                    self.logger.warning(f"‚ö†Ô∏è {channel_name} config updated but connection test failed")
                    return False
                    
            self.logger.info(f"‚öôÔ∏è {channel_name.title()} configuration updated")
            return True
            
        except Exception as e:
            self.logger.error(f"Channel config update failed: {e}")
            return False
            
    def clear_notification_queues(self):
        """Clear all pending notification queues"""
        try:
            cleared_count = 0
            
            for channel_name, channel in self.channels.items():
                queue_size = len(channel['message_queue'])
                channel['message_queue'].clear()
                cleared_count += queue_size
                
            self.logger.info(f"üßπ Cleared {cleared_count} pending notifications")
            return cleared_count
            
        except Exception as e:
            self.logger.error(f"Queue clearing failed: {e}")
            return 0
            
    def set_emergency_mode(self, enabled):
        """Enable/disable emergency notification mode"""
        try:
            if enabled:
                # Emergency mode: only critical notifications, all channels
                for ntype, config in self.notification_types.items():
                    if config['priority'] == 'critical':
                        config['channels'] = list(self.channels.keys())
                        config['cooldown'] = 0
                        
                self.logger.warning("üö® Emergency notification mode ENABLED")
            else:
                # Restore normal notification settings
                # This would restore from saved configuration
                self.logger.info("‚úÖ Emergency notification mode DISABLED")
                
            return True
            
        except Exception as e:
            self.logger.error(f"Emergency mode toggle failed: {e}")
            return False
            
    def get_notification_summary(self):
        """Get comprehensive notification system summary"""
        try:
            return {
                'system_status': 'operational',
                'total_notifications_today': sum(self.stats['total_sent'].values()),
                'active_channels': len([c for c in self.channels.values() if c['enabled']]),
                'pending_notifications': sum(len(c['message_queue']) for c in self.channels.values()),
                'health_status': self.get_channel_health(),
                'recent_notifications': {
                    ntype: config.get('last_sent', 0) 
                    for ntype, config in self.notification_types.items()
                    if config.get('last_sent', 0) > time.time() - 3600  # Last hour
                },
                'system_uptime': time.time() - getattr(self, 'start_time', time.time()),
                'error_rate': sum(self.stats['failed_notifications'].values()) / max(1, sum(self.stats['total_sent'].values())) * 100
            }
            
        except Exception as e:
            self.logger.error(f"Notification summary failed: {e}")
            return {'system_status': 'error', 'error': str(e)}
            
    def cleanup_old_data(self):
        """Clean up old notification data and statistics"""
        try:
            current_time = time.time()
            cutoff_time = current_time - (7 * 24 * 3600)  # 7 days
            
            # Clean up last_sent timestamps for each channel
            for channel in self.channels.values():
                channel['last_sent'] = {
                    k: v for k, v in channel['last_sent'].items() 
                    if v > cutoff_time
                }
                
            # Reset daily statistics if it's a new day
            if hasattr(self, 'last_cleanup_day'):
                current_day = datetime.now().day
                if current_day != self.last_cleanup_day:
                    # Reset daily stats
                    for stat_dict in [self.stats['total_sent'], self.stats['priority_stats']]:
                        stat_dict.clear()
                        
            self.last_cleanup_day = datetime.now().day
            
            self.logger.info("üßπ Notification data cleanup completed")
            
        except Exception as e:
            self.logger.error(f"Notification cleanup failed: {e}")
            
    def force_send_notification(self, notification_type, data, bypass_cooldown=True):
        """Force send notification bypassing normal restrictions"""
        try:
            if notification_type not in self.notification_types:
                return False
                
            # Temporarily bypass cooldown if requested
            original_cooldown = None
            if bypass_cooldown:
                original_cooldown = self.notification_types[notification_type]['cooldown']
                self.notification_types[notification_type]['cooldown'] = 0
                
            # Send the notification
            result = self.send_notification(notification_type, data)
            
            # Restore original cooldown
            if original_cooldown is not None:
                self.notification_types[notification_type]['cooldown'] = original_cooldown
                
            if result:
                self.logger.info(f"üöÄ Force sent notification: {notification_type}")
            
            return result
            
        except Exception as e:
            self.logger.error(f"Force notification failed: {e}")
            return False

class BacktestingEngine:
    """Advanced backtesting engine for comprehensive strategy validation and optimization"""
    
    def __init__(self, config, binance_client, memory_manager, performance_analytics):
        self.config = config
        self.binance_client = binance_client
        self.memory_manager = memory_manager
        self.performance_analytics = performance_analytics
        self.logger = logging.getLogger('MonsterBot.Backtesting')
        
        # Backtesting configuration
        self.backtest_config = {
            'default_timeframes': ['1m', '5m', '15m', '1h', '4h', '1d'],
            'default_symbols': ['BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'ADAUSDT', 'SOLUSDT', 'AVAXUSDT'],
            'max_historical_days': 365,
            'slippage_model': 'realistic',  # conservative, realistic, aggressive
            'commission_rate': 0.001,  # 0.1%
            'latency_simulation': True,
            'market_impact': True
        }
        
        # Strategy testing frameworks
        self.strategy_frameworks = {
            'momentum': {
                'parameters': {
                    'momentum_period': [10, 20, 50],
                    'momentum_threshold': [0.02, 0.05, 0.1],
                    'exit_threshold': [0.015, 0.03, 0.06],
                    'max_hold_time': [300, 900, 1800]  # seconds
                },
                'optimization_target': 'sharpe_ratio'
            },
            'mean_reversion': {
                'parameters': {
                    'lookback_period': [20, 50, 100],
                    'std_threshold': [1.5, 2.0, 2.5],
                    'reversion_period': [5, 10, 20],
                    'stop_loss': [0.02, 0.03, 0.05]
                },
                'optimization_target': 'profit_factor'
            },
            'breakout': {
                'parameters': {
                    'breakout_period': [10, 20, 30],
                    'volume_threshold': [1.5, 2.0, 3.0],
                    'confirmation_candles': [1, 2, 3],
                    'target_multiplier': [2, 3, 4]
                },
                'optimization_target': 'total_return'
            },
            'scalping': {
                'parameters': {
                    'tick_threshold': [0.001, 0.002, 0.005],
                    'volume_window': [10, 20, 50],
                    'quick_exit_time': [30, 60, 120],
                    'profit_target': [0.003, 0.005, 0.01]
                },
                'optimization_target': 'win_rate'
            },
            'arbitrage': {
                'parameters': {
                    'min_spread': [0.001, 0.003, 0.005],
                    'execution_delay': [0.1, 0.2, 0.5],
                    'position_size': [0.1, 0.25, 0.5],
                    'funding_threshold': [0.0001, 0.0005, 0.001]
                },
                'optimization_target': 'consistency'
            }
        }
        
        # Capital tier testing scenarios
        self.tier_scenarios = {
            'psycho': {
                'starting_balance': 500,
                'target_daily_return': 0.30,
                'max_leverage': 50,
                'max_position_size': 1.0,
                'risk_tolerance': 'extreme'
            },
            'hunter': {
                'starting_balance': 5000,
                'target_daily_return': 0.25,
                'max_leverage': 25,
                'max_position_size': 0.6,
                'risk_tolerance': 'high'
            },
            'accumulator': {
                'starting_balance': 50000,
                'target_daily_return': 0.20,
                'max_leverage': 15,
                'max_position_size': 0.4,
                'risk_tolerance': 'moderate'
            },
            'whale': {
                'starting_balance': 500000,
                'target_daily_return': 0.15,
                'max_leverage': 10,
                'max_position_size': 0.25,
                'risk_tolerance': 'conservative'
            }
        }
        
        # Backtesting results storage
        self.backtest_results = defaultdict(dict)
        self.optimization_results = defaultdict(dict)
        self.performance_metrics = defaultdict(dict)
        
        # Market condition simulation
        self.market_conditions = {
            'bull_market': {'trend': 1.2, 'volatility': 0.8, 'volume': 1.1},
            'bear_market': {'trend': 0.8, 'volatility': 1.3, 'volume': 0.9},
            'sideways': {'trend': 1.0, 'volatility': 0.6, 'volume': 0.8},
            'high_volatility': {'trend': 1.0, 'volatility': 2.0, 'volume': 1.5},
            'low_volume': {'trend': 1.0, 'volatility': 1.0, 'volume': 0.5}
        }
        
        # Simulation engine
        self.simulation_engine = {
            'order_book_simulation': True,
            'slippage_calculation': True,
            'latency_modeling': True,
            'market_impact_modeling': True,
            'funding_rate_simulation': True
        }
        
        # Initialize backtesting system
        self.initialize_backtesting_system()
        
    def initialize_backtesting_system(self):
        """Initialize backtesting system"""
        try:
            # Load historical data
            self.load_historical_data()
            
            # Initialize simulation models
            self.initialize_simulation_models()
            
            # Start optimization queue processor
            self.start_optimization_processor()
            
            self.logger.info("üìä Backtesting Engine initialized")
            self.logger.info(f"   üìà Strategies: {len(self.strategy_frameworks)}")
            self.logger.info(f"   üí∞ Tier scenarios: {len(self.tier_scenarios)}")
            self.logger.info(f"   üéØ Market conditions: {len(self.market_conditions)}")
            
        except Exception as e:
            self.logger.error(f"Backtesting system initialization failed: {e}")
            
    def load_historical_data(self):
        """Load historical market data for backtesting"""
        try:
            self.historical_data = defaultdict(dict)
            
            symbols = self.backtest_config['default_symbols']
            timeframes = self.backtest_config['default_timeframes']
            
            for symbol in symbols:
                for timeframe in timeframes:
                    # Load from memory manager or fetch fresh data
                    data = self.memory_manager.get_historical_data(symbol, timeframe)
                    
                    if not data:
                        # Fetch fresh data
                        data = self.fetch_historical_data(symbol, timeframe)
                        
                        if data:
                            self.memory_manager.store_historical_data(symbol, timeframe, data)
                            
                    if data:
                        self.historical_data[symbol][timeframe] = data
                        
            total_datasets = sum(len(timeframes) for timeframes in self.historical_data.values())
            self.logger.info(f"üìö Loaded {total_datasets} historical datasets")
            
        except Exception as e:
            self.logger.error(f"Historical data loading failed: {e}")
            
    def fetch_historical_data(self, symbol, timeframe, days=90):
        """Fetch historical data from Binance"""
        try:
            # Calculate limit based on timeframe
            timeframe_minutes = {
                '1m': 1, '5m': 5, '15m': 15, '1h': 60, '4h': 240, '1d': 1440
            }
            
            minutes_per_day = 1440
            limit = min(1000, (days * minutes_per_day) // timeframe_minutes.get(timeframe, 60))
            
            klines = self.binance_client.get_klines(symbol, timeframe, limit)
            
            if klines:
                # Convert to structured format
                structured_data = []
                
                for kline in klines:
                    structured_data.append({
                        'timestamp': int(kline[0]),
                        'open': float(kline[1]),
                        'high': float(kline[2]),
                        'low': float(kline[3]),
                        'close': float(kline[4]),
                        'volume': float(kline[5]),
                        'quote_volume': float(kline[7]),
                        'trades': int(kline[8])
                    })
                    
                return structured_data
                
            return None
            
        except Exception as e:
            self.logger.error(f"Historical data fetch failed for {symbol} {timeframe}: {e}")
            return None
            
    def initialize_simulation_models(self):
        """Initialize realistic market simulation models"""
        try:
            # Slippage model
            self.slippage_model = {
                'conservative': lambda size, volatility: min(0.001, size * 0.0001 + volatility * 0.0005),
                'realistic': lambda size, volatility: min(0.002, size * 0.0002 + volatility * 0.001),
                'aggressive': lambda size, volatility: min(0.005, size * 0.0005 + volatility * 0.002)
            }
            
            # Latency model (milliseconds)
            self.latency_model = {
                'order_placement': lambda: np.random.normal(50, 15),
                'market_data': lambda: np.random.normal(20, 5),
                'execution_confirmation': lambda: np.random.normal(30, 10)
            }
            
            # Market impact model
            self.market_impact_model = lambda order_size, avg_volume: min(0.001, (order_size / avg_volume) * 0.01)
            
            self.logger.info("üéØ Simulation models initialized")
            
        except Exception as e:
            self.logger.error(f"Simulation models initialization failed: {e}")
            
    def start_optimization_processor(self):
        """Start background optimization processor"""
        def optimization_processor():
            while True:
                try:
                    # Process any pending optimization jobs
                    self.process_optimization_queue()
                    time.sleep(60)  # Check every minute
                except Exception as e:
                    self.logger.error(f"Optimization processor error: {e}")
                    time.sleep(300)
                    
        threading.Thread(target=optimization_processor, daemon=True).start()
        
    def run_comprehensive_backtest(self, strategy_name, symbol='BTCUSDT', timeframe='5m', days=30):
        """Run comprehensive backtest for a strategy"""
        try:
            self.logger.info(f"üöÄ Starting comprehensive backtest: {strategy_name} on {symbol} {timeframe}")
            
            # Get historical data
            if symbol not in self.historical_data or timeframe not in self.historical_data[symbol]:
                self.logger.error(f"No historical data for {symbol} {timeframe}")
                return None
                
            data = self.historical_data[symbol][timeframe]
            
            # Limit data to requested days
            cutoff_timestamp = time.time() - (days * 24 * 3600)
            data = [d for d in data if d['timestamp'] >= cutoff_timestamp * 1000]
            
            if len(data) < 100:
                self.logger.error(f"Insufficient data for backtest: {len(data)} candles")
                return None
                
            # Run backtest for each tier
            tier_results = {}
            
            for tier_name, tier_config in self.tier_scenarios.items():
                self.logger.info(f"   üìä Testing {tier_name} tier scenario...")
                
                tier_result = self.run_tier_backtest(
                    strategy_name, data, tier_config, symbol, timeframe
                )
                
                if tier_result:
                    tier_results[tier_name] = tier_result
                    
            # Aggregate results
            backtest_summary = self.aggregate_backtest_results(tier_results, strategy_name, symbol, timeframe)
            
            # Store results
            result_key = f"{strategy_name}_{symbol}_{timeframe}_{days}d"
            self.backtest_results[result_key] = backtest_summary
            
            self.logger.info(f"‚úÖ Comprehensive backtest completed: {strategy_name}")
            self.logger.info(f"   üìà Best tier: {backtest_summary.get('best_tier', 'N/A')}")
            self.logger.info(f"   üí∞ Max return: {backtest_summary.get('max_daily_return', 0):.1%}")
            
            return backtest_summary
            
        except Exception as e:
            self.logger.error(f"Comprehensive backtest failed: {e}")
            return None
            
    def run_tier_backtest(self, strategy_name, data, tier_config, symbol, timeframe):
        """Run backtest for specific capital tier"""
        try:
            # Initialize simulation state
            state = {
                'balance': tier_config['starting_balance'],
                'initial_balance': tier_config['starting_balance'],
                'positions': [],
                'trades': [],
                'equity_curve': [],
                'daily_returns': [],
                'max_drawdown': 0,
                'peak_balance': tier_config['starting_balance']
            }
            
            # Strategy parameters
            strategy_params = self.get_default_strategy_params(strategy_name)
            
            # Process each candle
            for i, candle in enumerate(data[1:], 1):  # Skip first candle
                prev_candle = data[i-1]
                
                # Generate trading signal
                signal = self.generate_strategy_signal(
                    strategy_name, data[:i+1], strategy_params, tier_config
                )
                
                # Execute signal
                if signal:
                    trade_result = self.execute_simulated_trade(
                        signal, candle, state, tier_config, symbol
                    )
                    
                    if trade_result:
                        state['trades'].append(trade_result)
                        
                # Update positions
                self.update_simulated_positions(state, candle, tier_config)
                
                # Update equity curve
                current_equity = self.calculate_current_equity(state, candle)
                state['equity_curve'].append({
                    'timestamp': candle['timestamp'],
                    'equity': current_equity,
                    'balance': state['balance']
                })
                
                # Update peak and drawdown
                if current_equity > state['peak_balance']:
                    state['peak_balance'] = current_equity
                    
                current_drawdown = (state['peak_balance'] - current_equity) / state['peak_balance']
                state['max_drawdown'] = max(state['max_drawdown'], current_drawdown)
                
            # Calculate performance metrics
            performance = self.calculate_backtest_performance(state, tier_config)
            
            return {
                'tier': tier_config,
                'performance': performance,
                'trades': state['trades'],
                'equity_curve': state['equity_curve'][-100:],  # Last 100 points
                'summary': {
                    'total_trades': len(state['trades']),
                    'total_return': performance.get('total_return', 0),
                    'daily_return': performance.get('avg_daily_return', 0),
                    'max_drawdown': state['max_drawdown'],
                    'sharpe_ratio': performance.get('sharpe_ratio', 0),
                    'win_rate': performance.get('win_rate', 0)
                }
            }
            
        except Exception as e:
            self.logger.error(f"Tier backtest failed: {e}")
            return None
            
    def get_default_strategy_params(self, strategy_name):
        """Get default parameters for strategy"""
        try:
            if strategy_name in self.strategy_frameworks:
                params = {}
                for param, values in self.strategy_frameworks[strategy_name]['parameters'].items():
                    # Use middle value as default
                    params[param] = values[len(values) // 2]
                return params
            else:
                # Generic parameters
                return {
                    'lookback_period': 20,
                    'threshold': 0.02,
                    'stop_loss': 0.03,
                    'take_profit': 0.06
                }
                
        except Exception as e:
            self.logger.error(f"Default strategy params failed: {e}")
            return {}
            
    def generate_strategy_signal(self, strategy_name, data, params, tier_config):
        """Generate trading signal based on strategy"""
        try:
            if len(data) < params.get('lookback_period', 20):
                return None
                
            current_candle = data[-1]
            
            if strategy_name == 'momentum':
                return self.generate_momentum_signal(data, params, tier_config)
            elif strategy_name == 'mean_reversion':
                return self.generate_mean_reversion_signal(data, params, tier_config)
            elif strategy_name == 'breakout':
                return self.generate_breakout_signal(data, params, tier_config)
            elif strategy_name == 'scalping':
                return self.generate_scalping_signal(data, params, tier_config)
            elif strategy_name == 'arbitrage':
                return self.generate_arbitrage_signal(data, params, tier_config)
            else:
                # Generic momentum signal
                return self.generate_momentum_signal(data, params, tier_config)
                
        except Exception as e:
            self.logger.error(f"Signal generation failed: {e}")
            return None
            
    def generate_momentum_signal(self, data, params, tier_config):
        """Generate momentum trading signal"""
        try:
            lookback = params.get('momentum_period', 20)
            threshold = params.get('momentum_threshold', 0.02)
            
            if len(data) < lookback + 1:
                return None
                
            # Calculate momentum
            current_price = data[-1]['close']
            past_price = data[-lookback-1]['close']
            momentum = (current_price - past_price) / past_price
            
            # Check volume confirmation
            recent_volume = np.mean([d['volume'] for d in data[-5:]])
            avg_volume = np.mean([d['volume'] for d in data[-lookback:]])
            volume_ratio = recent_volume / avg_volume if avg_volume > 0 else 1
            
            # Generate signal
            if momentum > threshold and volume_ratio > 1.2:
                return {
                    'type': 'LONG',
                    'strategy': 'momentum',
                    'confidence': min(1.0, abs(momentum) / threshold),
                    'entry_price': current_price,
                    'stop_loss': current_price * (1 - params.get('exit_threshold', 0.03)),
                    'take_profit': current_price * (1 + params.get('exit_threshold', 0.03) * 2),
                    'position_size': self.calculate_position_size(tier_config, momentum)
                }
            elif momentum < -threshold and volume_ratio > 1.2:
                return {
                    'type': 'SHORT',
                    'strategy': 'momentum',
                    'confidence': min(1.0, abs(momentum) / threshold),
                    'entry_price': current_price,
                    'stop_loss': current_price * (1 + params.get('exit_threshold', 0.03)),
                    'take_profit': current_price * (1 - params.get('exit_threshold', 0.03) * 2),
                    'position_size': self.calculate_position_size(tier_config, abs(momentum))
                }
                
            return None
            
        except Exception as e:
            self.logger.error(f"Momentum signal generation failed: {e}")
            return None
            
    def generate_mean_reversion_signal(self, data, params, tier_config):
        """Generate mean reversion signal"""
        try:
            lookback = params.get('lookback_period', 50)
            std_threshold = params.get('std_threshold', 2.0)
            
            if len(data) < lookback:
                return None
                
            # Calculate mean and standard deviation
            prices = [d['close'] for d in data[-lookback:]]
            mean_price = np.mean(prices)
            std_price = np.std(prices)
            current_price = data[-1]['close']
            
            # Z-score calculation
            z_score = (current_price - mean_price) / std_price if std_price > 0 else 0
            
            # Generate reversion signal
            if z_score > std_threshold:
                # Price too high, expect reversion down
                return {
                    'type': 'SHORT',
                    'strategy': 'mean_reversion',
                    'confidence': min(1.0, abs(z_score) / std_threshold - 1),
                    'entry_price': current_price,
                    'stop_loss': current_price * 1.02,
                    'take_profit': mean_price,
                    'position_size': self.calculate_position_size(tier_config, abs(z_score) / std_threshold)
                }
            elif z_score < -std_threshold:
                # Price too low, expect reversion up
                return {
                    'type': 'LONG',
                    'strategy': 'mean_reversion',
                    'confidence': min(1.0, abs(z_score) / std_threshold - 1),
                    'entry_price': current_price,
                    'stop_loss': current_price * 0.98,
                    'take_profit': mean_price,
                    'position_size': self.calculate_position_size(tier_config, abs(z_score) / std_threshold)
                }
                
            return None
            
        except Exception as e:
            self.logger.error(f"Mean reversion signal generation failed: {e}")
            return None
            
    def generate_breakout_signal(self, data, params, tier_config):
        """Generate breakout trading signal"""
        try:
            lookback = params.get('breakout_period', 20)
            volume_threshold = params.get('volume_threshold', 2.0)
            
            if len(data) < lookback + 1:
                return None
                
            # Calculate resistance and support
            highs = [d['high'] for d in data[-lookback:]]
            lows = [d['low'] for d in data[-lookback:]]
            resistance = max(highs)
            support = min(lows)
            
            current_candle = data[-1]
            current_price = current_candle['close']
            current_volume = current_candle['volume']
            
            # Average volume
            avg_volume = np.mean([d['volume'] for d in data[-lookback:]])
            volume_ratio = current_volume / avg_volume if avg_volume > 0 else 1
            
            # Breakout detection
            if current_price > resistance and volume_ratio > volume_threshold:
                # Upward breakout
                target_distance = resistance - support
                return {
                    'type': 'LONG',
                    'strategy': 'breakout',
                    'confidence': min(1.0, volume_ratio / volume_threshold),
                    'entry_price': current_price,
                    'stop_loss': resistance * 0.99,  # Just below breakout level
                    'take_profit': current_price + (target_distance * params.get('target_multiplier', 2)),
                    'position_size': self.calculate_position_size(tier_config, volume_ratio / volume_threshold)
                }
            elif current_price < support and volume_ratio > volume_threshold:
                # Downward breakout
                target_distance = resistance - support
                return {
                    'type': 'SHORT',
                    'strategy': 'breakout',
                    'confidence': min(1.0, volume_ratio / volume_threshold),
                    'entry_price': current_price,
                    'stop_loss': support * 1.01,  # Just above breakdown level
                    'take_profit': current_price - (target_distance * params.get('target_multiplier', 2)),
                    'position_size': self.calculate_position_size(tier_config, volume_ratio / volume_threshold)
                }
                
            return None
            
        except Exception as e:
            self.logger.error(f"Breakout signal generation failed: {e}")
            return None
            
    def generate_scalping_signal(self, data, params, tier_config):
        """Generate scalping signal for quick trades"""
        try:
            if len(data) < 10:
                return None
                
            # Short-term price action analysis
            recent_candles = data[-5:]
            current_price = data[-1]['close']
            
            # Tick-based momentum
            price_changes = []
            for i in range(1, len(recent_candles)):
                change = (recent_candles[i]['close'] - recent_candles[i-1]['close']) / recent_candles[i-1]['close']
                price_changes.append(change)
                
            avg_change = np.mean(price_changes)
            tick_threshold = params.get('tick_threshold', 0.002)
            
            # Quick momentum signal
            if avg_change > tick_threshold:
                return {
                    'type': 'LONG',
                    'strategy': 'scalping',
                    'confidence': min(1.0, abs(avg_change) / tick_threshold),
                    'entry_price': current_price,
                    'stop_loss': current_price * 0.999,
                    'take_profit': current_price * (1 + params.get('profit_target', 0.005)),
                    'position_size': tier_config['max_position_size'],
                    'max_hold_time': params.get('quick_exit_time', 60)
                }
            elif avg_change < -tick_threshold:
                return {
                    'type': 'SHORT',
                    'strategy': 'scalping',
                    'confidence': min(1.0, abs(avg_change) / tick_threshold),
                    'entry_price': current_price,
                    'stop_loss': current_price * 1.001,
                    'take_profit': current_price * (1 - params.get('profit_target', 0.005)),
                    'position_size': tier_config['max_position_size'],
                    'max_hold_time': params.get('quick_exit_time', 60)
                }
                
            return None
            
        except Exception as e:
            self.logger.error(f"Scalping signal generation failed: {e}")
            return None
            
    def generate_arbitrage_signal(self, data, params, tier_config):
        """Generate arbitrage signal (simulated)"""
        try:
            # Simulate arbitrage opportunities
            if len(data) < 5:
                return None
                
            current_price = data[-1]['close']
            
            # Simulate spread opportunity
            simulated_spread = np.random.uniform(-0.01, 0.01)
            min_spread = params.get('min_spread', 0.003)
            
            if abs(simulated_spread) > min_spread:
                return {
                    'type': 'ARBITRAGE',
                    'strategy': 'arbitrage',
                    'confidence': min(1.0, abs(simulated_spread) / min_spread),
                    'entry_price': current_price,
                    'profit_target': abs(simulated_spread),
                    'position_size': params.get('position_size', 0.25),
                    'expected_duration': 300  # 5 minutes
                }
                
            return None
            
        except Exception as e:
            self.logger.error(f"Arbitrage signal generation failed: {e}")
            return None
            
    def calculate_position_size(self, tier_config, confidence_factor):
        """Calculate position size based on tier and confidence"""
        try:
            base_size = tier_config['max_position_size']
            risk_multiplier = {
                'extreme': 1.0,
                'high': 0.8,
                'moderate': 0.6,
                'conservative': 0.4
            }.get(tier_config['risk_tolerance'], 0.6)
            
            # Adjust by confidence
            adjusted_size = base_size * risk_multiplier * confidence_factor
            
            return min(base_size, max(0.1, adjusted_size))
            
        except Exception as e:
            self.logger.error(f"Position size calculation failed: {e}")
            return 0.1
            
    def execute_simulated_trade(self, signal, candle, state, tier_config, symbol):
        """Execute simulated trade with realistic conditions"""
        try:
            # Calculate slippage
            volatility = self.calculate_volatility(candle)
            position_size_usd = state['balance'] * signal['position_size']
            
            slippage_model = self.slippage_model[self.backtest_config['slippage_model']]
            slippage = slippage_model(position_size_usd / state['balance'], volatility)
            
            # Apply slippage to entry price
            if signal['type'] in ['LONG', 'BUY']:
                actual_entry_price = signal['entry_price'] * (1 + slippage)
            else:
                actual_entry_price = signal['entry_price'] * (1 - slippage)
                
            # Calculate fees
            fees = position_size_usd * self.backtest_config['commission_rate']
            
            # Check if we have enough balance
            if position_size_usd + fees > state['balance']:
                return None
                
            # Create trade record
            trade = {
                'timestamp': candle['timestamp'],
                'symbol': symbol,
                'strategy': signal['strategy'],
                'type': signal['type'],
                'entry_price': actual_entry_price,
                'position_size_usd': position_size_usd,
                'quantity': position_size_usd / actual_entry_price,
                'stop_loss': signal.get('stop_loss'),
                'take_profit': signal.get('take_profit'),
                'fees': fees,
                'confidence': signal.get('confidence', 1.0),
                'max_hold_time': signal.get('max_hold_time'),
                'status': 'open'
            }
            
            # Add to positions
            state['positions'].append(trade)
            
            # Update balance
            state['balance'] -= (position_size_usd + fees)
            
            return trade
            
        except Exception as e:
            self.logger.error(f"Simulated trade execution failed: {e}")
            return None
            
    def update_simulated_positions(self, state, candle, tier_config):
        """Update open positions and check for exits"""
        try:
            current_price = candle['close']
            current_time = candle['timestamp']
            
            positions_to_close = []
            
            for i, position in enumerate(state['positions']):
                if position['status'] != 'open':
                    continue
                    
                # Check stop loss
                if position.get('stop_loss'):
                    if position['type'] in ['LONG', 'BUY']:
                        if current_price <= position['stop_loss']:
                            positions_to_close.append((i, 'stop_loss', current_price))
                            continue
                    else:
                        if current_price >= position['stop_loss']:
                            positions_to_close.append((i, 'stop_loss', current_price))
                            continue
                            
                # Check take profit
                if position.get('take_profit'):
                    if position['type'] in ['LONG', 'BUY']:
                        if current_price >= position['take_profit']:
                            positions_to_close.append((i, 'take_profit', current_price))
                            continue
                    else:
                        if current_price <= position['take_profit']:
                            positions_to_close.append((i, 'take_profit', current_price))
                            continue
                            
                # Check max hold time
                if position.get('max_hold_time'):
                    hold_time = (current_time - position['timestamp']) / 1000  # Convert to seconds
                    if hold_time >= position['max_hold_time']:
                        positions_to_close.append((i, 'timeout', current_price))
                        continue
                        
            # Close positions
            for position_index, exit_reason, exit_price in reversed(positions_to_close):
                self.close_simulated_position(state, position_index, exit_price, exit_reason, candle)
                
        except Exception as e:
            self.logger.error(f"Position update failed: {e}")
            
    def close_simulated_position(self, state, position_index, exit_price, exit_reason, candle):
        """Close a simulated position"""
        try:
            position = state['positions'][position_index]
            
            # Calculate slippage for exit
            volatility = self.calculate_volatility(candle)
            slippage_model = self.slippage_model[self.backtest_config['slippage_model']]
            slippage = slippage_model(position['position_size_usd'] / state['balance'], volatility)
            
            # Apply slippage to exit price
            if position['type'] in ['LONG', 'BUY']:
                actual_exit_price = exit_price * (1 - slippage)
            else:
                actual_exit_price = exit_price * (1 + slippage)
                
            # Calculate PnL
            if position['type'] in ['LONG', 'BUY']:
                pnl = position['quantity'] * (actual_exit_price - position['entry_price'])
            else:
                pnl = position['quantity'] * (position['entry_price'] - actual_exit_price)
                
            # Calculate exit fees
            exit_fees = position['position_size_usd'] * self.backtest_config['commission_rate']
            
            # Net PnL after fees
            net_pnl = pnl - position['fees'] - exit_fees
            
            # Update position record
            position.update({
                'exit_price': actual_exit_price,
                'exit_timestamp': candle['timestamp'],
                'exit_reason': exit_reason,
                'pnl': net_pnl,
                'pnl_pct': net_pnl / position['position_size_usd'],
                'hold_time': (candle['timestamp'] - position['timestamp']) / 1000,
                'status': 'closed'
            })
            
            # Update balance
            if position['type'] in ['LONG', 'BUY']:
                state['balance'] += position['quantity'] * actual_exit_price - exit_fees
            else:
                # For short positions, return the collateral plus profit/loss
                state['balance'] += position['position_size_usd'] + net_pnl
                
            # Move to trades history
            state['trades'].append(position.copy())
            
            # Remove from active positions
            state['positions'].pop(position_index)
            
        except Exception as e:
            self.logger.error(f"Position closing failed: {e}")
            
    def calculate_volatility(self, candle):
        """Calculate current volatility for slippage modeling"""
        try:
            if candle['high'] == candle['low']:
                return 0.001  # Minimum volatility
                
            # True range as volatility proxy
            true_range = (candle['high'] - candle['low']) / candle['close']
            return min(0.1, true_range)  # Cap at 10%
            
        except Exception as e:
            self.logger.error(f"Volatility calculation failed: {e}")
            return 0.001
            
    def calculate_current_equity(self, state, candle):
        """Calculate current equity including open positions"""
        try:
            current_price = candle['close']
            equity = state['balance']
            
            # Add unrealized PnL from open positions
            for position in state['positions']:
                if position['status'] == 'open':
                    if position['type'] in ['LONG', 'BUY']:
                        unrealized_pnl = position['quantity'] * (current_price - position['entry_price'])
                    else:
                        unrealized_pnl = position['quantity'] * (position['entry_price'] - current_price)
                        
                    equity += unrealized_pnl
                    
            return equity
            
        except Exception as e:
            self.logger.error(f"Equity calculation failed: {e}")
            return state['balance']
            
    def calculate_backtest_performance(self, state, tier_config):
        """Calculate comprehensive performance metrics"""
        try:
            if not state['trades']:
                return {'total_return': 0, 'avg_daily_return': 0, 'sharpe_ratio': 0, 'win_rate': 0}
                
            # Basic metrics
            initial_balance = tier_config['starting_balance']
            final_equity = state['equity_curve'][-1]['equity'] if state['equity_curve'] else state['balance']
            total_return = (final_equity - initial_balance) / initial_balance
            
            # Trade analysis
            profitable_trades = [t for t in state['trades'] if t['pnl'] > 0]
            losing_trades = [t for t in state['trades'] if t['pnl'] < 0]
            
            win_rate = len(profitable_trades) / len(state['trades'])
            
            # Average win/loss
            avg_win = np.mean([t['pnl'] for t in profitable_trades]) if profitable_trades else 0
            avg_loss = np.mean([t['pnl'] for t in losing_trades]) if losing_trades else 0
            
            # Profit factor
            gross_profit = sum(t['pnl'] for t in profitable_trades)
            gross_loss = abs(sum(t['pnl'] for t in losing_trades))
            profit_factor = gross_profit / gross_loss if gross_loss > 0 else float('inf')
            
            # Daily returns for Sharpe calculation
            daily_returns = self.calculate_daily_returns(state['equity_curve'])
            
            # Sharpe ratio
            if len(daily_returns) > 1:
                mean_return = np.mean(daily_returns)
                std_return = np.std(daily_returns)
                sharpe_ratio = (mean_return * 365) / (std_return * np.sqrt(365)) if std_return > 0 else 0
            else:
                sharpe_ratio = 0
                
            # Maximum consecutive losses
            max_consecutive_losses = self.calculate_max_consecutive_losses(state['trades'])
            
            # Average hold time
            hold_times = [t.get('hold_time', 0) for t in state['trades'] if t.get('hold_time')]
            avg_hold_time = np.mean(hold_times) if hold_times else 0
            
            # Calculate daily return rate
            total_days = len(daily_returns) if daily_returns else 1
            avg_daily_return = total_return / total_days if total_days > 0 else 0
            
            # Target achievement analysis
            target_daily_return = tier_config['target_daily_return']
            days_above_target = sum(1 for r in daily_returns if r >= target_daily_return)
            target_hit_rate = days_above_target / len(daily_returns) if daily_returns else 0
            
            return {
                'total_return': total_return,
                'avg_daily_return': avg_daily_return,
                'sharpe_ratio': sharpe_ratio,
                'win_rate': win_rate,
                'profit_factor': profit_factor,
                'avg_win': avg_win,
                'avg_loss': avg_loss,
                'max_drawdown': state['max_drawdown'],
                'max_consecutive_losses': max_consecutive_losses,
                'avg_hold_time': avg_hold_time,
                'total_trades': len(state['trades']),
                'target_hit_rate': target_hit_rate,
                'final_balance': final_equity,
                'total_fees_paid': sum(t.get('fees', 0) for t in state['trades']),
                'best_trade': max(state['trades'], key=lambda x: x['pnl'])['pnl'] if state['trades'] else 0,
                'worst_trade': min(state['trades'], key=lambda x: x['pnl'])['pnl'] if state['trades'] else 0
            }
            
        except Exception as e:
            self.logger.error(f"Performance calculation failed: {e}")
            return {}
            
    def calculate_daily_returns(self, equity_curve):
        """Calculate daily returns from equity curve"""
        try:
            if len(equity_curve) < 2:
                return []
                
            daily_returns = []
            
            # Group by day
            daily_equity = defaultdict(list)
            
            for point in equity_curve:
                day = datetime.fromtimestamp(point['timestamp'] / 1000).strftime('%Y-%m-%d')
                daily_equity[day].append(point['equity'])
                
            # Calculate daily returns
            sorted_days = sorted(daily_equity.keys())
            
            for i in range(1, len(sorted_days)):
                prev_day_end = daily_equity[sorted_days[i-1]][-1]
                current_day_end = daily_equity[sorted_days[i]][-1]
                
                daily_return = (current_day_end - prev_day_end) / prev_day_end
                daily_returns.append(daily_return)
                
            return daily_returns
            
        except Exception as e:
            self.logger.error(f"Daily returns calculation failed: {e}")
            return []
            
    def calculate_max_consecutive_losses(self, trades):
        """Calculate maximum consecutive losing trades"""
        try:
            max_consecutive = 0
            current_consecutive = 0
            
            for trade in trades:
                if trade['pnl'] < 0:
                    current_consecutive += 1
                    max_consecutive = max(max_consecutive, current_consecutive)
                else:
                    current_consecutive = 0
                    
            return max_consecutive
            
        except Exception as e:
            self.logger.error(f"Consecutive losses calculation failed: {e}")
            return 0
            
    def aggregate_backtest_results(self, tier_results, strategy_name, symbol, timeframe):
        """Aggregate results across all tiers"""
        try:
            if not tier_results:
                return None
                
            # Find best performing tier
            best_tier = max(tier_results.keys(), 
                          key=lambda x: tier_results[x]['performance'].get('total_return', 0))
            
            # Aggregate statistics
            total_trades = sum(r['summary']['total_trades'] for r in tier_results.values())
            avg_sharpe = np.mean([r['performance']['sharpe_ratio'] for r in tier_results.values()])
            avg_win_rate = np.mean([r['performance']['win_rate'] for r in tier_results.values()])
            
            # Best and worst performances
            max_daily_return = max(r['performance']['avg_daily_return'] for r in tier_results.values())
            max_drawdown = max(r['performance']['max_drawdown'] for r in tier_results.values())
            
            # Target achievement analysis
            target_achievements = {}
            for tier, results in tier_results.items():
                target_achievements[tier] = results['performance'].get('target_hit_rate', 0)
                
            return {
                'strategy': strategy_name,
                'symbol': symbol,
                'timeframe': timeframe,
                'timestamp': time.time(),
                'best_tier': best_tier,
                'tier_results': tier_results,
                'aggregate_stats': {
                    'total_trades': total_trades,
                    'avg_sharpe_ratio': avg_sharpe,
                    'avg_win_rate': avg_win_rate,
                    'max_daily_return': max_daily_return,
                    'max_drawdown': max_drawdown,
                    'target_achievements': target_achievements
                },
                'recommendations': self.generate_strategy_recommendations(tier_results)
            }
            
        except Exception as e:
            self.logger.error(f"Result aggregation failed: {e}")
            return None
            
    def generate_strategy_recommendations(self, tier_results):
        """Generate recommendations based on backtest results"""
        try:
            recommendations = []
            
            # Find most profitable tier
            best_performance = max(tier_results.values(), 
                                 key=lambda x: x['performance'].get('total_return', 0))
            
            best_tier_name = None
            for tier, results in tier_results.items():
                if results == best_performance:
                    best_tier_name = tier
                    break
                    
            if best_tier_name:
                recommendations.append({
                    'type': 'tier_preference',
                    'message': f"Strategy performs best in {best_tier_name} tier",
                    'data': {'recommended_tier': best_tier_name}
                })
                
            # Risk recommendations
            for tier, results in tier_results.items():
                max_drawdown = results['performance'].get('max_drawdown', 0)
                
                if max_drawdown > 0.2:  # >20% drawdown
                    recommendations.append({
                        'type': 'risk_warning',
                        'message': f"High drawdown risk in {tier} tier: {max_drawdown:.1%}",
                        'data': {'tier': tier, 'drawdown': max_drawdown}
                    })
                    
            # Performance recommendations
            avg_daily_returns = [r['performance']['avg_daily_return'] for r in tier_results.values()]
            
            if max(avg_daily_returns) < 0.05:  # <5% average daily return
                recommendations.append({
                    'type': 'performance_warning',
                    'message': "Strategy shows low daily returns across all tiers",
                    'data': {'max_daily_return': max(avg_daily_returns)}
                })
            elif max(avg_daily_returns) > 0.30:  # >30% average daily return
                recommendations.append({
                    'type': 'performance_excellent',
                    'message': "Strategy shows exceptional daily returns",
                    'data': {'max_daily_return': max(avg_daily_returns)}
                })
                
            return recommendations
            
        except Exception as e:
            self.logger.error(f"Recommendation generation failed: {e}")
            return []
            
    def optimize_strategy(self, strategy_name, symbol='BTCUSDT', timeframe='5m', days=30):
        """Optimize strategy parameters using grid search"""
        try:
            self.logger.info(f"üîß Starting strategy optimization: {strategy_name}")
            
            if strategy_name not in self.strategy_frameworks:
                self.logger.error(f"Unknown strategy: {strategy_name}")
                return None
                
            framework = self.strategy_frameworks[strategy_name]
            parameters = framework['parameters']
            optimization_target = framework['optimization_target']
            
            # Generate parameter combinations
            param_combinations = self.generate_parameter_combinations(parameters)
            
            self.logger.info(f"   üéØ Testing {len(param_combinations)} parameter combinations")
            
            # Test each combination
            best_result = None
            best_score = -float('inf')
            
            for i, param_set in enumerate(param_combinations):
                if i % 10 == 0:
                    self.logger.info(f"   üìä Progress: {i}/{len(param_combinations)}")
                    
                # Run backtest with these parameters
                result = self.run_optimization_backtest(
                    strategy_name, param_set, symbol, timeframe, days
                )
                
                if result:
                    # Score based on optimization target
                    score = self.calculate_optimization_score(result, optimization_target)
                    
                    if score > best_score:
                        best_score = score
                        best_result = {
                            'parameters': param_set,
                            'score': score,
                            'results': result
                        }
                        
            if best_result:
                self.logger.info(f"‚úÖ Optimization completed: {strategy_name}")
                self.logger.info(f"   üèÜ Best score: {best_score:.4f}")
                self.logger.info(f"   ‚öôÔ∏è Best parameters: {best_result['parameters']}")
                
                # Store optimization result
                opt_key = f"{strategy_name}_{symbol}_{timeframe}_optimization"
                self.optimization_results[opt_key] = best_result
                
                return best_result
            else:
                self.logger.warning(f"‚ùå Optimization failed: {strategy_name}")
                return None
                
        except Exception as e:
            self.logger.error(f"Strategy optimization failed: {e}")
            return None
            
    def generate_parameter_combinations(self, parameters):
        """Generate all parameter combinations for grid search"""
        try:
            import itertools
            
            param_names = list(parameters.keys())
            param_values = list(parameters.values())
            
            # Generate all combinations
            combinations = list(itertools.product(*param_values))
            
            # Convert to dictionaries
            param_combinations = []
            for combo in combinations:
                param_dict = dict(zip(param_names, combo))
                param_combinations.append(param_dict)
                
            return param_combinations
            
        except Exception as e:
            self.logger.error(f"Parameter combination generation failed: {e}")
            return []
            
    def run_optimization_backtest(self, strategy_name, parameters, symbol, timeframe, days):
        """Run simplified backtest for optimization"""
        try:
            # Get historical data
            if symbol not in self.historical_data or timeframe not in self.historical_data[symbol]:
                return None
                
            data = self.historical_data[symbol][timeframe]
            
            # Limit data
            cutoff_timestamp = time.time() - (days * 24 * 3600)
            data = [d for d in data if d['timestamp'] >= cutoff_timestamp * 1000]
            
            if len(data) < 100:
                return None
                
            # Use hunter tier as baseline for optimization
            tier_config = self.tier_scenarios['hunter']
            
            # Run simplified backtest
            state = {
                'balance': tier_config['starting_balance'],
                'initial_balance': tier_config['starting_balance'],
                'positions': [],
                'trades': [],
                'max_drawdown': 0,
                'peak_balance': tier_config['starting_balance']
            }
            
            # Process data
            for i, candle in enumerate(data[1:], 1):
                # Generate signal with custom parameters
                signal = self.generate_strategy_signal(
                    strategy_name, data[:i+1], parameters, tier_config
                )
                
                if signal:
                    trade_result = self.execute_simulated_trade(
                        signal, candle, state, tier_config, symbol
                    )
                    
                    if trade_result:
                        state['trades'].append(trade_result)
                        
                # Update positions
                self.update_simulated_positions(state, candle, tier_config)
                
                # Update drawdown
                current_equity = self.calculate_current_equity(state, candle)
                if current_equity > state['peak_balance']:
                    state['peak_balance'] = current_equity
                    
                current_drawdown = (state['peak_balance'] - current_equity) / state['peak_balance']
                state['max_drawdown'] = max(state['max_drawdown'], current_drawdown)
                
            # Calculate performance
            performance = self.calculate_backtest_performance(state, tier_config)
            
            return performance
            
        except Exception as e:
            self.logger.error(f"Optimization backtest failed: {e}")
            return None
            
    def calculate_optimization_score(self, results, target):
        """Calculate optimization score based on target metric"""
        try:
            if target == 'sharpe_ratio':
                return results.get('sharpe_ratio', 0)
            elif target == 'total_return':
                return results.get('total_return', 0)
            elif target == 'profit_factor':
                return results.get('profit_factor', 1)
            elif target == 'win_rate':
                return results.get('win_rate', 0)
            elif target == 'consistency':
                # Combination of win rate and low drawdown
                win_rate = results.get('win_rate', 0)
                max_drawdown = results.get('max_drawdown', 1)
                return win_rate * (1 - max_drawdown)
            else:
                return results.get('total_return', 0)
                
        except Exception as e:
            self.logger.error(f"Optimization score calculation failed: {e}")
            return 0
            
    def run_market_condition_tests(self, strategy_name, symbol='BTCUSDT', timeframe='5m'):
        """Test strategy under different market conditions"""
        try:
            self.logger.info(f"üåä Testing market conditions: {strategy_name}")
            
            condition_results = {}
            
            for condition_name, condition_params in self.market_conditions.items():
                self.logger.info(f"   üìä Testing {condition_name} market...")
                
                # Modify historical data to simulate market condition
                modified_data = self.simulate_market_condition(
                    symbol, timeframe, condition_params
                )
                
                if modified_data:
                    # Run backtest with modified data
                    result = self.run_condition_backtest(
                        strategy_name, modified_data, condition_name
                    )
                    
                    if result:
                        condition_results[condition_name] = result
                        
            if condition_results:
                self.logger.info(f"‚úÖ Market condition tests completed: {strategy_name}")
                
                # Find best and worst conditions
                best_condition = max(condition_results.keys(),
                                   key=lambda x: condition_results[x]['performance']['total_return'])
                worst_condition = min(condition_results.keys(),
                                    key=lambda x: condition_results[x]['performance']['total_return'])
                
                return {
                    'strategy': strategy_name,
                    'condition_results': condition_results,
                    'best_condition': best_condition,
                    'worst_condition': worst_condition,
                    'condition_analysis': self.analyze_condition_performance(condition_results)
                }
            else:
                return None
                
        except Exception as e:
            self.logger.error(f"Market condition testing failed: {e}")
            return None
            
    def simulate_market_condition(self, symbol, timeframe, condition_params):
        """Simulate specific market condition by modifying historical data"""
        try:
            if symbol not in self.historical_data or timeframe not in self.historical_data[symbol]:
                return None
                
            original_data = self.historical_data[symbol][timeframe][-1000:]  # Last 1000 candles
            modified_data = []
            
            trend_factor = condition_params['trend']
            volatility_factor = condition_params['volatility']
            volume_factor = condition_params['volume']
            
            for i, candle in enumerate(original_data):
                # Apply trend
                if i > 0:
                    prev_close = modified_data[-1]['close']
                    trend_adjustment = (candle['close'] - original_data[i-1]['close']) * trend_factor
                    new_close = prev_close + trend_adjustment
                else:
                    new_close = candle['close']
                    
                # Apply volatility
                volatility_range = (candle['high'] - candle['low']) * volatility_factor
                new_high = new_close + (volatility_range * 0.6)
                new_low = new_close - (volatility_range * 0.4)
                
                # Ensure OHLC logic
                new_open = modified_data[-1]['close'] if i > 0 else candle['open']
                
                modified_candle = {
                    'timestamp': candle['timestamp'],
                    'open': new_open,
                    'high': max(new_open, new_close, new_high),
                    'low': min(new_open, new_close, new_low),
                    'close': new_close,
                    'volume': candle['volume'] * volume_factor,
                    'quote_volume': candle['quote_volume'] * volume_factor,
                    'trades': candle['trades']
                }
                
                modified_data.append(modified_candle)
                
            return modified_data
            
        except Exception as e:
            self.logger.error(f"Market condition simulation failed: {e}")
            return None
            
    def run_condition_backtest(self, strategy_name, data, condition_name):
        """Run backtest with specific market condition data"""
        try:
            # Use hunter tier for condition testing
            tier_config = self.tier_scenarios['hunter']
            
            state = {
                'balance': tier_config['starting_balance'],
                'initial_balance': tier_config['starting_balance'],
                'positions': [],
                'trades': [],
                'max_drawdown': 0,
                'peak_balance': tier_config['starting_balance']
            }
            
            # Default parameters
            strategy_params = self.get_default_strategy_params(strategy_name)
            
            # Process data
            for i, candle in enumerate(data[1:], 1):
                signal = self.generate_strategy_signal(
                    strategy_name, data[:i+1], strategy_params, tier_config
                )
                
                if signal:
                    trade_result = self.execute_simulated_trade(
                        signal, candle, state, tier_config, 'TESTUSDT'
                    )
                    
                    if trade_result:
                        state['trades'].append(trade_result)
                        
                self.update_simulated_positions(state, candle, tier_config)
                
                # Update drawdown
                current_equity = self.calculate_current_equity(state, candle)
                if current_equity > state['peak_balance']:
                    state['peak_balance'] = current_equity
                    
                current_drawdown = (state['peak_balance'] - current_equity) / state['peak_balance']
                state['max_drawdown'] = max(state['max_drawdown'], current_drawdown)
                
            # Calculate performance
            performance = self.calculate_backtest_performance(state, tier_config)
            
            return {
                'condition': condition_name,
                'performance': performance,
                'trade_count': len(state['trades'])
            }
            
        except Exception as e:
            self.logger.error(f"Condition backtest failed: {e}")
            return None
            
    def analyze_condition_performance(self, condition_results):
        """Analyze performance across different market conditions"""
        try:
            analysis = {
                'consistency_score': 0,
                'best_conditions': [],
                'worst_conditions': [],
                'adaptability_rating': 'moderate'
            }
            
            returns = [r['performance']['total_return'] for r in condition_results.values()]
            
            # Consistency (lower standard deviation = higher consistency)
            if len(returns) > 1:
                std_returns = np.std(returns)
                mean_returns = np.mean(returns)
                
                if mean_returns > 0:
                    analysis['consistency_score'] = mean_returns / (std_returns + 0.001)
                    
            # Best and worst conditions
            sorted_conditions = sorted(condition_results.items(),
                                     key=lambda x: x[1]['performance']['total_return'],
                                     reverse=True)
            
            analysis['best_conditions'] = [c[0] for c in sorted_conditions[:2]]
            analysis['worst_conditions'] = [c[0] for c in sorted_conditions[-2:]]
            
            # Adaptability rating
            positive_returns = sum(1 for r in returns if r > 0)
            adaptability_ratio = positive_returns / len(returns)
            
            if adaptability_ratio >= 0.8:
                analysis['adaptability_rating'] = 'excellent'
            elif adaptability_ratio >= 0.6:
                analysis['adaptability_rating'] = 'good'
            elif adaptability_ratio >= 0.4:
                analysis['adaptability_rating'] = 'moderate'
            else:
                analysis['adaptability_rating'] = 'poor'
                
            return analysis
            
        except Exception as e:
            self.logger.error(f"Condition performance analysis failed: {e}")
            return {}
            
    def process_optimization_queue(self):
        """Process pending optimization jobs"""
        try:
            # This would process queued optimization jobs
            # For now, just log that the processor is running
            pass
            
        except Exception as e:
            self.logger.error(f"Optimization queue processing failed: {e}")
            
    def get_backtest_summary(self):
        """Get comprehensive backtesting summary"""
        try:
            return {
                'total_backtests_run': len(self.backtest_results),
                'total_optimizations': len(self.optimization_results),
                'strategies_tested': list(set(
                    result['strategy'] for result in self.backtest_results.values()
                    if 'strategy' in result
                )),
                'best_performing_strategy': self.find_best_strategy(),
                'recent_results': list(self.backtest_results.values())[-5:],  # Last 5 results
                'optimization_summary': self.get_optimization_summary(),
                'data_coverage': {
                    'symbols': list(self.historical_data.keys()),
                    'timeframes': list(self.backtest_config['default_timeframes']),
                    'total_data_points': sum(
                        sum(len(tf_data) for tf_data in symbol_data.values())
                        for symbol_data in self.historical_data.values()
                    )
                }
            }
            
        except Exception as e:
            self.logger.error(f"Backtest summary generation failed: {e}")
            return {}
            
    def find_best_strategy(self):
        """Find the best performing strategy across all backtests"""
        try:
            if not self.backtest_results:
                return None
                
            best_strategy = None
            best_score = -float('inf')
            
            for result in self.backtest_results.values():
                if 'aggregate_stats' in result:
                    # Use max daily return as primary metric
                    score = result['aggregate_stats'].get('max_daily_return', 0)
                    
                    if score > best_score:
                        best_score = score
                        best_strategy = {
                            'strategy_name': result.get('strategy', 'Unknown'),
                            'symbol': result.get('symbol', 'Unknown'),
                            'timeframe': result.get('timeframe', 'Unknown'),
                            'max_daily_return': score,
                            'best_tier': result.get('best_tier', 'Unknown'),
                            'avg_win_rate': result['aggregate_stats'].get('avg_win_rate', 0)
                        }
                        
            return best_strategy
            
        except Exception as e:
            self.logger.error(f"Best strategy finding failed: {e}")
            return None
            
    def get_optimization_summary(self):
        """Get optimization results summary"""
        try:
            if not self.optimization_results:
                return {'total_optimizations': 0}
                
            return {
                'total_optimizations': len(self.optimization_results),
                'optimized_strategies': list(set(
                    key.split('_')[0] for key in self.optimization_results.keys()
                )),
                'best_optimization': self.find_best_optimization(),
                'avg_improvement': self.calculate_avg_optimization_improvement()
            }
            
        except Exception as e:
            self.logger.error(f"Optimization summary failed: {e}")
            return {}
            
    def find_best_optimization(self):
        """Find the best optimization result"""
        try:
            if not self.optimization_results:
                return None
                
            best_opt = max(self.optimization_results.values(), 
                          key=lambda x: x.get('score', 0))
            
            return {
                'score': best_opt.get('score', 0),
                'parameters': best_opt.get('parameters', {}),
                'improvement': best_opt.get('score', 0)  # vs baseline
            }
            
        except Exception as e:
            self.logger.error(f"Best optimization finding failed: {e}")
            return None
            
    def calculate_avg_optimization_improvement(self):
        """Calculate average improvement from optimization"""
        try:
            if not self.optimization_results:
                return 0
                
            improvements = [result.get('score', 0) for result in self.optimization_results.values()]
            return np.mean(improvements) if improvements else 0
            
        except Exception as e:
            self.logger.error(f"Average improvement calculation failed: {e}")
            return 0
            
    def export_backtest_results(self, filename=None):
        """Export backtest results to file"""
        try:
            if filename is None:
                filename = f"backtest_results_{int(time.time())}.json"
                
            export_data = {
                'export_timestamp': time.time(),
                'backtest_results': self.backtest_results,
                'optimization_results': self.optimization_results,
                'configuration': {
                    'backtest_config': self.backtest_config,
                    'strategy_frameworks': self.strategy_frameworks,
                    'tier_scenarios': self.tier_scenarios
                },
                'summary': self.get_backtest_summary()
            }
            
            # Store in memory manager
            self.memory_manager.export_backtest_data(export_data, filename)
            
            self.logger.info(f"üìÅ Backtest results exported: {filename}")
            return filename
            
        except Exception as e:
            self.logger.error(f"Backtest export failed: {e}")
            return None
            
    def import_backtest_results(self, filename):
        """Import backtest results from file"""
        try:
            import_data = self.memory_manager.import_backtest_data(filename)
            
            if import_data:
                self.backtest_results.update(import_data.get('backtest_results', {}))
                self.optimization_results.update(import_data.get('optimization_results', {}))
                
                self.logger.info(f"üìÅ Backtest results imported: {filename}")
                return True
            else:
                self.logger.error(f"Failed to import: {filename}")
                return False
                
        except Exception as e:
            self.logger.error(f"Backtest import failed: {e}")
            return False
            
    def run_strategy_comparison(self, strategies, symbol='BTCUSDT', timeframe='5m', days=30):
        """Compare multiple strategies head-to-head"""
        try:
            self.logger.info(f"‚öîÔ∏è Running strategy comparison: {', '.join(strategies)}")
            
            comparison_results = {}
            
            for strategy in strategies:
                self.logger.info(f"   üß™ Testing {strategy}...")
                
                result = self.run_comprehensive_backtest(strategy, symbol, timeframe, days)
                
                if result:
                    comparison_results[strategy] = result
                    
            if comparison_results:
                # Analyze comparison
                comparison_analysis = self.analyze_strategy_comparison(comparison_results)
                
                self.logger.info(f"‚úÖ Strategy comparison completed")
                self.logger.info(f"   üèÜ Winner: {comparison_analysis.get('winner', 'N/A')}")
                
                return {
                    'strategies_compared': strategies,
                    'symbol': symbol,
                    'timeframe': timeframe,
                    'days': days,
                    'results': comparison_results,
                    'analysis': comparison_analysis,
                    'timestamp': time.time()
                }
            else:
                return None
                
        except Exception as e:
            self.logger.error(f"Strategy comparison failed: {e}")
            return None
            
    def analyze_strategy_comparison(self, comparison_results):
        """Analyze strategy comparison results"""
        try:
            analysis = {
                'winner': None,
                'rankings': [],
                'metrics_comparison': {},
                'tier_analysis': {}
            }
            
            # Rank strategies by max daily return
            strategy_scores = {}
            
            for strategy, results in comparison_results.items():
                max_daily_return = results['aggregate_stats'].get('max_daily_return', 0)
                strategy_scores[strategy] = max_daily_return
                
            # Sort by score
            rankings = sorted(strategy_scores.items(), key=lambda x: x[1], reverse=True)
            analysis['rankings'] = rankings
            analysis['winner'] = rankings[0][0] if rankings else None
            
            # Compare key metrics
            metrics = ['max_daily_return', 'avg_sharpe_ratio', 'avg_win_rate', 'max_drawdown']
            
            for metric in metrics:
                analysis['metrics_comparison'][metric] = {
                    strategy: results['aggregate_stats'].get(metric, 0)
                    for strategy, results in comparison_results.items()
                }
                
            # Tier analysis - which strategy works best in each tier
            for tier in ['psycho', 'hunter', 'accumulator', 'whale']:
                tier_performance = {}
                
                for strategy, results in comparison_results.items():
                    if tier in results.get('tier_results', {}):
                        tier_perf = results['tier_results'][tier]['performance']
                        tier_performance[strategy] = tier_perf.get('total_return', 0)
                        
                if tier_performance:
                    best_for_tier = max(tier_performance.items(), key=lambda x: x[1])
                    analysis['tier_analysis'][tier] = best_for_tier[0]
                    
            return analysis
            
        except Exception as e:
            self.logger.error(f"Strategy comparison analysis failed: {e}")
            return {}
            
    def validate_live_strategy(self, strategy_name, live_performance_days=7):
        """Validate strategy against recent live performance"""
        try:
            self.logger.info(f"üîç Validating live strategy: {strategy_name}")
            
            # Get recent live trades for this strategy
            recent_trades = self.performance_analytics.get_recent_trades(200)
            strategy_trades = [t for t in recent_trades 
                             if t.get('strategy') == strategy_name and 
                             time.time() - t['timestamp'] <= live_performance_days * 24 * 3600]
            
            if not strategy_trades:
                self.logger.warning(f"No recent live trades found for {strategy_name}")
                return None
                
            # Calculate live performance
            live_performance = self.calculate_live_performance(strategy_trades)
            
            # Run backtest for same period
            backtest_performance = self.run_validation_backtest(
                strategy_name, live_performance_days
            )
            
            if backtest_performance:
                # Compare live vs backtest
                validation_result = self.compare_live_vs_backtest(
                    live_performance, backtest_performance
                )
                
                self.logger.info(f"‚úÖ Strategy validation completed: {strategy_name}")
                self.logger.info(f"   üìä Validation score: {validation_result.get('validation_score', 0):.2f}")
                
                return {
                    'strategy': strategy_name,
                    'validation_period_days': live_performance_days,
                    'live_performance': live_performance,
                    'backtest_performance': backtest_performance,
                    'validation_result': validation_result,
                    'timestamp': time.time()
                }
            else:
                return None
                
        except Exception as e:
            self.logger.error(f"Strategy validation failed: {e}")
            return None
            
    def calculate_live_performance(self, trades):
        """Calculate performance metrics from live trades"""
        try:
            if not trades:
                return {}
                
            total_pnl = sum(t['pnl'] for t in trades)
            profitable_trades = [t for t in trades if t['pnl'] > 0]
            
            return {
                'total_trades': len(trades),
                'total_pnl': total_pnl,
                'win_rate': len(profitable_trades) / len(trades),
                'avg_pnl_per_trade': total_pnl / len(trades),
                'best_trade': max(trades, key=lambda x: x['pnl'])['pnl'],
                'worst_trade': min(trades, key=lambda x: x['pnl'])['pnl'],
                'avg_hold_time': np.mean([t.get('hold_time', 0) for t in trades])
            }
            
        except Exception as e:
            self.logger.error(f"Live performance calculation failed: {e}")
            return {}
            
    def run_validation_backtest(self, strategy_name, days):
        """Run backtest for validation period"""
        try:
            # Use default symbol and timeframe
            return self.run_comprehensive_backtest(
                strategy_name, 'BTCUSDT', '5m', days
            )
            
        except Exception as e:
            self.logger.error(f"Validation backtest failed: {e}")
            return None
            
    def compare_live_vs_backtest(self, live_perf, backtest_perf):
        """Compare live performance vs backtest results"""
        try:
            # Get backtest performance for hunter tier (baseline)
            backtest_hunter = backtest_perf.get('tier_results', {}).get('hunter', {})
            backtest_metrics = backtest_hunter.get('performance', {})
            
            if not backtest_metrics:
                return {'validation_score': 0, 'status': 'insufficient_data'}
                
            # Compare key metrics
            comparisons = {}
            
            # Win rate comparison
            live_win_rate = live_perf.get('win_rate', 0)
            backtest_win_rate = backtest_metrics.get('win_rate', 0)
            comparisons['win_rate_diff'] = abs(live_win_rate - backtest_win_rate)
            
            # Average PnL comparison (normalized)
            live_avg_pnl = live_perf.get('avg_pnl_per_trade', 0)
            backtest_avg_pnl = backtest_metrics.get('avg_win', 0) + backtest_metrics.get('avg_loss', 0)
            
            if backtest_avg_pnl != 0:
                comparisons['pnl_ratio'] = live_avg_pnl / backtest_avg_pnl
            else:
                comparisons['pnl_ratio'] = 1.0
                
            # Calculate validation score (0-1, higher = better alignment)
            win_rate_score = max(0, 1 - comparisons['win_rate_diff'] * 2)  # Penalize differences
            pnl_score = min(1, max(0, comparisons['pnl_ratio']))  # Cap at 1
            
            validation_score = (win_rate_score + pnl_score) / 2
            
            # Determine status
            if validation_score >= 0.8:
                status = 'excellent'
            elif validation_score >= 0.6:
                status = 'good'
            elif validation_score >= 0.4:
                status = 'acceptable'
            else:
                status = 'poor'
                
            return {
                'validation_score': validation_score,
                'status': status,
                'comparisons': comparisons,
                'live_metrics': live_perf,
                'backtest_metrics': backtest_metrics,
                'recommendations': self.generate_validation_recommendations(validation_score, comparisons)
            }
            
        except Exception as e:
            self.logger.error(f"Live vs backtest comparison failed: {e}")
            return {}
            
    def generate_validation_recommendations(self, score, comparisons):
        """Generate recommendations based on validation results"""
        try:
            recommendations = []
            
            if score < 0.4:
                recommendations.append({
                    'type': 'strategy_review',
                    'message': 'Strategy performance significantly differs from backtest - review parameters',
                    'priority': 'high'
                })
                
            if comparisons.get('win_rate_diff', 0) > 0.2:
                recommendations.append({
                    'type': 'win_rate_investigation',
                    'message': 'Live win rate differs significantly from backtest - investigate market conditions',
                    'priority': 'medium'
                })
                
            pnl_ratio = comparisons.get('pnl_ratio', 1.0)
            if pnl_ratio < 0.5:
                recommendations.append({
                    'type': 'profitability_concern',
                    'message': 'Live trades less profitable than expected - check slippage and fees',
                    'priority': 'high'
                })
            elif pnl_ratio > 2.0:
                recommendations.append({
                    'type': 'overperformance',
                    'message': 'Strategy outperforming backtest - verify market conditions are sustainable',
                    'priority': 'low'
                })
                
            if score >= 0.8:
                recommendations.append({
                    'type': 'validation_success',
                    'message': 'Strategy performing as expected - continue with current parameters',
                    'priority': 'info'
                })
                
            return recommendations
            
        except Exception as e:
            self.logger.error(f"Validation recommendations failed: {e}")
            return []
            
    def schedule_regular_backtests(self):
        """Schedule regular backtesting for all strategies"""
        try:
            def regular_backtest_runner():
                while True:
                    try:
                        # Run weekly comprehensive backtests
                        for strategy in self.strategy_frameworks.keys():
                            self.logger.info(f"üîÑ Running scheduled backtest: {strategy}")
                            
                            result = self.run_comprehensive_backtest(
                                strategy, 'BTCUSDT', '5m', 30
                            )
                            
                            if result:
                                # Store with timestamp
                                key = f"{strategy}_scheduled_{int(time.time())}"
                                self.backtest_results[key] = result
                                
                        # Sleep for a week
                        time.sleep(7 * 24 * 3600)
                        
                    except Exception as e:
                        self.logger.error(f"Scheduled backtest error: {e}")
                        time.sleep(24 * 3600)  # Retry in 24 hours
                        
            threading.Thread(target=regular_backtest_runner, daemon=True).start()
            
            self.logger.info("üìÖ Regular backtesting scheduled (weekly)")
            
        except Exception as e:
            self.logger.error(f"Backtest scheduling failed: {e}")
            
    def get_strategy_health_report(self, strategy_name):
        """Get comprehensive health report for a strategy"""
        try:
            health_report = {
                'strategy': strategy_name,
                'timestamp': time.time(),
                'backtest_history': [],
                'optimization_history': [],
                'validation_results': [],
                'overall_health': 'unknown',
                'recommendations': []
            }
            
            # Gather backtest history
            for key, result in self.backtest_results.items():
                if result.get('strategy') == strategy_name:
                    health_report['backtest_history'].append({
                        'timestamp': result.get('timestamp', 0),
                        'max_daily_return': result['aggregate_stats'].get('max_daily_return', 0),
                        'best_tier': result.get('best_tier', 'unknown')
                    })
                    
            # Gather optimization history
            for key, result in self.optimization_results.items():
                if key.startswith(strategy_name):
                    health_report['optimization_history'].append({
                        'timestamp': time.time(),  # Would store actual timestamp
                        'score': result.get('score', 0),
                        'parameters': result.get('parameters', {})
                    })
                    
            # Assess overall health
            if health_report['backtest_history']:
                recent_performance = health_report['backtest_history'][-1]['max_daily_return']
                
                if recent_performance > 0.2:  # >20% daily return
                    health_report['overall_health'] = 'excellent'
                elif recent_performance > 0.1:  # >10% daily return
                    health_report['overall_health'] = 'good'
                elif recent_performance > 0.05:  # >5% daily return
                    health_report['overall_health'] = 'average'
                else:
                    health_report['overall_health'] = 'poor'
            else:
                health_report['overall_health'] = 'untested'
                
            # Generate recommendations
            if health_report['overall_health'] == 'poor':
                health_report['recommendations'].append({
                    'type': 'strategy_revision',
                    'message': 'Strategy showing poor performance - consider parameter optimization',
                    'priority': 'high'
                })
            elif health_report['overall_health'] == 'untested':
                health_report['recommendations'].append({
                    'type': 'testing_needed',
                    'message': 'Strategy needs comprehensive backtesting',
                    'priority': 'medium'
                })
                
            return health_report
            
        except Exception as e:
            self.logger.error(f"Strategy health report failed: {e}")
            return {}
            
    def cleanup_old_results(self, days_to_keep=30):
        """Clean up old backtest results to manage memory"""
        try:
            cutoff_time = time.time() - (days_to_keep * 24 * 3600)
            
            # Clean backtest results
            keys_to_remove = []
            for key, result in self.backtest_results.items():
                if result.get('timestamp', time.time()) < cutoff_time:
                    keys_to_remove.append(key)
                    
            for key in keys_to_remove:
                del self.backtest_results[key]
                
            # Clean optimization results (keep recent optimizations)
            opt_keys_to_remove = []
            for key in list(self.optimization_results.keys()):
                # Keep last 10 optimizations per strategy
                strategy_opts = [k for k in self.optimization_results.keys() if k.startswith(key.split('_')[0])]
                if len(strategy_opts) > 10:
                    opt_keys_to_remove.extend(strategy_opts[:-10])
                    
            for key in opt_keys_to_remove:
                if key in self.optimization_results:
                    del self.optimization_results[key]
                    
            removed_count = len(keys_to_remove) + len(opt_keys_to_remove)
            
            if removed_count > 0:
                self.logger.info(f"üßπ Cleaned up {removed_count} old backtest results")
                
        except Exception as e:
            self.logger.error(f"Backtest cleanup failed: {e}")

class OnChainAnalysisEngine:
    """Advanced on-chain analysis engine for whale tracking and market intelligence"""
    
    def __init__(self, config, binance_client, memory_manager, performance_analytics):
        self.config = config
        self.binance_client = binance_client
        self.memory_manager = memory_manager
        self.performance_analytics = performance_analytics
        self.logger = logging.getLogger('MonsterBot.OnChain')
        
        # Blockchain data sources configuration
        self.data_sources = {
            'etherscan': {
                'api_key': config.get('etherscan_api_key', ''),
                'base_url': 'https://api.etherscan.io/api',
                'rate_limit': 5,  # requests per second
                'enabled': True
            },
            'bscscan': {
                'api_key': config.get('bscscan_api_key', ''),
                'base_url': 'https://api.bscscan.com/api',
                'rate_limit': 5,
                'enabled': True
            },
            'polygonscan': {
                'api_key': config.get('polygonscan_api_key', ''),
                'base_url': 'https://api.polygonscan.com/api',
                'rate_limit': 5,
                'enabled': False
            },
            'whale_alert': {
                'api_key': config.get('whale_alert_api_key', ''),
                'base_url': 'https://api.whale-alert.io/v1',
                'rate_limit': 1,
                'enabled': False
            }
        }
        
        # Whale tracking configuration
        self.whale_config = {
            'btc_whale_threshold': 100,  # BTC
            'eth_whale_threshold': 1000,  # ETH
            'usdt_whale_threshold': 1000000,  # USDT
            'usdc_whale_threshold': 1000000,  # USDC
            'bnb_whale_threshold': 10000,  # BNB
            'tracking_addresses': {
                'known_whales': [],
                'exchanges': [
                    '0x28c6c06298d514db089934071355e5743bf21d60',  # Binance hot wallet
                    '0x21a31ee1afc51d94c2efccaa2092ad1028285549',  # Binance cold wallet
                    '0x56eddb7aa87536c09ccc2793473599fd21a8b17f',  # Binance wallet
                    '0x9696f59e4d72e237be84ffd425dcad154bf96976',  # Coinbase
                    '0xa7efae728d2936e78bda97dc267687568dd593f3',  # Coinbase cold
                ],
                'institutions': [],
                'defi_protocols': [
                    '0xa0b86a33e6776a5d18a6f06e6f4a7b305f77893a',  # Compound
                    '0x7d2768de32b0b80b7a3454c06bdac94a69ddc7a9',  # Aave
                    '0xbe0eb53f46cd790cd13851d5eff43d12404d33e8',  # Uniswap
                ]
            }
        }
        
        # DeFi protocol monitoring
        self.defi_protocols = {
            'compound': {
                'tokens': ['COMP', 'cUSDT', 'cUSDC', 'cETH', 'cDAI'],
                'metrics': ['total_supply', 'total_borrow', 'utilization_rate'],
                'whale_threshold': 1000000  # $1M
            },
            'aave': {
                'tokens': ['AAVE', 'aUSDT', 'aUSDC', 'aETH', 'aDAI'],
                'metrics': ['total_liquidity', 'total_borrows', 'health_factor'],
                'whale_threshold': 1000000
            },
            'uniswap': {
                'tokens': ['UNI', 'WETH', 'USDC', 'USDT'],
                'metrics': ['pool_liquidity', 'volume_24h', 'fees_24h'],
                'whale_threshold': 500000
            },
            'curve': {
                'tokens': ['CRV', '3CRV', 'stETH', 'FRAX'],
                'metrics': ['pool_tvl', 'trading_volume', 'pool_apy'],
                'whale_threshold': 2000000
            }
        }
        
        # On-chain metrics tracking
        self.onchain_metrics = {
            'network_stats': defaultdict(dict),
            'whale_movements': defaultdict(list),
            'large_transactions': defaultdict(list),
            'exchange_flows': defaultdict(dict),
            'defi_movements': defaultdict(dict),
            'institutional_activity': defaultdict(list)
        }
        
        # Alert thresholds for on-chain events
        self.alert_thresholds = {
            'large_btc_move': 100,  # BTC
            'large_eth_move': 1000,  # ETH
            'large_stablecoin_move': 10000000,  # $10M
            'exchange_inflow_spike': 5000000,  # $5M
            'exchange_outflow_spike': 5000000,  # $5M
            'whale_accumulation': 1000000,  # $1M
            'institutional_buy': 5000000,  # $5M
            'defi_liquidation': 1000000  # $1M
        }
        
        # Market impact correlation
        self.impact_correlation = {
            'immediate': {},  # Price impact within 1 hour
            'short_term': {},  # Price impact within 24 hours
            'medium_term': {}  # Price impact within 7 days
        }
        
        # Whale behavior patterns
        self.whale_patterns = {
            'accumulation_phase': {
                'indicators': ['consistent_buying', 'exchange_withdrawals', 'long_term_holding'],
                'market_signal': 'bullish',
                'confidence_threshold': 0.7
            },
            'distribution_phase': {
                'indicators': ['consistent_selling', 'exchange_deposits', 'profit_taking'],
                'market_signal': 'bearish',
                'confidence_threshold': 0.7
            },
            'rotation_phase': {
                'indicators': ['token_swaps', 'portfolio_rebalancing', 'cross_chain_moves'],
                'market_signal': 'neutral',
                'confidence_threshold': 0.6
            }
        }
        
        # Initialize on-chain analysis
        self.initialize_onchain_system()
        
    def initialize_onchain_system(self):
        """Initialize on-chain analysis system"""
        try:
            # Test data source connections
            self.test_data_sources()
            
            # Load known whale addresses
            self.load_whale_addresses()
            
            # Start real-time monitoring
            self.start_onchain_monitoring()
            
            # Start whale behavior analysis
            self.start_whale_analysis()
            
            # Start DeFi monitoring
            self.start_defi_monitoring()
            
            self.logger.info("üîó On-chain Analysis Engine initialized")
            self.logger.info(f"   üì° Data sources: {len([s for s in self.data_sources.values() if s['enabled']])}")
            self.logger.info(f"   üêã Whale addresses: {len(self.whale_config['tracking_addresses']['known_whales'])}")
            self.logger.info(f"   üè¶ DeFi protocols: {len(self.defi_protocols)}")
            
        except Exception as e:
            self.logger.error(f"On-chain system initialization failed: {e}")
            
    def test_data_sources(self):
        """Test connections to blockchain data sources"""
        try:
            for source_name, source_config in self.data_sources.items():
                if source_config['enabled'] and source_config['api_key']:
                    if self.test_source_connection(source_name):
                        self.logger.info(f"‚úÖ {source_name.title()} API connected")
                    else:
                        self.logger.warning(f"‚ùå {source_name.title()} API connection failed")
                        source_config['enabled'] = False
                        
        except Exception as e:
            self.logger.error(f"Data source testing failed: {e}")
            
    def test_source_connection(self, source_name):
        """Test specific data source connection"""
        try:
            if source_name == 'etherscan':
                return self.test_etherscan_connection()
            elif source_name == 'bscscan':
                return self.test_bscscan_connection()
            elif source_name == 'whale_alert':
                return self.test_whale_alert_connection()
            else:
                return False
                
        except Exception as e:
            self.logger.error(f"{source_name} connection test failed: {e}")
            return False
            
    def test_etherscan_connection(self):
        """Test Etherscan API connection"""
        try:
            import requests
            
            config = self.data_sources['etherscan']
            
            if not config['api_key']:
                return False
                
            # Test API call - get latest block
            url = f"{config['base_url']}?module=proxy&action=eth_blockNumber&apikey={config['api_key']}"
            response = requests.get(url, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                return 'result' in data
                
            return False
            
        except Exception as e:
            self.logger.error(f"Etherscan connection test failed: {e}")
            return False
            
    def test_bscscan_connection(self):
        """Test BSCScan API connection"""
        try:
            import requests
            
            config = self.data_sources['bscscan']
            
            if not config['api_key']:
                return False
                
            # Test API call
            url = f"{config['base_url']}?module=proxy&action=eth_blockNumber&apikey={config['api_key']}"
            response = requests.get(url, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                return 'result' in data
                
            return False
            
        except Exception as e:
            self.logger.error(f"BSCScan connection test failed: {e}")
            return False
            
    def test_whale_alert_connection(self):
        """Test Whale Alert API connection"""
        try:
            import requests
            
            config = self.data_sources['whale_alert']
            
            if not config['api_key']:
                return False
                
            # Test API call
            url = f"{config['base_url']}/status"
            headers = {'X-WA-API-KEY': config['api_key']}
            response = requests.get(url, headers=headers, timeout=10)
            
            return response.status_code == 200
            
        except Exception as e:
            self.logger.error(f"Whale Alert connection test failed: {e}")
            return False
            
    def load_whale_addresses(self):
        """Load known whale addresses from storage"""
        try:
            # Load from memory manager
            saved_whales = self.memory_manager.get_whale_addresses()
            
            if saved_whales:
                self.whale_config['tracking_addresses']['known_whales'].extend(saved_whales)
                
            # Add some well-known whale addresses
            known_whale_addresses = [
                '0x47ac0fb4f2d84898e4d9e7b4dab3c24507a6d503',  # Known BTC whale
                '0x8894e0a0c962cb723c1976a4421c95949be2d4e3',  # ETH whale
                '0x40ec5b33f54e0e8a33a975908c5ba1c14e5bbbdf',  # Polygon Exchange
                '0xe93381fb4c4f14bda253907b18fad305d799241a',  # Huobi wallet
            ]
            
            for address in known_whale_addresses:
                if address not in self.whale_config['tracking_addresses']['known_whales']:
                    self.whale_config['tracking_addresses']['known_whales'].append(address)
                    
            self.logger.info(f"üìã Loaded {len(self.whale_config['tracking_addresses']['known_whales'])} whale addresses")
            
        except Exception as e:
            self.logger.error(f"Whale address loading failed: {e}")
            
    def start_onchain_monitoring(self):
        """Start real-time on-chain monitoring"""
        def onchain_monitor():
            while True:
                try:
                    # Monitor large transactions
                    self.monitor_large_transactions()
                    
                    # Monitor exchange flows
                    self.monitor_exchange_flows()
                    
                    # Monitor whale movements
                    self.monitor_whale_movements()
                    
                    # Update network statistics
                    self.update_network_stats()
                    
                    time.sleep(60)  # Update every minute
                except Exception as e:
                    self.logger.error(f"On-chain monitoring error: {e}")
                    time.sleep(300)  # Wait 5 minutes on error
                    
        threading.Thread(target=onchain_monitor, daemon=True).start()
        
    def start_whale_analysis(self):
        """Start whale behavior analysis"""
        def whale_analyzer():
            while True:
                try:
                    # Analyze whale patterns
                    self.analyze_whale_patterns()
                    
                    # Detect accumulation/distribution
                    self.detect_whale_phases()
                    
                    # Generate whale signals
                    self.generate_whale_signals()
                    
                    time.sleep(300)  # Update every 5 minutes
                except Exception as e:
                    self.logger.error(f"Whale analysis error: {e}")
                    time.sleep(600)
                    
        threading.Thread(target=whale_analyzer, daemon=True).start()
        
    def start_defi_monitoring(self):
        """Start DeFi protocol monitoring"""
        def defi_monitor():
            while True:
                try:
                    # Monitor DeFi protocols
                    self.monitor_defi_protocols()
                    
                    # Track large DeFi movements
                    self.track_defi_movements()
                    
                    # Analyze yield farming trends
                    self.analyze_yield_trends()
                    
                    time.sleep(600)  # Update every 10 minutes
                except Exception as e:
                    self.logger.error(f"DeFi monitoring error: {e}")
                    time.sleep(600)
                    
        threading.Thread(target=defi_monitor, daemon=True).start()
        
    def monitor_large_transactions(self):
        """Monitor large on-chain transactions"""
        try:
            # Monitor Bitcoin large transactions
            btc_large_txs = self.get_large_bitcoin_transactions()
            
            if btc_large_txs:
                for tx in btc_large_txs:
                    self.process_large_transaction(tx, 'BTC')
                    
            # Monitor Ethereum large transactions
            eth_large_txs = self.get_large_ethereum_transactions()
            
            if eth_large_txs:
                for tx in eth_large_txs:
                    self.process_large_transaction(tx, 'ETH')
                    
            # Monitor BSC large transactions
            bsc_large_txs = self.get_large_bsc_transactions()
            
            if bsc_large_txs:
                for tx in bsc_large_txs:
                    self.process_large_transaction(tx, 'BNB')
                    
        except Exception as e:
            self.logger.error(f"Large transaction monitoring failed: {e}")
            
    def get_large_bitcoin_transactions(self):
        """Get large Bitcoin transactions (simulated)"""
        try:
            # In production, this would use actual Bitcoin API
            # For demo, simulate large transactions
            large_txs = []
            
            # Simulate random large BTC transactions
            if np.random.random() < 0.1:  # 10% chance
                tx_amount = np.random.uniform(100, 1000)  # 100-1000 BTC
                
                large_txs.append({
                    'hash': f"btc_tx_{int(time.time())}_{np.random.randint(1000, 9999)}",
                    'amount': tx_amount,
                    'usd_value': tx_amount * 50000,  # Assume $50k BTC price
                    'from_address': 'unknown_whale_address',
                    'to_address': 'exchange_address',
                    'timestamp': time.time(),
                    'block_height': 800000 + np.random.randint(1000),
                    'fees': tx_amount * 0.0001
                })
                
            return large_txs
            
        except Exception as e:
            self.logger.error(f"Bitcoin transaction fetching failed: {e}")
            return []
            
    def get_large_ethereum_transactions(self):
        """Get large Ethereum transactions using Etherscan"""
        try:
            if not self.data_sources['etherscan']['enabled']:
                return []
                
            import requests
            
            config = self.data_sources['etherscan']
            large_txs = []
            
            # Get latest block
            latest_block_url = f"{config['base_url']}?module=proxy&action=eth_blockNumber&apikey={config['api_key']}"
            response = requests.get(latest_block_url, timeout=10)
            
            if response.status_code == 200:
                latest_block = int(response.json()['result'], 16)
                
                # Check last few blocks for large transactions
                for block_offset in range(5):
                    block_number = latest_block - block_offset
                    
                    # Get block transactions
                    block_url = f"{config['base_url']}?module=proxy&action=eth_getBlockByNumber&tag=0x{block_number:x}&boolean=true&apikey={config['api_key']}"
                    block_response = requests.get(block_url, timeout=10)
                    
                    if block_response.status_code == 200:
                        block_data = block_response.json()
                        
                        if 'result' in block_data and block_data['result']:
                            transactions = block_data['result'].get('transactions', [])
                            
                            for tx in transactions:
                                value_wei = int(tx.get('value', '0'), 16)
                                value_eth = value_wei / 1e18
                                
                                # Check if it's a large transaction
                                if value_eth >= self.whale_config['eth_whale_threshold']:
                                    large_txs.append({
                                        'hash': tx['hash'],
                                        'amount': value_eth,
                                        'usd_value': value_eth * 3000,  # Assume $3k ETH price
                                        'from_address': tx['from'],
                                        'to_address': tx['to'],
                                        'timestamp': int(tx['blockNumber'], 16),
                                        'block_height': int(tx['blockNumber'], 16),
                                        'gas_price': int(tx['gasPrice'], 16),
                                        'gas_used': int(tx['gas'], 16)
                                    })
                                    
                    time.sleep(0.2)  # Rate limiting
                    
            return large_txs
            
        except Exception as e:
            self.logger.error(f"Ethereum transaction fetching failed: {e}")
            return []
            
    def get_large_bsc_transactions(self):
        """Get large BSC transactions using BSCScan"""
        try:
            if not self.data_sources['bscscan']['enabled']:
                return []
                
            # Similar implementation to Ethereum but for BSC
            # For demo purposes, simulate some transactions
            large_txs = []
            
            if np.random.random() < 0.05:  # 5% chance
                tx_amount = np.random.uniform(10000, 50000)  # BNB amount
                
                large_txs.append({
                    'hash': f"bsc_tx_{int(time.time())}_{np.random.randint(1000, 9999)}",
                    'amount': tx_amount,
                    'usd_value': tx_amount * 300,  # Assume $300 BNB price
                    'from_address': 'bsc_whale_address',
                    'to_address': 'pancakeswap_router',
                    'timestamp': time.time(),
                    'block_height': 25000000 + np.random.randint(1000),
                    'gas_price': 5000000000,  # 5 gwei
                    'gas_used': 21000
                })
                
            return large_txs
            
        except Exception as e:
            self.logger.error(f"BSC transaction fetching failed: {e}")
            return []
            
    def process_large_transaction(self, transaction, chain):
        """Process and analyze large transaction"""
        try:
            # Store transaction
            tx_key = f"{chain}_{transaction['timestamp']}"
            self.onchain_metrics['large_transactions'][tx_key] = transaction
            
            # Determine transaction type
            tx_type = self.classify_transaction(transaction, chain)
            transaction['type'] = tx_type
            
            # Check if it involves known addresses
            is_whale = self.check_whale_involvement(transaction)
            transaction['whale_involved'] = is_whale
            
            # Calculate market impact potential
            impact_score = self.calculate_impact_score(transaction, chain)
            transaction['impact_score'] = impact_score
            
            # Generate alert if significant
            if self.should_alert_transaction(transaction):
                self.generate_transaction_alert(transaction, chain)
                
            # Update whale tracking if applicable
            if is_whale:
                self.update_whale_tracking(transaction, chain)
                
            self.logger.info(f"üîó Large {chain} transaction: ${transaction['usd_value']:,.0f} ({tx_type})")
            
        except Exception as e:
            self.logger.error(f"Transaction processing failed: {e}")
            
    def classify_transaction(self, transaction, chain):
        """Classify transaction type based on addresses and patterns"""
        try:
            from_addr = transaction.get('from_address', '').lower()
            to_addr = transaction.get('to_address', '').lower()
            
            # Check if exchange-related
            exchange_addresses = [addr.lower() for addr in self.whale_config['tracking_addresses']['exchanges']]
            
            if from_addr in exchange_addresses:
                return 'exchange_outflow'
            elif to_addr in exchange_addresses:
                return 'exchange_inflow'
                
            # Check if DeFi-related
            defi_addresses = [addr.lower() for addr in self.whale_config['tracking_addresses']['defi_protocols']]
            
            if from_addr in defi_addresses or to_addr in defi_addresses:
                return 'defi_interaction'
                
            # Check if whale-to-whale
            whale_addresses = [addr.lower() for addr in self.whale_config['tracking_addresses']['known_whales']]
            
            if from_addr in whale_addresses and to_addr in whale_addresses:
                return 'whale_to_whale'
            elif from_addr in whale_addresses:
                return 'whale_distribution'
            elif to_addr in whale_addresses:
                return 'whale_accumulation'
                
            # Default classification
            return 'large_transfer'
            
        except Exception as e:
            self.logger.error(f"Transaction classification failed: {e}")
            return 'unknown'
            
    def check_whale_involvement(self, transaction):
        """Check if transaction involves known whale addresses"""
        try:
            from_addr = transaction.get('from_address', '').lower()
            to_addr = transaction.get('to_address', '').lower()
            
            all_whale_addresses = []
            all_whale_addresses.extend([addr.lower() for addr in self.whale_config['tracking_addresses']['known_whales']])
            all_whale_addresses.extend([addr.lower() for addr in self.whale_config['tracking_addresses']['exchanges']])
            all_whale_addresses.extend([addr.lower() for addr in self.whale_config['tracking_addresses']['institutions']])
            
            return from_addr in all_whale_addresses or to_addr in all_whale_addresses
            
        except Exception as e:
            self.logger.error(f"Whale involvement check failed: {e}")
            return False
            
    def calculate_impact_score(self, transaction, chain):
        """Calculate potential market impact score (0-100)"""
        try:
            usd_value = transaction.get('usd_value', 0)
            tx_type = transaction.get('type', 'unknown')
            
            # Base score from USD value
            base_score = min(100, (usd_value / 10000000) * 50)  # $10M = 50 points
            
            # Type multipliers
            type_multipliers = {
                'exchange_inflow': 1.5,   # Potential selling pressure
                'exchange_outflow': 0.8,  # Potential accumulation
                'whale_distribution': 1.3,
                'whale_accumulation': 0.7,
                'defi_interaction': 0.9,
                'large_transfer': 1.0
            }
            
            multiplier = type_multipliers.get(tx_type, 1.0)
            
            # Chain multipliers (based on market cap influence)
            chain_multipliers = {'BTC': 1.2, 'ETH': 1.0, 'BNB': 0.8}
            chain_mult = chain_multipliers.get(chain, 1.0)
            
            impact_score = base_score * multiplier * chain_mult
            
            return min(100, max(0, impact_score))
            
        except Exception as e:
            self.logger.error(f"Impact score calculation failed: {e}")
            return 0
            
    def should_alert_transaction(self, transaction):
        """Determine if transaction should trigger alert"""
        try:
            usd_value = transaction.get('usd_value', 0)
            impact_score = transaction.get('impact_score', 0)
            tx_type = transaction.get('type', 'unknown')
            
            # Alert thresholds
            if usd_value >= 50000000:  # $50M+ always alert
                return True
            elif impact_score >= 75:  # High impact score
                return True
            elif tx_type in ['exchange_inflow', 'whale_distribution'] and usd_value >= 10000000:  # $10M+ bearish moves
                return True
            elif transaction.get('whale_involved', False) and usd_value >= 5000000:  # $5M+ whale moves
                return True
                
            return False
            
        except Exception as e:
            self.logger.error(f"Alert decision failed: {e}")
            return False
            
    def generate_transaction_alert(self, transaction, chain):
        """Generate alert for significant transaction"""
        try:
            alert_data = {
                'type': 'large_transaction',
                'chain': chain,
                'hash': transaction.get('hash', 'unknown'),
                'amount': transaction.get('amount', 0),
                'usd_value': transaction.get('usd_value', 0),
                'transaction_type': transaction.get('type', 'unknown'),
                'impact_score': transaction.get('impact_score', 0),
                'whale_involved': transaction.get('whale_involved', False),
                'timestamp': transaction.get('timestamp', time.time())
            }
            
            # Determine alert priority
            if alert_data['usd_value'] >= 100000000:  # $100M+
                priority = 'critical'
            elif alert_data['usd_value'] >= 50000000:  # $50M+
                priority = 'high'
            else:
                priority = 'medium'
                
            alert_data['priority'] = priority
            
            # Store alert
            self.onchain_metrics['institutional_activity'][chain].append(alert_data)
            
            # Send to notification system (would integrate with Part 16)
            self.logger.info(f"üö® {chain} WHALE ALERT: ${alert_data['usd_value']:,.0f} {alert_data['transaction_type']}")
            
        except Exception as e:
            self.logger.error(f"Transaction alert generation failed: {e}")
            
    def monitor_exchange_flows(self):
        """Monitor exchange inflow/outflow patterns"""
        try:
            for exchange_addr in self.whale_config['tracking_addresses']['exchanges']:
                # Get exchange activity
                inflows, outflows = self.get_exchange_activity(exchange_addr)
                
                # Calculate net flow
                net_flow = sum(outflows) - sum(inflows)
                
                # Store exchange flow data
                self.onchain_metrics['exchange_flows'][exchange_addr] = {
                    'inflows': inflows,
                    'outflows': outflows,
                    'net_flow': net_flow,
                    'timestamp': time.time(),
                    'flow_direction': 'outflow' if net_flow > 0 else 'inflow'
                }
                
                # Check for significant flows
                if abs(net_flow) >= self.alert_thresholds['exchange_inflow_spike']:
                    self.generate_exchange_flow_alert(exchange_addr, net_flow)
                    
        except Exception as e:
            self.logger.error(f"Exchange flow monitoring failed: {e}")
            
    def get_exchange_activity(self, exchange_address):
        """Get exchange inflow/outflow activity (simulated)"""
        try:
            # In production, this would analyze actual blockchain data
            # For demo, simulate exchange activity
            
            # Simulate random inflows and outflows
            num_inflows = np.random.poisson(3)  # Average 3 inflows per period
            num_outflows = np.random.poisson(2)  # Average 2 outflows per period
            
            inflows = [np.random.uniform(100000, 5000000) for _ in range(num_inflows)]
            outflows = [np.random.uniform(200000, 3000000) for _ in range(num_outflows)]
            
            return inflows, outflows
            
        except Exception as e:
            self.logger.error(f"Exchange activity fetching failed: {e}")
            return [], []
            
    def generate_exchange_flow_alert(self, exchange_address, net_flow):
        """Generate alert for significant exchange flows"""
        try:
	    flow_type = 'massive outflow' if net_flow > 0 else 'massive inflow'
            
            alert_data = {
                'type': 'exchange_flow',
                'exchange_address': exchange_address,
                'net_flow_usd': abs(net_flow),
                'flow_direction': 'outflow' if net_flow > 0 else 'inflow',
                'flow_type': flow_type,
                'timestamp': time.time(),
                'market_implication': 'bullish' if net_flow > 0 else 'bearish'
            }
            
            self.logger.warning(f"üè¶ EXCHANGE FLOW ALERT: ${abs(net_flow):,.0f} {flow_type}")
            
        except Exception as e:
            self.logger.error(f"Exchange flow alert generation failed: {e}")
            
    def monitor_whale_movements(self):
        """Monitor specific whale address movements"""
        try:
            for whale_addr in self.whale_config['tracking_addresses']['known_whales']:
                # Get whale activity
                whale_activity = self.get_whale_activity(whale_addr)
                
                if whale_activity:
                    # Analyze whale behavior
                    behavior_pattern = self.analyze_whale_behavior(whale_addr, whale_activity)
                    
                    # Store whale movement data
                    self.onchain_metrics['whale_movements'][whale_addr].append({
                        'activity': whale_activity,
                        'behavior_pattern': behavior_pattern,
                        'timestamp': time.time()
                    })
                    
                    # Generate signals if significant
                    if behavior_pattern.get('significance', 0) > 0.7:
                        self.generate_whale_movement_signal(whale_addr, behavior_pattern)
                        
        except Exception as e:
            self.logger.error(f"Whale movement monitoring failed: {e}")
            
    def get_whale_activity(self, whale_address):
        """Get activity for specific whale address (simulated)"""
        try:
            # In production, this would query blockchain APIs for the address
            # For demo, simulate whale activity
            
            if np.random.random() < 0.3:  # 30% chance of activity
                activity_type = np.random.choice(['accumulation', 'distribution', 'rotation'])
                amount_usd = np.random.uniform(1000000, 50000000)  # $1M - $50M
                
                return {
                    'type': activity_type,
                    'amount_usd': amount_usd,
                    'tokens_involved': np.random.choice(['BTC', 'ETH', 'BNB', 'USDT'], size=np.random.randint(1, 3)).tolist(),
                    'transaction_count': np.random.randint(1, 10),
                    'time_span_hours': np.random.uniform(1, 24),
                    'confidence': np.random.uniform(0.6, 1.0)
                }
                
            return None
            
        except Exception as e:
            self.logger.error(f"Whale activity fetching failed: {e}")
            return None
            
    def analyze_whale_behavior(self, whale_address, activity):
        """Analyze whale behavior patterns"""
        try:
            behavior_analysis = {
                'pattern_type': activity['type'],
                'magnitude': self.calculate_magnitude(activity['amount_usd']),
                'urgency': self.calculate_urgency(activity),
                'market_signal': self.determine_market_signal(activity),
                'significance': 0,
                'confidence': activity.get('confidence', 0.5)
            }
            
            # Calculate significance score
            magnitude_score = min(1.0, activity['amount_usd'] / 100000000)  # $100M = max score
            urgency_score = behavior_analysis['urgency']
            confidence_score = behavior_analysis['confidence']
            
            behavior_analysis['significance'] = (magnitude_score + urgency_score + confidence_score) / 3
            
            return behavior_analysis
            
        except Exception as e:
            self.logger.error(f"Whale behavior analysis failed: {e}")
            return {}
            
    def calculate_magnitude(self, amount_usd):
        """Calculate magnitude category for whale movement"""
        try:
            if amount_usd >= 100000000:  # $100M+
                return 'massive'
            elif amount_usd >= 50000000:  # $50M+
                return 'very_large'
            elif amount_usd >= 20000000:  # $20M+
                return 'large'
            elif amount_usd >= 5000000:   # $5M+
                return 'medium'
            else:
                return 'small'
                
        except Exception as e:
            self.logger.error(f"Magnitude calculation failed: {e}")
            return 'unknown'
            
    def calculate_urgency(self, activity):
        """Calculate urgency score based on activity characteristics"""
        try:
            # Fast transactions = higher urgency
            time_span = activity.get('time_span_hours', 24)
            tx_count = activity.get('transaction_count', 1)
            
            # Calculate transaction rate
            tx_rate = tx_count / time_span if time_span > 0 else 0
            
            # Normalize urgency (0-1)
            urgency = min(1.0, tx_rate / 2.0)  # 2 tx/hour = max urgency
            
            return urgency
            
        except Exception as e:
            self.logger.error(f"Urgency calculation failed: {e}")
            return 0.5
            
    def determine_market_signal(self, activity):
        """Determine market signal from whale activity"""
        try:
            activity_type = activity.get('type', 'unknown')
            amount_usd = activity.get('amount_usd', 0)
            
            # Large accumulation = bullish
            if activity_type == 'accumulation' and amount_usd > 10000000:
                return {'signal': 'bullish', 'strength': 'strong'}
            elif activity_type == 'accumulation':
                return {'signal': 'bullish', 'strength': 'moderate'}
                
            # Large distribution = bearish
            elif activity_type == 'distribution' and amount_usd > 10000000:
                return {'signal': 'bearish', 'strength': 'strong'}
            elif activity_type == 'distribution':
                return {'signal': 'bearish', 'strength': 'moderate'}
                
            # Rotation = neutral
            elif activity_type == 'rotation':
                return {'signal': 'neutral', 'strength': 'weak'}
                
            else:
                return {'signal': 'neutral', 'strength': 'none'}
                
        except Exception as e:
            self.logger.error(f"Market signal determination failed: {e}")
            return {'signal': 'neutral', 'strength': 'none'}
            
    def generate_whale_movement_signal(self, whale_address, behavior_pattern):
        """Generate trading signal from whale movement"""
        try:
            signal_data = {
                'type': 'whale_movement',
                'whale_address': whale_address,
                'pattern': behavior_pattern['pattern_type'],
                'magnitude': behavior_pattern['magnitude'],
                'market_signal': behavior_pattern['market_signal'],
                'significance': behavior_pattern['significance'],
                'confidence': behavior_pattern['confidence'],
                'timestamp': time.time(),
                'recommended_action': self.determine_recommended_action(behavior_pattern)
            }
            
            self.logger.info(f"üêã WHALE SIGNAL: {signal_data['market_signal']['signal'].upper()} from {behavior_pattern['magnitude']} {behavior_pattern['pattern_type']}")
            
            # Store signal for strategy system
            self.memory_manager.store_whale_signal(signal_data)
            
            return signal_data
            
        except Exception as e:
            self.logger.error(f"Whale movement signal generation failed: {e}")
            return None
            
    def determine_recommended_action(self, behavior_pattern):
        """Determine recommended trading action from whale behavior"""
        try:
            market_signal = behavior_pattern.get('market_signal', {})
            signal_type = market_signal.get('signal', 'neutral')
            signal_strength = market_signal.get('strength', 'none')
            significance = behavior_pattern.get('significance', 0)
            
            if signal_type == 'bullish' and signal_strength == 'strong' and significance > 0.8:
                return {'action': 'buy_aggressive', 'position_size': 0.8, 'time_horizon': 'short'}
            elif signal_type == 'bullish' and significance > 0.6:
                return {'action': 'buy_moderate', 'position_size': 0.5, 'time_horizon': 'medium'}
            elif signal_type == 'bearish' and signal_strength == 'strong' and significance > 0.8:
                return {'action': 'sell_aggressive', 'position_size': 0.8, 'time_horizon': 'short'}
            elif signal_type == 'bearish' and significance > 0.6:
                return {'action': 'sell_moderate', 'position_size': 0.5, 'time_horizon': 'medium'}
            else:
                return {'action': 'hold', 'position_size': 0, 'time_horizon': 'none'}
                
        except Exception as e:
            self.logger.error(f"Recommended action determination failed: {e}")
            return {'action': 'hold', 'position_size': 0, 'time_horizon': 'none'}
            
    def analyze_whale_patterns(self):
        """Analyze overall whale behavior patterns"""
        try:
            # Collect recent whale movements
            recent_movements = []
            
            for whale_addr, movements in self.onchain_metrics['whale_movements'].items():
                # Get movements from last 24 hours
                recent = [m for m in movements if time.time() - m['timestamp'] < 86400]
                recent_movements.extend(recent)
                
            if not recent_movements:
                return
                
            # Analyze patterns
            pattern_analysis = self.detect_collective_patterns(recent_movements)
            
            # Update whale pattern metrics
            self.onchain_metrics['whale_patterns'] = {
                'timestamp': time.time(),
                'total_movements': len(recent_movements),
                'dominant_pattern': pattern_analysis.get('dominant_pattern', 'mixed'),
                'market_sentiment': pattern_analysis.get('market_sentiment', 'neutral'),
                'confidence': pattern_analysis.get('confidence', 0.5),
                'pattern_strength': pattern_analysis.get('pattern_strength', 'weak')
            }
            
        except Exception as e:
            self.logger.error(f"Whale pattern analysis failed: {e}")
            
    def detect_collective_patterns(self, movements):
        """Detect collective whale behavior patterns"""
        try:
            if not movements:
                return {}
                
            # Count pattern types
            pattern_counts = defaultdict(int)
            total_significance = 0
            
            for movement in movements:
                behavior = movement.get('behavior_pattern', {})
                pattern_type = behavior.get('pattern_type', 'unknown')
                significance = behavior.get('significance', 0)
                
                pattern_counts[pattern_type] += 1
                total_significance += significance
                
            # Find dominant pattern
            dominant_pattern = max(pattern_counts.items(), key=lambda x: x[1])[0] if pattern_counts else 'mixed'
            
            # Calculate overall sentiment
            bullish_patterns = pattern_counts.get('accumulation', 0)
            bearish_patterns = pattern_counts.get('distribution', 0)
            neutral_patterns = pattern_counts.get('rotation', 0)
            
            total_patterns = sum(pattern_counts.values())
            
            if total_patterns == 0:
                market_sentiment = 'neutral'
                confidence = 0
            else:
                bullish_ratio = bullish_patterns / total_patterns
                bearish_ratio = bearish_patterns / total_patterns
                
                if bullish_ratio > 0.6:
                    market_sentiment = 'bullish'
                    confidence = bullish_ratio
                elif bearish_ratio > 0.6:
                    market_sentiment = 'bearish'
                    confidence = bearish_ratio
                else:
                    market_sentiment = 'neutral'
                    confidence = max(bullish_ratio, bearish_ratio)
                    
            # Pattern strength
            avg_significance = total_significance / len(movements) if movements else 0
            
            if avg_significance > 0.8:
                pattern_strength = 'very_strong'
            elif avg_significance > 0.6:
                pattern_strength = 'strong'
            elif avg_significance > 0.4:
                pattern_strength = 'moderate'
            else:
                pattern_strength = 'weak'
                
            return {
                'dominant_pattern': dominant_pattern,
                'market_sentiment': market_sentiment,
                'confidence': confidence,
                'pattern_strength': pattern_strength,
                'total_movements': total_patterns,
                'avg_significance': avg_significance
            }
            
        except Exception as e:
            self.logger.error(f"Collective pattern detection failed: {e}")
            return {}
            
    def detect_whale_phases(self):
        """Detect market phases based on whale behavior"""
        try:
            # Get whale pattern data
            whale_patterns = self.onchain_metrics.get('whale_patterns', {})
            
            if not whale_patterns:
                return
                
            market_sentiment = whale_patterns.get('market_sentiment', 'neutral')
            confidence = whale_patterns.get('confidence', 0)
            pattern_strength = whale_patterns.get('pattern_strength', 'weak')
            
            # Determine market phase
            current_phase = self.determine_market_phase(market_sentiment, confidence, pattern_strength)
            
            # Store phase information
            self.onchain_metrics['market_phase'] = {
                'phase': current_phase,
                'confidence': confidence,
                'timestamp': time.time(),
                'sentiment': market_sentiment,
                'strength': pattern_strength
            }
            
            self.logger.info(f"üìä WHALE PHASE: {current_phase.upper()} (confidence: {confidence:.1%})")
            
        except Exception as e:
            self.logger.error(f"Whale phase detection failed: {e}")
            
    def determine_market_phase(self, sentiment, confidence, strength):
        """Determine current market phase"""
        try:
            # High confidence phases
            if confidence > 0.7:
                if sentiment == 'bullish' and strength in ['strong', 'very_strong']:
                    return 'accumulation_phase'
                elif sentiment == 'bearish' and strength in ['strong', 'very_strong']:
                    return 'distribution_phase'
                    
            # Medium confidence phases
            elif confidence > 0.5:
                if sentiment == 'bullish':
                    return 'early_accumulation'
                elif sentiment == 'bearish':
                    return 'early_distribution'
                    
            # Low confidence or mixed signals
            return 'consolidation_phase'
            
        except Exception as e:
            self.logger.error(f"Market phase determination failed: {e}")
            return 'unknown_phase'
            
    def generate_whale_signals(self):
        """Generate trading signals based on whale analysis"""
        try:
            market_phase = self.onchain_metrics.get('market_phase', {})
            
            if not market_phase:
                return
                
            phase = market_phase.get('phase', 'unknown_phase')
            confidence = market_phase.get('confidence', 0)
            
            # Generate signals based on phase
            signal = self.create_phase_signal(phase, confidence)
            
            if signal:
                # Store signal for trading system
                self.memory_manager.store_market_phase_signal(signal)
                
                self.logger.info(f"üéØ WHALE-BASED SIGNAL: {signal['action'].upper()} (strength: {signal['strength']})")
                
            return signal
            
        except Exception as e:
            self.logger.error(f"Whale signal generation failed: {e}")
            return None
            
    def create_phase_signal(self, phase, confidence):
        """Create trading signal from market phase"""
        try:
            signal_mapping = {
                'accumulation_phase': {
                    'action': 'buy',
                    'strength': 'strong',
                    'position_size': 0.7,
                    'time_horizon': 'medium'
                },
                'early_accumulation': {
                    'action': 'buy',
                    'strength': 'moderate',
                    'position_size': 0.4,
                    'time_horizon': 'medium'
                },
                'distribution_phase': {
                    'action': 'sell',
                    'strength': 'strong',
                    'position_size': 0.7,
                    'time_horizon': 'short'
                },
                'early_distribution': {
                    'action': 'sell',
                    'strength': 'moderate',
                    'position_size': 0.4,
                    'time_horizon': 'short'
                },
                'consolidation_phase': {
                    'action': 'hold',
                    'strength': 'weak',
                    'position_size': 0,
                    'time_horizon': 'none'
                }
            }
            
            base_signal = signal_mapping.get(phase, signal_mapping['consolidation_phase'])
            
            # Adjust signal strength based on confidence
            if confidence < 0.5:
                base_signal['strength'] = 'weak'
                base_signal['position_size'] *= 0.5
                
            signal = {
                'type': 'whale_phase',
                'phase': phase,
                'action': base_signal['action'],
                'strength': base_signal['strength'],
                'position_size': base_signal['position_size'],
                'time_horizon': base_signal['time_horizon'],
                'confidence': confidence,
                'timestamp': time.time(),
                'source': 'on_chain_whale_analysis'
            }
            
            return signal
            
        except Exception as e:
            self.logger.error(f"Phase signal creation failed: {e}")
            return None
            
    def monitor_defi_protocols(self):
        """Monitor DeFi protocol activities"""
        try:
            for protocol_name, protocol_config in self.defi_protocols.items():
                # Get protocol metrics
                protocol_metrics = self.get_protocol_metrics(protocol_name, protocol_config)
                
                if protocol_metrics:
                    # Analyze for unusual activity
                    unusual_activity = self.detect_unusual_defi_activity(protocol_name, protocol_metrics)
                    
                    # Store protocol data
                    self.onchain_metrics['defi_movements'][protocol_name] = {
                        'metrics': protocol_metrics,
                        'unusual_activity': unusual_activity,
                        'timestamp': time.time()
                    }
                    
                    # Generate alerts for significant activity
                    if unusual_activity.get('significance', 0) > 0.7:
                        self.generate_defi_alert(protocol_name, unusual_activity)
                        
        except Exception as e:
            self.logger.error(f"DeFi protocol monitoring failed: {e}")
            
    def get_protocol_metrics(self, protocol_name, protocol_config):
        """Get metrics for specific DeFi protocol (simulated)"""
        try:
            # In production, this would query actual DeFi protocols
            # For demo, simulate protocol metrics
            
            base_tvl = {
                'compound': 8000000000,  # $8B
                'aave': 12000000000,     # $12B
                'uniswap': 6000000000,   # $6B
                'curve': 4000000000      # $4B
            }.get(protocol_name, 1000000000)
            
            # Add some variation
            tvl_change = np.random.uniform(-0.1, 0.1)  # ¬±10%
            current_tvl = base_tvl * (1 + tvl_change)
            
            volume_24h = current_tvl * np.random.uniform(0.05, 0.3)  # 5-30% of TVL
            
            metrics = {
                'total_value_locked': current_tvl,
                'volume_24h': volume_24h,
                'tvl_change_24h': tvl_change,
                'volume_change_24h': np.random.uniform(-0.2, 0.2),
                'unique_users_24h': np.random.randint(1000, 10000),
                'large_transactions_24h': np.random.randint(5, 50),
                'avg_transaction_size': volume_24h / max(1, np.random.randint(100, 1000))
            }
            
            return metrics
            
        except Exception as e:
            self.logger.error(f"Protocol metrics fetching failed: {e}")
            return None
            
    def detect_unusual_defi_activity(self, protocol_name, metrics):
        """Detect unusual DeFi protocol activity"""
        try:
            unusual_indicators = []
            significance_scores = []
            
            # Check TVL changes
            tvl_change = metrics.get('tvl_change_24h', 0)
            if abs(tvl_change) > 0.15:  # >15% change
                unusual_indicators.append('large_tvl_change')
                significance_scores.append(min(1.0, abs(tvl_change) / 0.3))
                
            # Check volume spikes
            volume_change = metrics.get('volume_change_24h', 0)
            if volume_change > 0.5:  # >50% volume increase
                unusual_indicators.append('volume_spike')
                significance_scores.append(min(1.0, volume_change / 1.0))
                
            # Check large transaction activity
            large_txs = metrics.get('large_transactions_24h', 0)
            if large_txs > 30:  # More than 30 large transactions
                unusual_indicators.append('high_whale_activity')
                significance_scores.append(min(1.0, large_txs / 50))
                
            # Calculate overall significance
            overall_significance = np.mean(significance_scores) if significance_scores else 0
            
            return {
                'indicators': unusual_indicators,
                'significance': overall_significance,
                'details': {
                    'tvl_change': tvl_change,
                    'volume_change': volume_change,
                    'large_transactions': large_txs
                }
            }
            
        except Exception as e:
            self.logger.error(f"DeFi unusual activity detection failed: {e}")
            return {}
            
    def generate_defi_alert(self, protocol_name, unusual_activity):
        """Generate alert for unusual DeFi activity"""
        try:
            alert_data = {
                'type': 'defi_unusual_activity',
                'protocol': protocol_name,
                'indicators': unusual_activity.get('indicators', []),
                'significance': unusual_activity.get('significance', 0),
                'details': unusual_activity.get('details', {}),
                'timestamp': time.time()
            }
            
            self.logger.warning(f"üèõÔ∏è DeFi ALERT: Unusual activity in {protocol_name.upper()} - {', '.join(alert_data['indicators'])}")
            
        except Exception as e:
            self.logger.error(f"DeFi alert generation failed: {e}")
            
    def track_defi_movements(self):
        """Track large DeFi token movements"""
        try:
            for protocol_name, protocol_config in self.defi_protocols.items():
                whale_threshold = protocol_config['whale_threshold']
                
                # Get large movements for protocol tokens
                large_movements = self.get_large_defi_movements(protocol_name, whale_threshold)
                
                if large_movements:
                    for movement in large_movements:
                        self.process_defi_movement(protocol_name, movement)
                        
        except Exception as e:
            self.logger.error(f"DeFi movement tracking failed: {e}")
            
    def get_large_defi_movements(self, protocol_name, threshold):
        """Get large movements for DeFi protocol tokens (simulated)"""
        try:
            # Simulate large DeFi movements
            movements = []
            
            if np.random.random() < 0.2:  # 20% chance
                movement_value = np.random.uniform(threshold, threshold * 10)
                
                movements.append({
                    'protocol': protocol_name,
                    'token': np.random.choice(self.defi_protocols[protocol_name]['tokens']),
                    'amount_usd': movement_value,
                    'type': np.random.choice(['mint', 'burn', 'transfer', 'stake', 'unstake']),
                    'from_address': 'defi_whale_address',
                    'to_address': 'protocol_contract',
                    'timestamp': time.time()
                })
                
            return movements
            
        except Exception as e:
            self.logger.error(f"DeFi movement fetching failed: {e}")
            return []
            
    def process_defi_movement(self, protocol_name, movement):
        """Process large DeFi movement"""
        try:
            # Analyze movement implications
            movement_analysis = self.analyze_defi_movement(movement)
            
            # Store movement data
            movement_key = f"{protocol_name}_{movement['timestamp']}"
            self.onchain_metrics['defi_movements'][movement_key] = {
                'movement': movement,
                'analysis': movement_analysis
            }
            
            self.logger.info(f"üèõÔ∏è Large DeFi movement: ${movement['amount_usd']:,.0f} {movement['type']} in {protocol_name}")
            
        except Exception as e:
            self.logger.error(f"DeFi movement processing failed: {e}")
            
    def analyze_defi_movement(self, movement):
        """Analyze DeFi movement implications"""
        try:
            movement_type = movement.get('type', 'unknown')
            amount_usd = movement.get('amount_usd', 0)
            
            # Determine market implications
            implications = {
                'mint': {'signal': 'bullish', 'reason': 'New liquidity/demand'},
                'burn': {'signal': 'bearish', 'reason': 'Reduced supply'},
                'stake': {'signal': 'bullish', 'reason': 'Long-term commitment'},
                'unstake': {'signal': 'bearish', 'reason': 'Exit preparation'},
                'transfer': {'signal': 'neutral', 'reason': 'Movement between wallets'}
            }
            
            implication = implications.get(movement_type, {'signal': 'neutral', 'reason': 'Unknown'})
            
            # Calculate impact score
            impact_score = min(100, (amount_usd / 10000000) * 50)  # $10M = 50 points
            
            return {
                'market_signal': implication['signal'],
                'reasoning': implication['reason'],
                'impact_score': impact_score,
                'magnitude': 'large' if amount_usd > 5000000 else 'medium'
            }
            
        except Exception as e:
            self.logger.error(f"DeFi movement analysis failed: {e}")
            return {}
            
    def analyze_yield_trends(self):
        """Analyze yield farming and staking trends"""
        try:
            yield_data = {}
            
            for protocol_name in self.defi_protocols.keys():
                # Get yield information (simulated)
                protocol_yields = self.get_protocol_yields(protocol_name)
                
                if protocol_yields:
                    yield_data[protocol_name] = protocol_yields
                    
            # Analyze trends across protocols
            yield_analysis = self.analyze_cross_protocol_yields(yield_data)
            
            # Store yield analysis
            self.onchain_metrics['yield_trends'] = {
                'protocol_yields': yield_data,
                'trend_analysis': yield_analysis,
                'timestamp': time.time()
            }
            
        except Exception as e:
            self.logger.error(f"Yield trend analysis failed: {e}")
            
    def get_protocol_yields(self, protocol_name):
        """Get yield information for protocol (simulated)"""
        try:
            # Simulate yield data
            base_yields = {
                'compound': {'lending': 0.03, 'borrowing': 0.05},
                'aave': {'lending': 0.025, 'borrowing': 0.045},
                'uniswap': {'lp_rewards': 0.15, 'trading_fees': 0.02},
                'curve': {'lp_rewards': 0.08, 'crv_rewards': 0.12}
            }
            
            if protocol_name in base_yields:
                yields = base_yields[protocol_name].copy()
                
                # Add some variation
                for key in yields:
                    variation = np.random.uniform(-0.3, 0.3)  # ¬±30% variation
                    yields[key] *= (1 + variation)
                    
                return yields
                
            return None
            
        except Exception as e:
            self.logger.error(f"Protocol yield fetching failed: {e}")
            return None
            
    def analyze_cross_protocol_yields(self, yield_data):
        """Analyze yield trends across protocols"""
        try:
            if not yield_data:
                return {}
                
            # Find highest yields
            all_yields = []
            for protocol, yields in yield_data.items():
                for yield_type, rate in yields.items():
                    all_yields.append({
                        'protocol': protocol,
                        'type': yield_type,
                        'rate': rate
                    })
                    
            # Sort by yield rate
            sorted_yields = sorted(all_yields, key=lambda x: x['rate'], reverse=True)
            
            analysis = {
                'highest_yield': sorted_yields[0] if sorted_yields else None,
                'avg_yield': np.mean([y['rate'] for y in all_yields]) if all_yields else 0,
                'yield_dispersion': np.std([y['rate'] for y in all_yields]) if all_yields else 0,
                'trending_protocols': self.identify_trending_protocols(yield_data)
            }
            
            return analysis
            
        except Exception as e:
            self.logger.error(f"Cross-protocol yield analysis failed: {e}")
            return {}
            
    def identify_trending_protocols(self, yield_data):
        """Identify protocols with trending yields"""
        try:
            trending = []
            
            for protocol, yields in yield_data.items():
                avg_yield = np.mean(list(yields.values()))
                
                if avg_yield > 0.1:  # >10% average yield
                    trending.append({
                        'protocol': protocol,
                        'avg_yield': avg_yield,
			'trend': 'high_yield'
                    })
                    
            return trending
            
        except Exception as e:
            self.logger.error(f"Trending protocol identification failed: {e}")
            return []
            
    def update_network_stats(self):
        """Update general network statistics"""
        try:
            # Update Bitcoin network stats
            btc_stats = self.get_bitcoin_network_stats()
            if btc_stats:
                self.onchain_metrics['network_stats']['bitcoin'] = btc_stats
                
            # Update Ethereum network stats
            eth_stats = self.get_ethereum_network_stats()
            if eth_stats:
                self.onchain_metrics['network_stats']['ethereum'] = eth_stats
                
            # Update BSC network stats
            bsc_stats = self.get_bsc_network_stats()
            if bsc_stats:
                self.onchain_metrics['network_stats']['bsc'] = bsc_stats
                
        except Exception as e:
            self.logger.error(f"Network stats update failed: {e}")
            
    def get_bitcoin_network_stats(self):
        """Get Bitcoin network statistics (simulated)"""
        try:
            # In production, this would query actual Bitcoin network data
            return {
                'block_height': 800000 + np.random.randint(1000),
                'hashrate': np.random.uniform(300, 400) * 1e18,  # EH/s
                'difficulty': np.random.uniform(40, 50) * 1e12,
                'mempool_size': np.random.randint(50000, 200000),
                'avg_block_time': np.random.uniform(9, 11),  # minutes
                'fee_rate': np.random.uniform(10, 100),  # sat/vB
                'active_addresses': np.random.randint(800000, 1200000),
                'timestamp': time.time()
            }
            
        except Exception as e:
            self.logger.error(f"Bitcoin network stats failed: {e}")
            return None
            
    def get_ethereum_network_stats(self):
        """Get Ethereum network statistics (simulated)"""
        try:
            return {
                'block_height': 18000000 + np.random.randint(10000),
                'gas_price': np.random.uniform(20, 100),  # gwei
                'gas_limit': 30000000,
                'gas_used': np.random.uniform(0.7, 0.95) * 30000000,
                'tx_count_24h': np.random.randint(1000000, 1500000),
                'avg_block_time': np.random.uniform(12, 14),  # seconds
                'active_addresses': np.random.randint(600000, 900000),
                'eth_staked': np.random.uniform(25, 30) * 1e6,  # ETH
                'timestamp': time.time()
            }
            
        except Exception as e:
            self.logger.error(f"Ethereum network stats failed: {e}")
            return None
            
    def get_bsc_network_stats(self):
        """Get BSC network statistics (simulated)"""
        try:
            return {
                'block_height': 32000000 + np.random.randint(10000),
                'gas_price': np.random.uniform(5, 20),  # gwei
                'tx_count_24h': np.random.randint(3000000, 5000000),
                'avg_block_time': 3,  # seconds
                'active_addresses': np.random.randint(1000000, 1500000),
                'bnb_burned': np.random.uniform(1000, 5000),  # BNB per day
                'timestamp': time.time()
            }
            
        except Exception as e:
            self.logger.error(f"BSC network stats failed: {e}")
            return None
            
    def correlate_onchain_to_price(self, symbol='BTCUSDT'):
        """Correlate on-chain metrics with price movements"""
        try:
            # Get recent price data
            price_data = self.binance_client.get_klines(symbol, '1h', 24)  # Last 24 hours
            
            if not price_data:
                return None
                
            # Get corresponding on-chain data
            onchain_data = self.get_recent_onchain_activity(24)  # Last 24 hours
            
            # Calculate correlations
            correlations = self.calculate_price_correlations(price_data, onchain_data)
            
            # Store correlation data
            self.impact_correlation['immediate'][symbol] = correlations
            
            return correlations
            
        except Exception as e:
            self.logger.error(f"Price correlation analysis failed: {e}")
            return None
            
    def get_recent_onchain_activity(self, hours):
        """Get on-chain activity for recent hours"""
        try:
            cutoff_time = time.time() - (hours * 3600)
            
            recent_activity = {
                'large_transactions': 0,
                'whale_movements': 0,
                'exchange_flows': 0,
                'defi_activity': 0,
                'total_volume_usd': 0
            }
            
            # Count large transactions
            for tx_data in self.onchain_metrics['large_transactions'].values():
                if isinstance(tx_data, dict) and tx_data.get('timestamp', 0) > cutoff_time:
                    recent_activity['large_transactions'] += 1
                    recent_activity['total_volume_usd'] += tx_data.get('usd_value', 0)
                    
            # Count whale movements
            for whale_movements in self.onchain_metrics['whale_movements'].values():
                recent_movements = [m for m in whale_movements if m.get('timestamp', 0) > cutoff_time]
                recent_activity['whale_movements'] += len(recent_movements)
                
            # Count exchange flows
            for flow_data in self.onchain_metrics['exchange_flows'].values():
                if isinstance(flow_data, dict) and flow_data.get('timestamp', 0) > cutoff_time:
                    recent_activity['exchange_flows'] += 1
                    
            return recent_activity
            
        except Exception as e:
            self.logger.error(f"Recent on-chain activity retrieval failed: {e}")
            return {}
            
    def calculate_price_correlations(self, price_data, onchain_data):
        """Calculate correlations between price and on-chain metrics"""
        try:
            if not price_data or not onchain_data:
                return {}
                
            # Extract price changes
            price_changes = []
            for i in range(1, len(price_data)):
                prev_close = float(price_data[i-1][4])
                curr_close = float(price_data[i][4])
                price_change = (curr_close - prev_close) / prev_close
                price_changes.append(price_change)
                
            # Create correlation metrics
            correlations = {
                'large_tx_correlation': 0,
                'whale_correlation': 0,
                'exchange_flow_correlation': 0,
                'volume_correlation': 0,
                'overall_correlation': 0
            }
            
            # Simple correlation calculation (in production, use proper statistical methods)
            large_tx_count = onchain_data.get('large_transactions', 0)
            whale_count = onchain_data.get('whale_movements', 0)
            
            if large_tx_count > 0 and price_changes:
                avg_price_change = np.mean(price_changes)
                correlations['large_tx_correlation'] = min(1.0, abs(avg_price_change) * large_tx_count * 10)
                
            if whale_count > 0 and price_changes:
                avg_price_change = np.mean(price_changes)
                correlations['whale_correlation'] = min(1.0, abs(avg_price_change) * whale_count * 15)
                
            # Overall correlation
            correlations['overall_correlation'] = np.mean(list(correlations.values()))
            
            return correlations
            
        except Exception as e:
            self.logger.error(f"Price correlation calculation failed: {e}")
            return {}
            
    def generate_onchain_trading_signals(self):
        """Generate trading signals from on-chain analysis"""
        try:
            signals = []
            
            # Whale-based signals
            whale_signal = self.generate_whale_signals()
            if whale_signal:
                signals.append(whale_signal)
                
            # Exchange flow signals
            flow_signal = self.generate_exchange_flow_signals()
            if flow_signal:
                signals.append(flow_signal)
                
            # DeFi signals
            defi_signal = self.generate_defi_signals()
            if defi_signal:
                signals.append(defi_signal)
                
            # Network congestion signals
            congestion_signal = self.generate_congestion_signals()
            if congestion_signal:
                signals.append(congestion_signal)
                
            # Combine signals
            combined_signal = self.combine_onchain_signals(signals)
            
            if combined_signal:
                self.logger.info(f"üîó ON-CHAIN SIGNAL: {combined_signal['action'].upper()} (confidence: {combined_signal['confidence']:.1%})")
                
            return combined_signal
            
        except Exception as e:
            self.logger.error(f"On-chain trading signal generation failed: {e}")
            return None
            
    def generate_exchange_flow_signals(self):
        """Generate signals from exchange flow analysis"""
        try:
            if not self.onchain_metrics['exchange_flows']:
                return None
                
            # Analyze recent exchange flows
            total_inflow = 0
            total_outflow = 0
            
            for flow_data in self.onchain_metrics['exchange_flows'].values():
                if isinstance(flow_data, dict):
                    net_flow = flow_data.get('net_flow', 0)
                    
                    if net_flow > 0:
                        total_outflow += net_flow
                    else:
                        total_inflow += abs(net_flow)
                        
            net_flow = total_outflow - total_inflow
            
            # Generate signal based on net flow
            if abs(net_flow) > 10000000:  # $10M threshold
                if net_flow > 0:
                    # Net outflow = bullish
                    return {
                        'type': 'exchange_flow',
                        'action': 'buy',
                        'strength': 'moderate',
                        'confidence': min(0.8, abs(net_flow) / 50000000),
                        'reasoning': 'Large exchange outflows detected'
                    }
                else:
                    # Net inflow = bearish
                    return {
                        'type': 'exchange_flow',
                        'action': 'sell',
                        'strength': 'moderate',
                        'confidence': min(0.8, abs(net_flow) / 50000000),
                        'reasoning': 'Large exchange inflows detected'
                    }
                    
            return None
            
        except Exception as e:
            self.logger.error(f"Exchange flow signal generation failed: {e}")
            return None
            
    def generate_defi_signals(self):
        """Generate signals from DeFi analysis"""
        try:
            if not self.onchain_metrics['defi_movements']:
                return None
                
            # Analyze recent DeFi activity
            recent_defi_activity = []
            cutoff_time = time.time() - 3600  # Last hour
            
            for movement_data in self.onchain_metrics['defi_movements'].values():
                if isinstance(movement_data, dict):
                    movement = movement_data.get('movement', {})
                    if movement.get('timestamp', 0) > cutoff_time:
                        recent_defi_activity.append(movement)
                        
            if not recent_defi_activity:
                return None
                
            # Analyze activity types
            bullish_activity = sum(1 for m in recent_defi_activity if m.get('type') in ['mint', 'stake'])
            bearish_activity = sum(1 for m in recent_defi_activity if m.get('type') in ['burn', 'unstake'])
            
            total_activity = len(recent_defi_activity)
            
            if total_activity >= 3:  # Minimum activity threshold
                if bullish_activity > bearish_activity * 1.5:
                    return {
                        'type': 'defi_activity',
                        'action': 'buy',
                        'strength': 'weak',
                        'confidence': min(0.6, bullish_activity / total_activity),
                        'reasoning': f'Increased DeFi bullish activity ({bullish_activity} vs {bearish_activity})'
                    }
                elif bearish_activity > bullish_activity * 1.5:
                    return {
                        'type': 'defi_activity',
                        'action': 'sell',
                        'strength': 'weak',
                        'confidence': min(0.6, bearish_activity / total_activity),
                        'reasoning': f'Increased DeFi bearish activity ({bearish_activity} vs {bullish_activity})'
                    }
                    
            return None
            
        except Exception as e:
            self.logger.error(f"DeFi signal generation failed: {e}")
            return None
            
    def generate_congestion_signals(self):
        """Generate signals from network congestion analysis"""
        try:
            # Analyze Ethereum gas prices as congestion indicator
            eth_stats = self.onchain_metrics['network_stats'].get('ethereum', {})
            
            if not eth_stats:
                return None
                
            gas_price = eth_stats.get('gas_price', 50)
            gas_used_pct = eth_stats.get('gas_used', 0) / eth_stats.get('gas_limit', 1)
            
            # High congestion often correlates with high activity/FOMO
            if gas_price > 100 and gas_used_pct > 0.9:  # High gas price and usage
                return {
                    'type': 'network_congestion',
                    'action': 'sell',
                    'strength': 'weak',
                    'confidence': 0.4,
                    'reasoning': 'High network congestion suggests FOMO peak'
                }
            elif gas_price < 30 and gas_used_pct < 0.5:  # Low activity
                return {
                    'type': 'network_congestion',
                    'action': 'buy',
                    'strength': 'weak',
                    'confidence': 0.3,
                    'reasoning': 'Low network activity suggests accumulation opportunity'
                }
                
            return None
            
        except Exception as e:
            self.logger.error(f"Congestion signal generation failed: {e}")
            return None
            
    def combine_onchain_signals(self, signals):
        """Combine multiple on-chain signals into unified signal"""
        try:
            if not signals:
                return None
                
            # Separate buy and sell signals
            buy_signals = [s for s in signals if s.get('action') == 'buy']
            sell_signals = [s for s in signals if s.get('action') == 'sell']
            
            # Calculate weighted scores
            buy_score = sum(s.get('confidence', 0) for s in buy_signals)
            sell_score = sum(s.get('confidence', 0) for s in sell_signals)
            
            # Determine dominant signal
            if buy_score > sell_score and buy_score > 0.5:
                action = 'buy'
                confidence = min(0.9, buy_score / len(signals))
                strength = 'strong' if confidence > 0.7 else 'moderate' if confidence > 0.4 else 'weak'
                
            elif sell_score > buy_score and sell_score > 0.5:
                action = 'sell'
                confidence = min(0.9, sell_score / len(signals))
                strength = 'strong' if confidence > 0.7 else 'moderate' if confidence > 0.4 else 'weak'
                
            else:
                action = 'hold'
                confidence = 0.3
                strength = 'weak'
                
            # Combine reasoning
            reasoning_parts = [s.get('reasoning', '') for s in signals if s.get('reasoning')]
            combined_reasoning = '; '.join(reasoning_parts)
            
            return {
                'type': 'combined_onchain',
                'action': action,
                'strength': strength,
                'confidence': confidence,
                'reasoning': combined_reasoning,
                'component_signals': len(signals),
                'timestamp': time.time()
            }
            
        except Exception as e:
            self.logger.error(f"Signal combination failed: {e}")
            return None
            
    def get_onchain_summary(self):
        """Get comprehensive on-chain analysis summary"""
        try:
            return {
                'whale_activity': {
                    'tracked_addresses': len(self.whale_config['tracking_addresses']['known_whales']),
                    'recent_movements': len([
                        m for movements in self.onchain_metrics['whale_movements'].values()
                        for m in movements
                        if time.time() - m.get('timestamp', 0) < 86400
                    ]),
                    'market_phase': self.onchain_metrics.get('market_phase', {}).get('phase', 'unknown')
                },
                'exchange_activity': {
                    'monitored_exchanges': len(self.whale_config['tracking_addresses']['exchanges']),
                    'recent_flows': len([
                        f for f in self.onchain_metrics['exchange_flows'].values()
                        if isinstance(f, dict) and time.time() - f.get('timestamp', 0) < 86400
                    ])
                },
                'defi_activity': {
                    'monitored_protocols': len(self.defi_protocols),
                    'recent_movements': len([
                        m for m in self.onchain_metrics['defi_movements'].values()
                        if isinstance(m, dict) and time.time() - m.get('timestamp', 0) < 86400
                    ])
                },
                'large_transactions': {
                    'last_24h': len([
                        tx for tx in self.onchain_metrics['large_transactions'].values()
                        if isinstance(tx, dict) and time.time() - tx.get('timestamp', 0) < 86400
                    ])
                },
                'network_health': {
                    'bitcoin': self.onchain_metrics['network_stats'].get('bitcoin', {}),
                    'ethereum': self.onchain_metrics['network_stats'].get('ethereum', {}),
                    'bsc': self.onchain_metrics['network_stats'].get('bsc', {})
                },
                'data_sources': {
                    'active_sources': len([s for s in self.data_sources.values() if s['enabled']]),
                    'last_update': time.time()
                }
            }
            
        except Exception as e:
            self.logger.error(f"On-chain summary generation failed: {e}")
            return {}
            
    def export_onchain_data(self, filename=None):
        """Export on-chain analysis data"""
        try:
            if filename is None:
                filename = f"onchain_data_{int(time.time())}.json"
                
            export_data = {
                'export_timestamp': time.time(),
                'whale_config': self.whale_config,
                'onchain_metrics': dict(self.onchain_metrics),
                'defi_protocols': self.defi_protocols,
                'alert_thresholds': self.alert_thresholds,
                'summary': self.get_onchain_summary()
            }
            
            # Store via memory manager
            self.memory_manager.export_onchain_data(export_data, filename)
            
            self.logger.info(f"üìÅ On-chain data exported: {filename}")
            return filename
            
        except Exception as e:
            self.logger.error(f"On-chain data export failed: {e}")
            return None
            
    def cleanup_old_onchain_data(self, days_to_keep=7):
        """Clean up old on-chain data to manage memory"""
        try:
            cutoff_time = time.time() - (days_to_keep * 24 * 3600)
            
            # Clean large transactions
            old_tx_keys = [
                key for key, tx in self.onchain_metrics['large_transactions'].items()
                if isinstance(tx, dict) and tx.get('timestamp', time.time()) < cutoff_time
            ]
            
            for key in old_tx_keys:
                del self.onchain_metrics['large_transactions'][key]
                
            # Clean whale movements
            for whale_addr in self.onchain_metrics['whale_movements']:
                self.onchain_metrics['whale_movements'][whale_addr] = [
                    m for m in self.onchain_metrics['whale_movements'][whale_addr]
                    if m.get('timestamp', time.time()) > cutoff_time
                ]
                
            # Clean exchange flows
            old_flow_keys = [
                key for key, flow in self.onchain_metrics['exchange_flows'].items()
                if isinstance(flow, dict) and flow.get('timestamp', time.time()) < cutoff_time
            ]
            
            for key in old_flow_keys:
                del self.onchain_metrics['exchange_flows'][key]
                
            removed_count = len(old_tx_keys) + len(old_flow_keys)
            
            if removed_count > 0:
                self.logger.info(f"üßπ Cleaned up {removed_count} old on-chain records")
                
        except Exception as e:
            self.logger.error(f"On-chain data cleanup failed: {e}")
            
    def get_whale_leaderboard(self):
        """Get leaderboard of most active whales"""
        try:
            whale_activity_scores = {}
            
            for whale_addr, movements in self.onchain_metrics['whale_movements'].items():
                # Calculate activity score for last 7 days
                recent_movements = [
                    m for m in movements
                    if time.time() - m.get('timestamp', 0) < 7 * 24 * 3600
                ]
                
                if recent_movements:
                    total_significance = sum(
                        m.get('behavior_pattern', {}).get('significance', 0)
                        for m in recent_movements
                    )
                    
                    whale_activity_scores[whale_addr] = {
                        'movements': len(recent_movements),
                        'total_significance': total_significance,
                        'avg_significance': total_significance / len(recent_movements),
                        'activity_score': total_significance * len(recent_movements)
                    }
                    
            # Sort by activity score
            leaderboard = sorted(
                whale_activity_scores.items(),
                key=lambda x: x[1]['activity_score'],
                reverse=True
            )
            
            return leaderboard[:10]  # Top 10 whales
            
        except Exception as e:
            self.logger.error(f"Whale leaderboard generation failed: {e}")
            return []

class CustomIndicatorsEngine:
    """Advanced custom indicators engine with adaptive signal generation and pattern recognition"""
    
    def __init__(self, config, binance_client, memory_manager, performance_analytics):
        self.config = config
        self.binance_client = binance_client
        self.memory_manager = memory_manager
        self.performance_analytics = performance_analytics
        self.logger = logging.getLogger('MonsterBot.CustomIndicators')
        
        # Custom indicator configurations
        self.indicator_configs = {
            'monster_momentum': {
                'description': 'Proprietary momentum indicator combining volume, price action, and volatility',
                'lookback_period': 20,
                'sensitivity': 'high',
                'weight': 1.5,
                'enabled': True,
                'parameters': {
                    'volume_weight': 0.4,
                    'price_weight': 0.4,
                    'volatility_weight': 0.2,
                    'smoothing_factor': 0.3
                }
            },
            'whale_pressure': {
                'description': 'Detects institutional buying/selling pressure from order flow',
                'lookback_period': 10,
                'sensitivity': 'medium',
                'weight': 1.3,
                'enabled': True,
                'parameters': {
                    'large_order_threshold': 100000,  # $100k+ orders
                    'pressure_smoothing': 5,
                    'imbalance_threshold': 0.7
                }
            },
            'breakout_velocity': {
                'description': 'Measures the speed and conviction of breakout moves',
                'lookback_period': 15,
                'sensitivity': 'high',
                'weight': 1.4,
                'enabled': True,
                'parameters': {
                    'velocity_threshold': 0.02,
                    'volume_confirmation': 1.5,
                    'time_decay': 0.1
                }
            },
            'liquidity_zones': {
                'description': 'Identifies high-liquidity support/resistance zones',
                'lookback_period': 50,
                'sensitivity': 'low',
                'weight': 1.1,
                'enabled': True,
                'parameters': {
                    'zone_strength_min': 3,
                    'price_tolerance': 0.005,
                    'volume_cluster_factor': 2.0
                }
            },
            'smart_money_flow': {
                'description': 'Tracks institutional money flow patterns',
                'lookback_period': 30,
                'sensitivity': 'medium',
                'weight': 1.2,
                'enabled': True,
                'parameters': {
                    'flow_sensitivity': 0.8,
                    'accumulation_threshold': 0.6,
                    'distribution_threshold': -0.6
                }
            },
            'volatility_squeeze': {
                'description': 'Detects low volatility periods before explosive moves',
                'lookback_period': 20,
                'sensitivity': 'medium',
                'weight': 1.0,
                'enabled': True,
                'parameters': {
                    'squeeze_threshold': 0.5,
                    'expansion_multiplier': 2.0,
                    'confirmation_periods': 3
                }
            },
            'market_structure': {
                'description': 'Analyzes market structure breaks and continuations',
                'lookback_period': 25,
                'sensitivity': 'high',
                'weight': 1.3,
                'enabled': True,
                'parameters': {
                    'structure_sensitivity': 0.015,
                    'confirmation_strength': 2,
                    'invalidation_distance': 0.02
                }
            },
            'fear_greed_oscillator': {
                'description': 'Custom fear/greed indicator based on multiple metrics',
                'lookback_period': 14,
                'sensitivity': 'medium',
                'weight': 0.9,
                'enabled': True,
                'parameters': {
                    'extreme_threshold': 80,
                    'reversal_sensitivity': 0.7,
                    'momentum_factor': 0.3
                }
            }
        }
        
        # Pattern recognition systems
        self.pattern_recognition = {
            'harmonic_patterns': {
                'enabled': True,
                'patterns': ['gartley', 'butterfly', 'bat', 'crab'],
                'accuracy_threshold': 0.8,
                'completion_tolerance': 0.02
            },
            'chart_patterns': {
                'enabled': True,
                'patterns': ['head_shoulders', 'double_top', 'double_bottom', 'triangles', 'flags'],
                'min_pattern_length': 10,
                'breakout_confirmation': 0.015
            },
            'candlestick_patterns': {
                'enabled': True,
                'patterns': ['doji', 'hammer', 'shooting_star', 'engulfing', 'piercing'],
                'significance_filter': 0.6
            },
            'volume_patterns': {
                'enabled': True,
                'patterns': ['accumulation', 'distribution', 'climax', 'dry_up'],
                'volume_threshold': 1.5
            }
        }
        
        # Adaptive signal generation
        self.signal_adaptation = {
            'learning_enabled': True,
            'performance_window': 100,  # trades
            'adaptation_rate': 0.1,
            'confidence_threshold': 0.6,
            'signal_decay_rate': 0.05
        }
        
        # Indicator calculation cache
        self.indicator_cache = defaultdict(dict)
        self.pattern_cache = defaultdict(list)
        self.signal_history = defaultdict(list)
        
        # Performance tracking per indicator
        self.indicator_performance = defaultdict(lambda: {
            'total_signals': 0,
            'successful_signals': 0,
            'total_return': 0,
            'avg_signal_strength': 0,
            'last_performance_update': time.time()
        })
        
        # Market regime adaptation
        self.market_regimes = {
            'trending_bull': {'volatility': 'medium', 'direction': 'up', 'strength': 'strong'},
            'trending_bear': {'volatility': 'medium', 'direction': 'down', 'strength': 'strong'},
            'ranging_high_vol': {'volatility': 'high', 'direction': 'sideways', 'strength': 'weak'},
            'ranging_low_vol': {'volatility': 'low', 'direction': 'sideways', 'strength': 'weak'},
            'breakout': {'volatility': 'high', 'direction': 'uncertain', 'strength': 'very_strong'}
        }
        
        # Initialize custom indicators system
        self.initialize_indicators_system()
        
    def initialize_indicators_system(self):
        """Initialize custom indicators system"""
        try:
            # Load indicator performance history
            self.load_indicator_performance()
            
            # Start real-time indicator calculations
            self.start_indicator_calculations()
            
            # Start pattern recognition
            self.start_pattern_recognition()
            
            # Start adaptive learning
            self.start_adaptive_learning()
            
            self.logger.info("üîÆ Custom Indicators Engine initialized")
            self.logger.info(f"   üìä Custom indicators: {len([i for i in self.indicator_configs.values() if i['enabled']])}")
            self.logger.info(f"   üéØ Pattern types: {sum(len(p['patterns']) for p in self.pattern_recognition.values() if p['enabled'])}")
            self.logger.info(f"   üß† Adaptive learning: {'Enabled' if self.signal_adaptation['learning_enabled'] else 'Disabled'}")
            
        except Exception as e:
            self.logger.error(f"Custom indicators initialization failed: {e}")
            
    def load_indicator_performance(self):
        """Load historical indicator performance data"""
        try:
            performance_data = self.memory_manager.get_indicator_performance()
            
            if performance_data:
                self.indicator_performance.update(performance_data)
                self.logger.info(f"üìö Loaded performance data for {len(performance_data)} indicators")
            else:
                self.logger.info("üÜï Starting fresh indicator performance tracking")
                
        except Exception as e:
            self.logger.error(f"Indicator performance loading failed: {e}")
            
    def start_indicator_calculations(self):
        """Start real-time indicator calculations"""
        def indicator_calculator():
            while True:
                try:
                    # Calculate indicators for major symbols
                    symbols = ['BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'ADAUSDT', 'SOLUSDT']
                    
                    for symbol in symbols:
                        self.calculate_all_indicators(symbol)
                        
                    time.sleep(30)  # Update every 30 seconds
                except Exception as e:
                    self.logger.error(f"Indicator calculation error: {e}")
                    time.sleep(60)
                    
        threading.Thread(target=indicator_calculator, daemon=True).start()
        
    def start_pattern_recognition(self):
        """Start pattern recognition system"""
        def pattern_recognizer():
            while True:
                try:
                    # Recognize patterns for major symbols
                    symbols = ['BTCUSDT', 'ETHUSDT', 'BNBUSDT']
                    
                    for symbol in symbols:
                        self.recognize_all_patterns(symbol)
                        
                    time.sleep(60)  # Update every minute
                except Exception as e:
                    self.logger.error(f"Pattern recognition error: {e}")
                    time.sleep(120)
                    
        threading.Thread(target=pattern_recognizer, daemon=True).start()
        
    def start_adaptive_learning(self):
        """Start adaptive learning system"""
        def adaptive_learner():
            while True:
                try:
                    if self.signal_adaptation['learning_enabled']:
                        self.adapt_indicator_parameters()
                        
                    time.sleep(300)  # Update every 5 minutes
                except Exception as e:
                    self.logger.error(f"Adaptive learning error: {e}")
                    time.sleep(600)
                    
        threading.Thread(target=adaptive_learner, daemon=True).start()
        
    def calculate_all_indicators(self, symbol):
        """Calculate all enabled custom indicators for a symbol"""
        try:
            # Get market data
            klines = self.binance_client.get_klines(symbol, '5m', 100)
            
            if not klines or len(klines) < 50:
                return
                
            # Convert to structured data
            ohlcv_data = self.prepare_ohlcv_data(klines)
            
            # Calculate each enabled indicator
            for indicator_name, config in self.indicator_configs.items():
                if config['enabled']:
                    try:
                        indicator_value = self.calculate_indicator(
                            indicator_name, ohlcv_data, config
                        )
                        
                        if indicator_value is not None:
                            # Store in cache
                            self.indicator_cache[symbol][indicator_name] = {
                                'value': indicator_value,
                                'timestamp': time.time(),
                                'signal_strength': self.calculate_signal_strength(indicator_value, config)
                            }
                            
                    except Exception as e:
                        self.logger.debug(f"Indicator {indicator_name} calculation failed for {symbol}: {e}")
                        
        except Exception as e:
            self.logger.error(f"Indicator calculations failed for {symbol}: {e}")
            
    def prepare_ohlcv_data(self, klines):
        """Prepare OHLCV data structure from klines"""
        try:
            ohlcv = {
                'open': [],
                'high': [],
                'low': [],
                'close': [],
                'volume': [],
                'timestamp': []
            }
            
            for kline in klines:
                ohlcv['timestamp'].append(int(kline[0]))
                ohlcv['open'].append(float(kline[1]))
                ohlcv['high'].append(float(kline[2]))
                ohlcv['low'].append(float(kline[3]))
                ohlcv['close'].append(float(kline[4]))
                ohlcv['volume'].append(float(kline[5]))
                
            return ohlcv
            
        except Exception as e:
            self.logger.error(f"OHLCV data preparation failed: {e}")
            return None
            
    def calculate_indicator(self, indicator_name, ohlcv_data, config):
        """Calculate specific custom indicator"""
        try:
            if indicator_name == 'monster_momentum':
                return self.calculate_monster_momentum(ohlcv_data, config)
            elif indicator_name == 'whale_pressure':
                return self.calculate_whale_pressure(ohlcv_data, config)
            elif indicator_name == 'breakout_velocity':
                return self.calculate_breakout_velocity(ohlcv_data, config)
            elif indicator_name == 'liquidity_zones':
                return self.calculate_liquidity_zones(ohlcv_data, config)
            elif indicator_name == 'smart_money_flow':
                return self.calculate_smart_money_flow(ohlcv_data, config)
            elif indicator_name == 'volatility_squeeze':
                return self.calculate_volatility_squeeze(ohlcv_data, config)
            elif indicator_name == 'market_structure':
                return self.calculate_market_structure(ohlcv_data, config)
            elif indicator_name == 'fear_greed_oscillator':
                return self.calculate_fear_greed_oscillator(ohlcv_data, config)
            else:
                return None
                
        except Exception as e:
            self.logger.error(f"Indicator {indicator_name} calculation failed: {e}")
            return None
            
    def calculate_monster_momentum(self, ohlcv, config):
        """Calculate proprietary Monster Momentum indicator"""
        try:
            closes = np.array(ohlcv['close'])
            volumes = np.array(ohlcv['volume'])
            highs = np.array(ohlcv['high'])
            lows = np.array(ohlcv['low'])
            
            period = config['lookback_period']
            params = config['parameters']
            
            if len(closes) < period:
                return None
                
            # Price momentum component
            price_changes = np.diff(closes)
            price_momentum = np.mean(price_changes[-period:])
            
            # Volume momentum component
            avg_volume = np.mean(volumes[-period:])
            recent_volume = np.mean(volumes[-5:])
            volume_momentum = (recent_volume / avg_volume - 1) if avg_volume > 0 else 0
            
            # Volatility component (True Range based)
            true_ranges = []
            for i in range(1, len(closes)):
                tr = max(
                    highs[i] - lows[i],
                    abs(highs[i] - closes[i-1]),
                    abs(lows[i] - closes[i-1])
                )
                true_ranges.append(tr)
                
            avg_tr = np.mean(true_ranges[-period:])
            recent_tr = np.mean(true_ranges[-5:])
            volatility_momentum = (recent_tr / avg_tr - 1) if avg_tr > 0 else 0
            
            # Weighted combination
            monster_momentum = (
                price_momentum * params['price_weight'] +
                volume_momentum * params['volume_weight'] +
                volatility_momentum * params['volatility_weight']
            )
            
            # Apply smoothing
            smoothing = params['smoothing_factor']
            if hasattr(self, '_last_monster_momentum'):
                monster_momentum = (smoothing * self._last_monster_momentum + 
                                  (1 - smoothing) * monster_momentum)
                                  
            self._last_monster_momentum = monster_momentum
            
            # Normalize to -100 to 100 scale
            normalized_momentum = np.tanh(monster_momentum * 100) * 100
            
            return {
                'value': normalized_momentum,
                'components': {
                    'price_momentum': price_momentum,
                    'volume_momentum': volume_momentum,
                    'volatility_momentum': volatility_momentum
                },
                'signal': 'bullish' if normalized_momentum > 20 else 'bearish' if normalized_momentum < -20 else 'neutral'
            }
            
        except Exception as e:
            self.logger.error(f"Monster Momentum calculation failed: {e}")
            return None
            
    def calculate_whale_pressure(self, ohlcv, config):
        """Calculate Whale Pressure indicator from order flow analysis"""
        try:
            closes = np.array(ohlcv['close'])
            volumes = np.array(ohlcv['volume'])
            highs = np.array(ohlcv['high'])
            lows = np.array(ohlcv['low'])
            
            period = config['lookback_period']
            params = config['parameters']
            
            if len(closes) < period:
                return None
                
            # Simulate order flow data (in production, use actual order book data)
            buy_pressure = 0
            sell_pressure = 0
            
            for i in range(-period, 0):
                # Estimate buying vs selling pressure from price/volume
                price_change = closes[i] - closes[i-1] if i > -len(closes) else 0
                volume = volumes[i]
                
                # High volume + price up = buying pressure
                if price_change > 0:
                    buy_pressure += volume * abs(price_change)
                elif price_change < 0:
                    sell_pressure += volume * abs(price_change)
                    
            # Calculate pressure imbalance
            total_pressure = buy_pressure + sell_pressure
            
            if total_pressure > 0:
                pressure_imbalance = (buy_pressure - sell_pressure) / total_pressure
            else:
                pressure_imbalance = 0
                
            # Apply smoothing
            smoothing_periods = params['pressure_smoothing']
            if hasattr(self, '_whale_pressure_history'):
                self._whale_pressure_history.append(pressure_imbalance)
                if len(self._whale_pressure_history) > smoothing_periods:
                    self._whale_pressure_history.pop(0)
                    
                smoothed_pressure = np.mean(self._whale_pressure_history)
            else:
                self._whale_pressure_history = [pressure_imbalance]
                smoothed_pressure = pressure_imbalance
                
            # Determine signal strength
            threshold = params['imbalance_threshold']
            
            if smoothed_pressure > threshold:
                signal = 'strong_buying'
                strength = min(1.0, abs(smoothed_pressure) / threshold)
            elif smoothed_pressure < -threshold:
                signal = 'strong_selling'
                strength = min(1.0, abs(smoothed_pressure) / threshold)
            else:
                signal = 'balanced'
                strength = 0.5
                
            return {
                'value': smoothed_pressure * 100,  # Scale to -100 to 100
                'signal': signal,
                'strength': strength,
                'buy_pressure': buy_pressure,
                'sell_pressure': sell_pressure
            }
            
        except Exception as e:
            self.logger.error(f"Whale Pressure calculation failed: {e}")
            return None
            
    def calculate_breakout_velocity(self, ohlcv, config):
        """Calculate Breakout Velocity indicator"""
        try:
            closes = np.array(ohlcv['close'])
            volumes = np.array(ohlcv['volume'])
            highs = np.array(ohlcv['high'])
            lows = np.array(ohlcv['low'])
            
            period = config['lookback_period']
            params = config['parameters']
            
            if len(closes) < period:
                return None
                
            # Calculate recent high/low levels
            recent_high = np.max(highs[-period:])
            recent_low = np.min(lows[-period:])
            current_price = closes[-1]
            
            # Calculate velocity (rate of price change)
            price_changes = np.diff(closes[-5:])  # Last 5 periods
            velocity = np.mean(price_changes) / closes[-1] if closes[-1] > 0 else 0
            
            # Volume confirmation
            avg_volume = np.mean(volumes[-period:])
            recent_volume = np.mean(volumes[-3:])
            volume_ratio = recent_volume / avg_volume if avg_volume > 0 else 1
            
            # Breakout detection
            breakout_threshold = params['velocity_threshold']
            volume_confirmation = params['volume_confirmation']
            
            is_breakout = False
            breakout_direction = 'none'
            
            # Upward breakout
            if (current_price > recent_high * 1.001 and  # Above recent high
                velocity > breakout_threshold and
                volume_ratio > volume_confirmation):
                is_breakout = True
                breakout_direction = 'upward'
                
            # Downward breakout
            elif (current_price < recent_low * 0.999 and  # Below recent low
                  velocity < -breakout_threshold and
                  volume_ratio > volume_confirmation):
                is_breakout = True
                breakout_direction = 'downward'
                
            # Calculate breakout strength
            if is_breakout:
                velocity_strength = min(1.0, abs(velocity) / (breakout_threshold * 2))
                volume_strength = min(1.0, (volume_ratio - 1) / volume_confirmation)
                breakout_strength = (velocity_strength + volume_strength) / 2
            else:
                breakout_strength = 0
                
            return {
                'value': velocity * 10000,  # Scale for readability
                'is_breakout': is_breakout,
                'direction': breakout_direction,
                'strength': breakout_strength,
                'volume_confirmation': volume_ratio > volume_confirmation,
                'velocity': velocity
            }
            
        except Exception as e:
            self.logger.error(f"Breakout Velocity calculation failed: {e}")
            return None
            
    def calculate_liquidity_zones(self, ohlcv, config):
        """Calculate Liquidity Zones indicator"""
        try:
            closes = np.array(ohlcv['close'])
            volumes = np.array(ohlcv['volume'])
            highs = np.array(ohlcv['high'])
            lows = np.array(ohlcv['low'])
            
            period = config['lookback_period']
            params = config['parameters']
            
            if len(closes) < period:
                return None
                
            # Find volume clusters (high volume areas = liquidity zones)
            price_volume_pairs = list(zip(closes[-period:], volumes[-period:]))
            
            # Group prices into zones
            zones = []
            tolerance = params['price_tolerance']
            
            for i, (price, volume) in enumerate(price_volume_pairs):
                # Find existing zone or create new one
                zone_found = False
                
                for zone in zones:
                    zone_center = zone['price_center']
                    if abs(price - zone_center) / zone_center <= tolerance:
                        # Add to existing zone
                        zone['volumes'].append(volume)
                        zone['prices'].append(price)
                        zone['total_volume'] += volume
                        zone_found = True
                        break
                        
                if not zone_found:
                    # Create new zone
                    zones.append({
                        'price_center': price,
                        'prices': [price],
                        'volumes': [volume],
                        'total_volume': volume,
                        'touches': 1
                    })
                    
            # Calculate zone strength and identify significant zones
            significant_zones = []
            avg_volume = np.mean(volumes[-period:])
            
            for zone in zones:
                zone_strength = zone['total_volume'] / avg_volume
                zone['strength'] = zone_strength
                zone['price_center'] = np.mean(zone['prices'])
                
                if (zone_strength >= params['volume_cluster_factor'] and
                    len(zone['volumes']) >= params['zone_strength_min']):
                    significant_zones.append(zone)
                    
            # Sort zones by strength
            significant_zones.sort(key=lambda x: x['strength'], reverse=True)
            
            # Determine current price position relative to zones
            current_price = closes[-1]
            nearest_support = None
            nearest_resistance = None
            
            for zone in significant_zones:
                zone_price = zone['price_center']
                
                if zone_price < current_price:
                    if nearest_support is None or zone_price > nearest_support['price_center']:
                        nearest_support = zone
                elif zone_price > current_price:
                    if nearest_resistance is None or zone_price < nearest_resistance['price_center']:
                        nearest_resistance = zone
                        
            return {
                'significant_zones': significant_zones[:5],  # Top 5 zones
                'nearest_support': nearest_support,
                'nearest_resistance': nearest_resistance,
                'current_price': current_price,
                'zone_count': len(significant_zones)
            }
            
        except Exception as e:
            self.logger.error(f"Liquidity Zones calculation failed: {e}")
            return None
            
    def calculate_smart_money_flow(self, ohlcv, config):
        """Calculate Smart Money Flow indicator"""
        try:
            closes = np.array(ohlcv['close'])
            volumes = np.array(ohlcv['volume'])
            highs = np.array(ohlcv['high'])
            lows = np.array(ohlcv['low'])
            
            period = config['lookback_period']
            params = config['parameters']
            
            if len(closes) < period:
                return None
                
            # Calculate Money Flow Index components
            typical_prices = (highs + lows + closes) / 3
            money_flows = typical_prices * volumes
            
            # Separate positive and negative money flows
            positive_flows = []
            negative_flows = []
            
            for i in range(1, len(typical_prices)):
                if typical_prices[i] > typical_prices[i-1]:
                    positive_flows.append(money_flows[i])
                    negative_flows.append(0)
                elif typical_prices[i] < typical_prices[i-1]:
                    positive_flows.append(0)
                    negative_flows.append(money_flows[i])
                else:
                    positive_flows.append(0)
                    negative_flows.append(0)
                    
            # Calculate money flow over period
            positive_sum = np.sum(positive_flows[-period:])
            negative_sum = np.sum(negative_flows[-period:])
            
            if negative_sum == 0:
                money_flow_ratio = 100
            else:
                money_flow_ratio = positive_sum / negative_sum
                
            # Calculate Smart Money Index (0-100)
            smart_money_index = 100 - (100 / (1 + money_flow_ratio))
            
            # Detect accumulation/distribution phases
            flow_sensitivity = params['flow_sensitivity']
            
            if smart_money_index > (50 + 50 * params['accumulation_threshold']):
                phase = 'accumulation'
                strength = (smart_money_index - 50) / 50
            elif smart_money_index < (50 + 50 * params['distribution_threshold']):
                phase = 'distribution'
                strength = (50 - smart_money_index) / 50
            else:
                phase = 'neutral'
                strength = 0.5
                
            return {
                'value': smart_money_index,
                'phase': phase,
                'strength': strength,
                'money_flow_ratio': money_flow_ratio,
                'positive_flow': positive_sum,
                'negative_flow': negative_sum
            }
            
        except Exception as e:
            self.logger.error(f"Smart Money Flow calculation failed: {e}")
            return None
            
    def calculate_volatility_squeeze(self, ohlcv, config):
        """Calculate Volatility Squeeze indicator"""
        try:
            closes = np.array(ohlcv['close'])
            highs = np.array(ohlcv['high'])
            lows = np.array(ohlcv['low'])
            
            period = config['lookback_period']
            params = config['parameters']
            
            if len(closes) < period:
                return None
                
            # Calculate Bollinger Bands
            sma = np.mean(closes[-period:])
            std = np.std(closes[-period:])
            bb_upper = sma + (2 * std)
            bb_lower = sma - (2 * std)
            bb_width = (bb_upper - bb_lower) / sma
            
            # Calculate Keltner Channels
            typical_price = (highs + lows + closes) / 3
            kc_middle = np.mean(typical_price[-period:])
            
            # True Range for ATR
            true_ranges = []
            for i in range(1, len(closes)):
                tr = max(
                    highs[i] - lows[i],
                    abs(highs[i] - closes[i-1]),
                    abs(lows[i] - closes[i-1])
                )
                true_ranges.append(tr)
                
            atr = np.mean(true_ranges[-period:])
            kc_upper = kc_middle + (1.5 * atr)
            kc_lower = kc_middle - (1.5 * atr)
            kc_width = (kc_upper - kc_lower) / kc_middle
            
            # Squeeze detection (BB inside KC)
            squeeze_detected = bb_width < kc_width * params['squeeze_threshold']
            
            # Momentum oscillator
            momentum = closes[-1] - sma
            
            # Historical squeeze analysis
            if hasattr(self, '_squeeze_history'):
                self._squeeze_history.append(squeeze_detected)
                if len(self._squeeze_history) > period:
                    self._squeeze_history.pop(0)
            else:
                self._squeeze_history = [squeeze_detected]
                
            # Count consecutive squeeze periods
            consecutive_squeeze = 0
            for squeeze in reversed(self._squeeze_history):
                if squeeze:
                    consecutive_squeeze += 1
                else:
                    break
                    
            # Predict expansion potential
            expansion_potential = min(1.0, consecutive_squeeze / 10)  # Max at 10 periods
            
            return {
                'squeeze_detected': squeeze_detected,
                'consecutive_periods': consecutive_squeeze,
                'expansion_potential': expansion_potential,
                'momentum': momentum,
                'bb_width': bb_width,
                'kc_width': kc_width

	    }
            
        except Exception as e:
            self.logger.error(f"Volatility Squeeze calculation failed: {e}")
            return None
            
    def calculate_market_structure(self, ohlcv, config):
        """Calculate Market Structure indicator"""
        try:
            closes = np.array(ohlcv['close'])
            highs = np.array(ohlcv['high'])
            lows = np.array(ohlcv['low'])
            
            period = config['lookback_period']
            params = config['parameters']
            
            if len(closes) < period:
                return None
                
            # Identify swing highs and lows
            swing_highs = []
            swing_lows = []
            
            lookback = 5  # Periods to look back for swing identification
            
            for i in range(lookback, len(highs) - lookback):
                # Swing high: higher than surrounding periods
                if all(highs[i] >= highs[j] for j in range(i-lookback, i+lookback+1) if j != i):
                    swing_highs.append((i, highs[i]))
                    
                # Swing low: lower than surrounding periods
                if all(lows[i] <= lows[j] for j in range(i-lookback, i+lookback+1) if j != i):
                    swing_lows.append((i, lows[i]))
                    
            # Analyze structure breaks
            structure_breaks = []
            
            # Check for higher highs/lower lows pattern breaks
            if len(swing_highs) >= 2:
                last_high = swing_highs[-1][1]
                prev_high = swing_highs[-2][1]
                
                # Structure break if recent price action breaks pattern
                current_price = closes[-1]
                sensitivity = params['structure_sensitivity']
                
                if last_high > prev_high:  # Uptrend structure
                    # Break if price falls significantly below last swing low
                    if swing_lows:
                        last_low = swing_lows[-1][1]
                        if current_price < last_low * (1 - sensitivity):
                            structure_breaks.append({
                                'type': 'uptrend_break',
                                'break_level': last_low,
                                'strength': abs(current_price - last_low) / last_low
                            })
                            
                elif last_high < prev_high:  # Downtrend structure
                    # Break if price rises significantly above last swing high
                    if current_price > last_high * (1 + sensitivity):
                        structure_breaks.append({
                            'type': 'downtrend_break',
                            'break_level': last_high,
                            'strength': abs(current_price - last_high) / last_high
                        })
                        
            # Market structure state
            if not structure_breaks:
                if len(swing_highs) >= 2 and swing_highs[-1][1] > swing_highs[-2][1]:
                    structure_state = 'uptrend'
                elif len(swing_highs) >= 2 and swing_highs[-1][1] < swing_highs[-2][1]:
                    structure_state = 'downtrend'
                else:
                    structure_state = 'ranging'
            else:
                structure_state = 'structure_break'
                
            return {
                'structure_state': structure_state,
                'structure_breaks': structure_breaks,
                'swing_highs': swing_highs[-5:],  # Last 5 swing highs
                'swing_lows': swing_lows[-5:],   # Last 5 swing lows
                'break_count': len(structure_breaks)
            }
            
        except Exception as e:
            self.logger.error(f"Market Structure calculation failed: {e}")
            return None
            
    def calculate_fear_greed_oscillator(self, ohlcv, config):
        """Calculate Fear/Greed Oscillator"""
        try:
            closes = np.array(ohlcv['close'])
            volumes = np.array(ohlcv['volume'])
            highs = np.array(ohlcv['high'])
            lows = np.array(ohlcv['low'])
            
            period = config['lookback_period']
            params = config['parameters']
            
            if len(closes) < period:
                return None
                
            # Component 1: Price momentum
            price_changes = np.diff(closes[-period:])
            momentum_score = np.mean(price_changes) / np.std(price_changes) if np.std(price_changes) > 0 else 0
            
            # Component 2: Volume trend
            volume_sma_short = np.mean(volumes[-7:])
            volume_sma_long = np.mean(volumes[-period:])
            volume_score = (volume_sma_short / volume_sma_long - 1) if volume_sma_long > 0 else 0
            
            # Component 3: Volatility (fear indicator)
            returns = np.diff(np.log(closes[-period:]))
            volatility = np.std(returns) * np.sqrt(period)
            historical_vol = np.mean([np.std(np.diff(np.log(closes[i:i+period]))) 
                                    for i in range(0, len(closes)-period, period//4)])
            vol_score = -(volatility / historical_vol - 1) if historical_vol > 0 else 0  # Negative because high vol = fear
            
            # Component 4: Range analysis
            true_ranges = []
            for i in range(1, len(closes)):
                tr = max(
                    highs[i] - lows[i],
                    abs(highs[i] - closes[i-1]),
                    abs(lows[i] - closes[i-1])
                )
                true_ranges.append(tr / closes[i])  # Normalize by price
                
            avg_range = np.mean(true_ranges[-period:])
            recent_range = np.mean(true_ranges[-3:])
            range_score = -(recent_range / avg_range - 1) if avg_range > 0 else 0
            
            # Combine components
            fear_greed_raw = (momentum_score * 0.4 + volume_score * 0.3 + 
                            vol_score * 0.2 + range_score * 0.1)
            
            # Apply momentum factor
            momentum_factor = params['momentum_factor']
            if hasattr(self, '_last_fear_greed'):
                fear_greed_raw = (momentum_factor * self._last_fear_greed + 
                                (1 - momentum_factor) * fear_greed_raw)
                                
            self._last_fear_greed = fear_greed_raw
            
            # Normalize to 0-100 scale (0 = extreme fear, 100 = extreme greed)
            fear_greed_index = 50 + (np.tanh(fear_greed_raw) * 50)
            
            # Determine market sentiment
            extreme_threshold = params['extreme_threshold']
            
            if fear_greed_index >= extreme_threshold:
                sentiment = 'extreme_greed'
                signal = 'sell'  # Contrarian
            elif fear_greed_index >= 70:
                sentiment = 'greed'
                signal = 'caution'
            elif fear_greed_index <= (100 - extreme_threshold):
                sentiment = 'extreme_fear'
                signal = 'buy'  # Contrarian
            elif fear_greed_index <= 30:
                sentiment = 'fear'
                signal = 'opportunity'
            else:
                sentiment = 'neutral'
                signal = 'hold'
                
            return {
                'value': fear_greed_index,
                'sentiment': sentiment,
                'signal': signal,
                'components': {
                    'momentum': momentum_score,
                    'volume': volume_score,
                    'volatility': vol_score,
                    'range': range_score
                }
            }
            
        except Exception as e:
            self.logger.error(f"Fear/Greed Oscillator calculation failed: {e}")
            return None
            
    def calculate_signal_strength(self, indicator_value, config):
        """Calculate signal strength from indicator value"""
        try:
            if not isinstance(indicator_value, dict):
                return 0.5
                
            # Extract relevant metrics
            value = indicator_value.get('value', 0)
            strength = indicator_value.get('strength', 0.5)
            
            # Scale based on sensitivity
            sensitivity = config.get('sensitivity', 'medium')
            sensitivity_multipliers = {'low': 0.7, 'medium': 1.0, 'high': 1.3}
            multiplier = sensitivity_multipliers.get(sensitivity, 1.0)
            
            # Calculate base strength
            if 'signal' in indicator_value:
                signal = indicator_value['signal']
                if signal in ['bullish', 'strong_buying', 'upward', 'buy']:
                    base_strength = 0.7 + (strength * 0.3)
                elif signal in ['bearish', 'strong_selling', 'downward', 'sell']:
                    base_strength = 0.7 + (strength * 0.3)
                else:
                    base_strength = 0.3 + (strength * 0.4)
            else:
                base_strength = 0.5
                
            return min(1.0, base_strength * multiplier)
            
        except Exception as e:
            self.logger.error(f"Signal strength calculation failed: {e}")
            return 0.5
            
    def recognize_all_patterns(self, symbol):
        """Recognize all enabled patterns for a symbol"""
        try:
            # Get market data
            klines = self.binance_client.get_klines(symbol, '5m', 200)
            
            if not klines or len(klines) < 100:
                return
                
            ohlcv_data = self.prepare_ohlcv_data(klines)
            
            patterns_found = []
            
            # Recognize harmonic patterns
            if self.pattern_recognition['harmonic_patterns']['enabled']:
                harmonic_patterns = self.recognize_harmonic_patterns(ohlcv_data)
                patterns_found.extend(harmonic_patterns)
                
            # Recognize chart patterns
            if self.pattern_recognition['chart_patterns']['enabled']:
                chart_patterns = self.recognize_chart_patterns(ohlcv_data)
                patterns_found.extend(chart_patterns)
                
            # Recognize candlestick patterns
            if self.pattern_recognition['candlestick_patterns']['enabled']:
                candlestick_patterns = self.recognize_candlestick_patterns(ohlcv_data)
                patterns_found.extend(candlestick_patterns)
                
            # Store patterns
            self.pattern_cache[symbol] = patterns_found
            
            # Log significant patterns
            significant_patterns = [p for p in patterns_found if p.get('confidence', 0) > 0.7]
            
            if significant_patterns:
                self.logger.info(f"üéØ Patterns found in {symbol}: {[p['type'] for p in significant_patterns]}")
                
        except Exception as e:
            self.logger.error(f"Pattern recognition failed for {symbol}: {e}")
            
    def recognize_harmonic_patterns(self, ohlcv):
        """Recognize harmonic patterns (Gartley, Butterfly, etc.)"""
        try:
            patterns = []
            
            closes = np.array(ohlcv['close'])
            highs = np.array(ohlcv['high'])
            lows = np.array(ohlcv['low'])
            
            if len(closes) < 50:
                return patterns
                
            # Simplified Gartley pattern detection
            # Look for XABCD pattern with specific Fibonacci ratios
            
            # Find potential swing points
            swings = self.find_swing_points(highs, lows, 5)
            
            if len(swings) >= 5:  # Need at least 5 points for XABCD
                for i in range(len(swings) - 4):
                    X = swings[i]
                    A = swings[i + 1]
                    B = swings[i + 2]
                    C = swings[i + 3]
                    D = swings[i + 4]
                    
                    # Check Gartley ratios
                    if self.check_gartley_ratios(X, A, B, C, D):
                        patterns.append({
                            'type': 'gartley',
                            'category': 'harmonic',
                            'points': [X, A, B, C, D],
                            'confidence': 0.8,
                            'target': self.calculate_harmonic_target(X, A, B, C, D),
                            'completion_point': D
                        })
                        
            return patterns
            
        except Exception as e:
            self.logger.error(f"Harmonic pattern recognition failed: {e}")
            return []
            
    def find_swing_points(self, highs, lows, lookback):
        """Find swing high and low points"""
        try:
            swings = []
            
            for i in range(lookback, len(highs) - lookback):
                # Check for swing high
                if all(highs[i] >= highs[j] for j in range(i-lookback, i+lookback+1) if j != i):
                    swings.append({'index': i, 'price': highs[i], 'type': 'high'})
                    
                # Check for swing low
                if all(lows[i] <= lows[j] for j in range(i-lookback, i+lookback+1) if j != i):
                    swings.append({'index': i, 'price': lows[i], 'type': 'low'})
                    
            # Sort by index
            swings.sort(key=lambda x: x['index'])
            
            return swings
            
        except Exception as e:
            self.logger.error(f"Swing point detection failed: {e}")
            return []
            
    def check_gartley_ratios(self, X, A, B, C, D):
        """Check if swing points form valid Gartley ratios"""
        try:
            # Gartley pattern ratios
            # AB should be 61.8% of XA
            # BC should be 38.2% or 88.6% of AB
            # CD should be 127.2% of BC
            # AD should be 78.6% of XA
            
            XA = abs(A['price'] - X['price'])
            AB = abs(B['price'] - A['price'])
            BC = abs(C['price'] - B['price'])
            CD = abs(D['price'] - C['price'])
            AD = abs(D['price'] - A['price'])
            
            if XA == 0 or AB == 0 or BC == 0:
                return False
                
            # Check ratios with tolerance
            tolerance = 0.05  # 5% tolerance
            
            ratio_AB_XA = AB / XA
            ratio_BC_AB = BC / AB
            ratio_CD_BC = CD / BC
            ratio_AD_XA = AD / XA
            
            # Gartley specific ratios
            if (abs(ratio_AB_XA - 0.618) < tolerance and
                (abs(ratio_BC_AB - 0.382) < tolerance or abs(ratio_BC_AB - 0.886) < tolerance) and
                abs(ratio_CD_BC - 1.272) < tolerance and
                abs(ratio_AD_XA - 0.786) < tolerance):
                return True
                
            return False
            
        except Exception as e:
            self.logger.error(f"Gartley ratio check failed: {e}")
            return False
            
    def calculate_harmonic_target(self, X, A, B, C, D):
        """Calculate target for harmonic pattern"""
        try:
            # Simple target calculation based on pattern completion
            XA = abs(A['price'] - X['price'])
            
            if D['type'] == 'low':
                # Bullish pattern - target above
                target = D['price'] + (XA * 0.618)
            else:
                # Bearish pattern - target below
                target = D['price'] - (XA * 0.618)
                
            return target
            
        except Exception as e:
            self.logger.error(f"Harmonic target calculation failed: {e}")
            return D['price']
            
    def recognize_chart_patterns(self, ohlcv):
        """Recognize chart patterns (head and shoulders, triangles, etc.)"""
        try:
            patterns = []
            
            closes = np.array(ohlcv['close'])
            highs = np.array(ohlcv['high'])
            lows = np.array(ohlcv['low'])
            
            if len(closes) < 50:
                return patterns
                
            # Double top pattern
            double_top = self.detect_double_top(highs, closes)
            if double_top:
                patterns.append(double_top)
                
            # Double bottom pattern
            double_bottom = self.detect_double_bottom(lows, closes)
            if double_bottom:
                patterns.append(double_bottom)
                
            # Triangle patterns
            triangle = self.detect_triangle_patterns(highs, lows, closes)
            if triangle:
                patterns.append(triangle)
                
            return patterns
            
        except Exception as e:
            self.logger.error(f"Chart pattern recognition failed: {e}")
            return []
            
    def detect_double_top(self, highs, closes):
        """Detect double top pattern"""
        try:
            if len(highs) < 30:
                return None
                
            # Find recent highs
            recent_highs = []
            lookback = 5
            
            for i in range(lookback, len(highs) - lookback):
                if all(highs[i] >= highs[j] for j in range(i-lookback, i+lookback+1) if j != i):
                    recent_highs.append({'index': i, 'price': highs[i]})
                    
            if len(recent_highs) < 2:
                return None
                
            # Check for double top (two similar highs with a valley between)
            for i in range(len(recent_highs) - 1):
                high1 = recent_highs[i]
                high2 = recent_highs[i + 1]
                
                # Check if highs are similar (within 2%)
                price_diff = abs(high1['price'] - high2['price']) / high1['price']
                
                if price_diff < 0.02 and (high2['index'] - high1['index']) > 10:
                    # Find the valley between highs
                    valley_start = high1['index']
                    valley_end = high2['index']
                    valley_low = min(lows[valley_start:valley_end])
                    
                    # Valley should be significantly lower
                    valley_depth = min(high1['price'], high2['price']) - valley_low
                    if valley_depth / high1['price'] > 0.03:  # At least 3% retracement
                        
                        return {
                            'type': 'double_top',
                            'category': 'chart_pattern',
                            'peak1': high1,
                            'peak2': high2,
                            'valley_low': valley_low,
                            'confidence': 0.7,
                            'target': valley_low - (valley_depth * 0.5),
                            'breakout_level': valley_low
                        }
                        
            return None
            
        except Exception as e:
            self.logger.error(f"Double top detection failed: {e}")
            return None
            
    def detect_double_bottom(self, lows, closes):
        """Detect double bottom pattern"""
        try:
            if len(lows) < 30:
                return None
                
            # Find recent lows
            recent_lows = []
            lookback = 5
            
            for i in range(lookback, len(lows) - lookback):
                if all(lows[i] <= lows[j] for j in range(i-lookback, i+lookback+1) if j != i):
                    recent_lows.append({'index': i, 'price': lows[i]})
                    
            if len(recent_lows) < 2:
                return None
                
            # Check for double bottom
            for i in range(len(recent_lows) - 1):
                low1 = recent_lows[i]
                low2 = recent_lows[i + 1]
                
                # Check if lows are similar
                price_diff = abs(low1['price'] - low2['price']) / low1['price']
                
                if price_diff < 0.02 and (low2['index'] - low1['index']) > 10:
                    # Find the peak between lows
                    peak_start = low1['index']
                    peak_end = low2['index']
                    peak_high = max(closes[peak_start:peak_end])
                    
                    # Peak should be significantly higher
                    peak_height = peak_high - max(low1['price'], low2['price'])
                    if peak_height / low1['price'] > 0.03:
                        
                        return {
                            'type': 'double_bottom',
                            'category': 'chart_pattern',
                            'bottom1': low1,
                            'bottom2': low2,
                            'peak_high': peak_high,
                            'confidence': 0.7,
                            'target': peak_high + (peak_height * 0.5),
                            'breakout_level': peak_high
                        }
                        
            return None
            
        except Exception as e:
            self.logger.error(f"Double bottom detection failed: {e}")
            return None
            
    def detect_triangle_patterns(self, highs, lows, closes):
        """Detect triangle patterns"""
        try:
            if len(closes) < 40:
                return None
                
            # Simplified ascending triangle detection
            # Look for horizontal resistance with rising support
            
            recent_data = 30  # Last 30 periods
            recent_highs = highs[-recent_data:]
            recent_lows = lows[-recent_data:]
            
            # Check for horizontal resistance (similar highs)
            high_levels = []
            for i in range(5, len(recent_highs) - 5):
                if (recent_highs[i] >= max(recent_highs[i-5:i]) and 
                    recent_highs[i] >= max(recent_highs[i+1:i+6])):
                    high_levels.append(recent_highs[i])
                    
            if len(high_levels) >= 2:
                avg_high = np.mean(high_levels)
                high_variance = np.std(high_levels) / avg_high
                
                # Check if highs are relatively flat (low variance)
                if high_variance < 0.01:  # Less than 1% variance
                    
                    # Check for rising lows
                    low_trend = np.polyfit(range(len(recent_lows)), recent_lows, 1)[0]
                    
                    if low_trend > 0:  # Rising trend
                        return {
                            'type': 'ascending_triangle',
                            'category': 'chart_pattern',
                            'resistance_level': avg_high,
                            'support_trend': low_trend,
                            'confidence': 0.6,
                            'target': avg_high + (avg_high - min(recent_lows)) * 0.618,
                            'breakout_level': avg_high
                        }
                        
            return None
            
        except Exception as e:
            self.logger.error(f"Triangle pattern detection failed: {e}")
            return None
            
    def recognize_candlestick_patterns(self, ohlcv):
        """Recognize candlestick patterns"""
        try:
            patterns = []
            
            opens = np.array(ohlcv['open'])
            highs = np.array(ohlcv['high'])
            lows = np.array(ohlcv['low'])
            closes = np.array(ohlcv['close'])
            
            if len(closes) < 10:
                return patterns
                
            # Check last few candles for patterns
            for i in range(-5, 0):
                # Doji pattern
                doji = self.detect_doji(opens[i], highs[i], lows[i], closes[i])
                if doji:
                    patterns.append(doji)
                    
                # Hammer pattern
                if i > -len(opens):
                    hammer = self.detect_hammer(opens[i], highs[i], lows[i], closes[i])
                    if hammer:
                        patterns.append(hammer)
                        
                # Engulfing pattern (needs previous candle)
                if i > -len(opens) and abs(i) < len(opens):
                    engulfing = self.detect_engulfing(
                        opens[i-1], highs[i-1], lows[i-1], closes[i-1],
                        opens[i], highs[i], lows[i], closes[i]
                    )
                    if engulfing:
                        patterns.append(engulfing)
                        
            return patterns
            
        except Exception as e:
            self.logger.error(f"Candlestick pattern recognition failed: {e}")
            return []
            
    def detect_doji(self, open_price, high, low, close):
        """Detect doji candlestick pattern"""
        try:
            body_size = abs(close - open_price)
            range_size = high - low
            
            # Doji: very small body relative to range
            if range_size > 0 and body_size / range_size < 0.1:
                return {
                    'type': 'doji',
                    'category': 'candlestick',
                    'signal': 'reversal_potential',
                    'confidence': 0.6,
                    'body_ratio': body_size / range_size
                }
                
            return None
            
        except Exception as e:
            self.logger.error(f"Doji detection failed: {e}")
            return None
            
    def detect_hammer(self, open_price, high, low, close):
        """Detect hammer candlestick pattern"""
        try:
            body_size = abs(close - open_price)
            lower_shadow = min(open_price, close) - low
            upper_shadow = high - max(open_price, close)
            
            # Hammer: long lower shadow, small body, small upper shadow
            if (lower_shadow > body_size * 2 and 
                upper_shadow < body_size and
                body_size > 0):
                
                return {
                    'type': 'hammer',
                    'category': 'candlestick',
                    'signal': 'bullish_reversal',
                    'confidence': 0.7,
                    'shadow_ratio': lower_shadow / body_size
                }
                
            return None
            
        except Exception as e:
            self.logger.error(f"Hammer detection failed: {e}")
            return None
            
    def detect_engulfing(self, open1, high1, low1, close1, open2, high2, low2, close2):
        """Detect engulfing candlestick pattern"""
        try:
            # Bullish engulfing: bearish candle followed by larger bullish candle
            if (close1 < open1 and  # First candle bearish
                close2 > open2 and  # Second candle bullish
                open2 < close1 and  # Second opens below first close
                close2 > open1):    # Second closes above first open
                
                return {
                    'type': 'bullish_engulfing',
                    'category': 'candlestick',
                    'signal': 'bullish_reversal',
                    'confidence': 0.8
                }
                
            # Bearish engulfing: bullish candle followed by larger bearish candle
            elif (close1 > open1 and  # First candle bullish
                  close2 < open2 and  # Second candle bearish
                  open2 > close1 and  # Second opens above first close
                  close2 < open1):    # Second closes below first open
                  
                return {
                    'type': 'bearish_engulfing',
                    'category': 'candlestick',
                    'signal': 'bearish_reversal',
                    'confidence': 0.8
                }
                
            return None
            
        except Exception as e:
            self.logger.error(f"Engulfing pattern detection failed: {e}")
            return None
            
    def adapt_indicator_parameters(self):
        """Adapt indicator parameters based on performance"""
        try:
            if not self.signal_adaptation['learning_enabled']:
                return
                
            # Get recent trade performance
            recent_trades = self.performance_analytics.get_recent_trades(
                self.signal_adaptation['performance_window']
            )
            
            if len(recent_trades) < 20:  # Need minimum trades for adaptation
                return
                
            # Analyze performance by indicator signals
            indicator_performance = self.analyze_indicator_trade_performance(recent_trades)
            
            # Adapt parameters for underperforming indicators
            for indicator_name, performance in indicator_performance.items():
                if indicator_name in self.indicator_configs:
                    self.adapt_single_indicator(indicator_name, performance)
                    
            self.logger.info("üß† Indicator parameters adapted based on performance")
            
        except Exception as e:
            self.logger.error(f"Indicator adaptation failed: {e}")
            
    def analyze_indicator_trade_performance(self, trades):
        """Analyze trade performance attributed to specific indicators"""
        try:
            indicator_performance = defaultdict(lambda: {
                'trade_count': 0,
                'successful_trades': 0,
                'total_pnl': 0,
                'avg_return': 0
            })
            
            for trade in trades:
                # Get indicators that were active when trade was made
                trade_indicators = trade.get('active_indicators', [])
                
                for indicator_name in trade_indicators:
                    perf = indicator_performance[indicator_name]
                    perf['trade_count'] += 1
                    
                    if trade.get('pnl', 0) > 0:
                        perf['successful_trades'] += 1
                        
                    perf['total_pnl'] += trade.get('pnl', 0)
                    
            # Calculate averages
            for indicator_name, perf in indicator_performance.items():
                if perf['trade_count'] > 0:
                    perf['success_rate'] = perf['successful_trades'] / perf['trade_count']
		    perf['avg_return'] = perf['total_pnl'] / perf['trade_count']
                else:
                    perf['success_rate'] = 0
                    perf['avg_return'] = 0
                    
            return indicator_performance
            
        except Exception as e:
            self.logger.error(f"Indicator trade performance analysis failed: {e}")
            return {}
            
    def adapt_single_indicator(self, indicator_name, performance):
        """Adapt parameters for a single indicator based on performance"""
        try:
            config = self.indicator_configs[indicator_name]
            adaptation_rate = self.signal_adaptation['adaptation_rate']
            
            success_rate = performance.get('success_rate', 0.5)
            avg_return = performance.get('avg_return', 0)
            
            # If performance is poor, adjust parameters
            if success_rate < 0.4 or avg_return < 0:
                
                if indicator_name == 'monster_momentum':
                    # Reduce sensitivity for poor performance
                    config['parameters']['smoothing_factor'] = min(0.5, 
                        config['parameters']['smoothing_factor'] * (1 + adaptation_rate))
                        
                elif indicator_name == 'whale_pressure':
                    # Adjust pressure thresholds
                    config['parameters']['imbalance_threshold'] = min(0.9,
                        config['parameters']['imbalance_threshold'] * (1 + adaptation_rate))
                        
                elif indicator_name == 'breakout_velocity':
                    # Increase velocity threshold to reduce false signals
                    config['parameters']['velocity_threshold'] = min(0.05,
                        config['parameters']['velocity_threshold'] * (1 + adaptation_rate))
                        
                elif indicator_name == 'volatility_squeeze':
                    # Adjust squeeze detection sensitivity
                    config['parameters']['squeeze_threshold'] = max(0.3,
                        config['parameters']['squeeze_threshold'] * (1 - adaptation_rate))
                        
            # If performance is good, be more aggressive
            elif success_rate > 0.7 and avg_return > 0:
                
                if indicator_name == 'monster_momentum':
                    # Increase sensitivity for good performance
                    config['parameters']['smoothing_factor'] = max(0.1,
                        config['parameters']['smoothing_factor'] * (1 - adaptation_rate))
                        
                elif indicator_name == 'whale_pressure':
                    # Lower threshold to catch more signals
                    config['parameters']['imbalance_threshold'] = max(0.5,
                        config['parameters']['imbalance_threshold'] * (1 - adaptation_rate))
                        
                elif indicator_name == 'breakout_velocity':
                    # Lower threshold to catch more breakouts
                    config['parameters']['velocity_threshold'] = max(0.01,
                        config['parameters']['velocity_threshold'] * (1 - adaptation_rate))
                        
            # Update performance tracking
            self.indicator_performance[indicator_name].update({
                'last_adaptation': time.time(),
                'success_rate': success_rate,
                'avg_return': avg_return
            })
            
        except Exception as e:
            self.logger.error(f"Single indicator adaptation failed for {indicator_name}: {e}")
            
    def generate_combined_signal(self, symbol):
        """Generate combined signal from all indicators and patterns"""
        try:
            if symbol not in self.indicator_cache:
                return None
                
            indicators = self.indicator_cache[symbol]
            patterns = self.pattern_cache.get(symbol, [])
            
            # Calculate weighted signal scores
            total_weight = 0
            bullish_score = 0
            bearish_score = 0
            
            # Process indicator signals
            for indicator_name, indicator_data in indicators.items():
                if indicator_name not in self.indicator_configs:
                    continue
                    
                config = self.indicator_configs[indicator_name]
                weight = config['weight']
                
                indicator_value = indicator_data.get('value')
                if not isinstance(indicator_value, dict):
                    continue
                    
                signal = indicator_value.get('signal', 'neutral')
                strength = indicator_data.get('signal_strength', 0.5)
                
                # Apply performance-based weight adjustment
                performance = self.indicator_performance.get(indicator_name, {})
                performance_multiplier = max(0.5, performance.get('success_rate', 0.5))
                adjusted_weight = weight * performance_multiplier
                
                total_weight += adjusted_weight
                
                # Add to bullish or bearish score
                if signal in ['bullish', 'strong_buying', 'upward', 'buy', 'accumulation']:
                    bullish_score += adjusted_weight * strength
                elif signal in ['bearish', 'strong_selling', 'downward', 'sell', 'distribution']:
                    bearish_score += adjusted_weight * strength
                    
            # Process pattern signals
            pattern_weight = 0.5
            
            for pattern in patterns:
                confidence = pattern.get('confidence', 0.5)
                signal = pattern.get('signal', 'neutral')
                
                if signal in ['bullish_reversal', 'bullish']:
                    bullish_score += pattern_weight * confidence
                elif signal in ['bearish_reversal', 'bearish']:
                    bearish_score += pattern_weight * confidence
                    
                total_weight += pattern_weight
                
            # Calculate final signal
            if total_weight == 0:
                return None
                
            bullish_pct = bullish_score / total_weight
            bearish_pct = bearish_score / total_weight
            
            # Determine signal direction and strength
            confidence_threshold = self.signal_adaptation['confidence_threshold']
            
            if bullish_pct > bearish_pct and bullish_pct > confidence_threshold:
                signal_direction = 'buy'
                signal_strength = min(1.0, bullish_pct)
                confidence = bullish_pct
                
            elif bearish_pct > bullish_pct and bearish_pct > confidence_threshold:
                signal_direction = 'sell'
                signal_strength = min(1.0, bearish_pct)
                confidence = bearish_pct
                
            else:
                signal_direction = 'hold'
                signal_strength = 0.5
                confidence = max(bullish_pct, bearish_pct)
                
            # Determine signal urgency
            if signal_strength > 0.8:
                urgency = 'very_high'
            elif signal_strength > 0.7:
                urgency = 'high'
            elif signal_strength > 0.6:
                urgency = 'medium'
            else:
                urgency = 'low'
                
            combined_signal = {
                'symbol': symbol,
                'signal': signal_direction,
                'strength': signal_strength,
                'confidence': confidence,
                'urgency': urgency,
                'bullish_score': bullish_pct,
                'bearish_score': bearish_pct,
                'contributing_indicators': len([i for i in indicators.keys() if i in self.indicator_configs]),
                'contributing_patterns': len(patterns),
                'timestamp': time.time()
            }
            
            # Store signal in history
            self.signal_history[symbol].append(combined_signal)
            
            # Limit signal history
            if len(self.signal_history[symbol]) > 100:
                self.signal_history[symbol].pop(0)
                
            return combined_signal
            
        except Exception as e:
            self.logger.error(f"Combined signal generation failed for {symbol}: {e}")
            return None
            
    def get_indicator_summary(self, symbol):
        """Get comprehensive indicator summary for a symbol"""
        try:
            if symbol not in self.indicator_cache:
                return {}
                
            indicators = self.indicator_cache[symbol]
            patterns = self.pattern_cache.get(symbol, [])
            
            summary = {
                'symbol': symbol,
                'last_update': max([ind.get('timestamp', 0) for ind in indicators.values()]) if indicators else 0,
                'active_indicators': len(indicators),
                'detected_patterns': len(patterns),
                'indicators': {},
                'patterns': patterns,
                'combined_signal': self.generate_combined_signal(symbol)
            }
            
            # Add individual indicator summaries
            for indicator_name, indicator_data in indicators.items():
                if indicator_name in self.indicator_configs:
                    config = self.indicator_configs[indicator_name]
                    
                    summary['indicators'][indicator_name] = {
                        'description': config['description'],
                        'value': indicator_data.get('value'),
                        'signal_strength': indicator_data.get('signal_strength', 0),
                        'weight': config['weight'],
                        'enabled': config['enabled'],
                        'performance': self.indicator_performance.get(indicator_name, {})
                    }
                    
            return summary
            
        except Exception as e:
            self.logger.error(f"Indicator summary generation failed for {symbol}: {e}")
            return {}
            
    def get_top_signals(self, limit=5):
        """Get top signals across all symbols"""
        try:
            all_signals = []
            
            for symbol in self.indicator_cache.keys():
                signal = self.generate_combined_signal(symbol)
                if signal and signal['signal'] != 'hold':
                    all_signals.append(signal)
                    
            # Sort by strength and confidence
            all_signals.sort(key=lambda x: x['strength'] * x['confidence'], reverse=True)
            
            return all_signals[:limit]
            
        except Exception as e:
            self.logger.error(f"Top signals retrieval failed: {e}")
            return []
            
    def get_indicator_performance_report(self):
        """Get comprehensive indicator performance report"""
        try:
            report = {
                'total_indicators': len(self.indicator_configs),
                'enabled_indicators': len([c for c in self.indicator_configs.values() if c['enabled']]),
                'performance_summary': {},
                'adaptation_status': {
                    'learning_enabled': self.signal_adaptation['learning_enabled'],
                    'last_adaptation': max([p.get('last_adaptation', 0) 
                                          for p in self.indicator_performance.values()]) if self.indicator_performance else 0
                },
                'top_performers': [],
                'underperformers': []
            }
            
            # Calculate performance metrics
            for indicator_name, performance in self.indicator_performance.items():
                if performance['total_signals'] > 0:
                    success_rate = performance['successful_signals'] / performance['total_signals']
                    avg_strength = performance['avg_signal_strength']
                    
                    perf_summary = {
                        'indicator': indicator_name,
                        'total_signals': performance['total_signals'],
                        'success_rate': success_rate,
                        'total_return': performance['total_return'],
                        'avg_signal_strength': avg_strength,
                        'performance_score': success_rate * avg_strength
                    }
                    
                    report['performance_summary'][indicator_name] = perf_summary
                    
                    # Categorize performers
                    if success_rate > 0.7:
                        report['top_performers'].append(perf_summary)
                    elif success_rate < 0.4:
                        report['underperformers'].append(perf_summary)
                        
            # Sort performers
            report['top_performers'].sort(key=lambda x: x['performance_score'], reverse=True)
            report['underperformers'].sort(key=lambda x: x['performance_score'])
            
            return report
            
        except Exception as e:
            self.logger.error(f"Performance report generation failed: {e}")
            return {}
            
    def update_indicator_performance(self, indicator_name, signal_result):
        """Update performance tracking for specific indicator"""
        try:
            performance = self.indicator_performance[indicator_name]
            
            performance['total_signals'] += 1
            
            if signal_result.get('successful', False):
                performance['successful_signals'] += 1
                
            performance['total_return'] += signal_result.get('return', 0)
            
            # Update average signal strength
            signal_strength = signal_result.get('signal_strength', 0.5)
            total_signals = performance['total_signals']
            
            performance['avg_signal_strength'] = (
                (performance['avg_signal_strength'] * (total_signals - 1) + signal_strength) / total_signals
            )
            
            performance['last_performance_update'] = time.time()
            
        except Exception as e:
            self.logger.error(f"Indicator performance update failed for {indicator_name}: {e}")
            
    def export_indicator_data(self, filename=None):
        """Export indicator configurations and performance data"""
        try:
            if filename is None:
                filename = f"indicators_data_{int(time.time())}.json"
                
            export_data = {
                'export_timestamp': time.time(),
                'indicator_configs': self.indicator_configs,
                'pattern_recognition': self.pattern_recognition,
                'signal_adaptation': self.signal_adaptation,
                'indicator_performance': dict(self.indicator_performance),
                'recent_signals': {
                    symbol: signals[-10:] for symbol, signals in self.signal_history.items()
                }
            }
            
            # Store via memory manager
            self.memory_manager.export_indicator_data(export_data, filename)
            
            self.logger.info(f"üìÅ Indicator data exported: {filename}")
            return filename
            
        except Exception as e:
            self.logger.error(f"Indicator data export failed: {e}")
            return None
            
    def optimize_indicator_weights(self):
        """Optimize indicator weights based on historical performance"""
        try:
            if not self.signal_adaptation['learning_enabled']:
                return
                
            # Get performance data
            performance_data = {}
            
            for indicator_name, performance in self.indicator_performance.items():
                if performance['total_signals'] > 10:  # Minimum signals for optimization
                    success_rate = performance['successful_signals'] / performance['total_signals']
                    avg_return = performance['total_return'] / performance['total_signals']
                    
                    # Calculate performance score (success rate weighted by returns)
                    performance_score = success_rate * (1 + max(0, avg_return))
                    performance_data[indicator_name] = performance_score
                    
            if not performance_data:
                return
                
            # Normalize performance scores and update weights
            max_performance = max(performance_data.values())
            
            for indicator_name, score in performance_data.items():
                if indicator_name in self.indicator_configs:
                    # Calculate new weight (0.5 to 2.0 range)
                    normalized_score = score / max_performance
                    new_weight = 0.5 + (normalized_score * 1.5)
                    
                    # Apply gradual adjustment
                    current_weight = self.indicator_configs[indicator_name]['weight']
                    adjustment_rate = 0.1
                    
                    self.indicator_configs[indicator_name]['weight'] = (
                        current_weight * (1 - adjustment_rate) + new_weight * adjustment_rate
                    )
                    
            self.logger.info("‚öñÔ∏è Indicator weights optimized based on performance")
            
        except Exception as e:
            self.logger.error(f"Indicator weight optimization failed: {e}")
            
    def detect_market_regime(self, symbol):
        """Detect current market regime for indicator adaptation"""
        try:
            if symbol not in self.indicator_cache:
                return 'unknown'
                
            indicators = self.indicator_cache[symbol]
            
            # Analyze multiple indicators to determine regime
            volatility_indicator = indicators.get('volatility_squeeze', {}).get('value', {})
            momentum_indicator = indicators.get('monster_momentum', {}).get('value', {})
            structure_indicator = indicators.get('market_structure', {}).get('value', {})
            
            # Determine volatility level
            if isinstance(volatility_indicator, dict):
                squeeze_detected = volatility_indicator.get('squeeze_detected', False)
                if squeeze_detected:
                    volatility_level = 'low'
                else:
                    volatility_level = 'high'
            else:
                volatility_level = 'medium'
                
            # Determine trend direction
            if isinstance(momentum_indicator, dict):
                momentum_value = momentum_indicator.get('value', 0)
                if momentum_value > 20:
                    trend_direction = 'up'
                elif momentum_value < -20:
                    trend_direction = 'down'
                else:
                    trend_direction = 'sideways'
            else:
                trend_direction = 'uncertain'
                
            # Determine trend strength
            if isinstance(structure_indicator, dict):
                structure_state = structure_indicator.get('structure_state', 'ranging')
                if 'trend' in structure_state:
                    trend_strength = 'strong'
                else:
                    trend_strength = 'weak'
            else:
                trend_strength = 'medium'
                
            # Map to regime
            if trend_direction == 'up' and trend_strength == 'strong':
                regime = 'trending_bull'
            elif trend_direction == 'down' and trend_strength == 'strong':
                regime = 'trending_bear'
            elif volatility_level == 'high' and trend_direction == 'sideways':
                regime = 'ranging_high_vol'
            elif volatility_level == 'low' and trend_direction == 'sideways':
                regime = 'ranging_low_vol'
            else:
                regime = 'transitional'
                
            return regime
            
        except Exception as e:
            self.logger.error(f"Market regime detection failed for {symbol}: {e}")
            return 'unknown'
            
    def adapt_to_market_regime(self, symbol, regime):
        """Adapt indicator parameters based on market regime"""
        try:
            if regime == 'trending_bull' or regime == 'trending_bear':
                # Increase momentum indicator sensitivity
                if 'monster_momentum' in self.indicator_configs:
                    self.indicator_configs['monster_momentum']['weight'] = 1.8
                    
                # Reduce mean reversion indicators
                if 'volatility_squeeze' in self.indicator_configs:
                    self.indicator_configs['volatility_squeeze']['weight'] = 0.8
                    
            elif regime == 'ranging_high_vol' or regime == 'ranging_low_vol':
                # Increase mean reversion indicators
                if 'volatility_squeeze' in self.indicator_configs:
                    self.indicator_configs['volatility_squeeze']['weight'] = 1.5
                    
                # Reduce momentum indicators
                if 'monster_momentum' in self.indicator_configs:
                    self.indicator_configs['monster_momentum']['weight'] = 1.0
                    
            elif regime == 'breakout':
                # Maximize breakout detection
                if 'breakout_velocity' in self.indicator_configs:
                    self.indicator_configs['breakout_velocity']['weight'] = 2.0
                    
        except Exception as e:
            self.logger.error(f"Market regime adaptation failed: {e}")
            
    def cleanup_old_data(self, days_to_keep=7):
        """Clean up old indicator data to manage memory"""
        try:
            cutoff_time = time.time() - (days_to_keep * 24 * 3600)
            
            # Clean indicator cache
            for symbol in list(self.indicator_cache.keys()):
                indicators_to_remove = []
                
                for indicator_name, indicator_data in self.indicator_cache[symbol].items():
                    if indicator_data.get('timestamp', time.time()) < cutoff_time:
                        indicators_to_remove.append(indicator_name)
                        
                for indicator_name in indicators_to_remove:
                    del self.indicator_cache[symbol][indicator_name]
                    
                # Remove empty symbol entries
                if not self.indicator_cache[symbol]:
                    del self.indicator_cache[symbol]
                    
            # Clean signal history
            for symbol in self.signal_history:
                self.signal_history[symbol] = [
                    signal for signal in self.signal_history[symbol]
                    if signal.get('timestamp', time.time()) > cutoff_time
                ]
                
            self.logger.info("üßπ Cleaned up old indicator data")
            
        except Exception as e:
            self.logger.error(f"Indicator data cleanup failed: {e}")

class MarketMakerDetectionEngine:
    """Advanced market maker detection engine for identifying bot behavior and market manipulation"""
    
    def __init__(self, config, binance_client, memory_manager, performance_analytics):
        self.config = config
        self.binance_client = binance_client
        self.memory_manager = memory_manager
        self.performance_analytics = performance_analytics
        self.logger = logging.getLogger('MonsterBot.MarketMakerDetection')
        
        # Market maker detection parameters
        self.detection_params = {
            'order_book_analysis': {
                'wall_detection_threshold': 50000,  # $50K+ orders
                'spoofing_time_threshold': 5,       # seconds
                'layer_analysis_depth': 20,        # order book levels
                'fake_wall_ratio': 0.3,            # 30% cancellation rate
                'volume_cluster_factor': 3.0       # 3x normal volume
            },
            'trading_pattern_analysis': {
                'ping_pong_threshold': 10,          # trades between same prices
                'wash_trading_correlation': 0.9,    # correlation threshold
                'momentum_ignition_volume': 100000, # $100K+ sudden volume
                'iceberg_order_detection': 0.1,    # 10% visible size
                'time_window_seconds': 300          # 5 minute analysis window
            },
            'liquidity_analysis': {
                'liquidity_removal_threshold': 0.5,  # 50% removal rate
                'bid_ask_manipulation_factor': 2.0,  # 2x normal spread
                'volume_pump_threshold': 5.0,        # 5x average volume
                'dark_pool_estimation': True,        # estimate hidden liquidity
                'microstructure_analysis': True      # high-frequency patterns
            },
            'bot_behavior_patterns': {
                'regular_interval_trading': 1.0,   # exact second intervals
                'round_number_bias': 0.8,          # preference for round prices
                'response_time_threshold': 0.1,    # 100ms response time
                'order_size_clustering': 0.9,      # similar order sizes
                'cross_exchange_coordination': 0.7  # synchronized behavior
            }
        }
        
        # Market maker behavior signatures
        self.mm_signatures = {
            'high_frequency_market_maker': {
                'characteristics': {
                    'order_cancellation_rate': (0.8, 1.0),     # 80-100%
                    'average_order_lifetime': (0.1, 2.0),      # 0.1-2 seconds
                    'bid_ask_presence': (0.9, 1.0),            # 90-100% time
                    'inventory_neutrality': (0.8, 1.0),        # maintains neutral position
                    'tick_by_tick_adjustments': True
                },
                'detection_confidence': 0.0,
                'recent_activity': []
            },
            'spoofing_bot': {
                'characteristics': {
                    'large_order_placement': True,
                    'quick_cancellation': (0.5, 5.0),          # 0.5-5 seconds
                    'price_impact_intent': False,              # no intention to trade
                    'order_size_disproportionate': (5.0, 50.0), # 5-50x normal
                    'pattern_repetition': True
                },
                'detection_confidence': 0.0,
                'recent_activity': []
            },
            'momentum_ignition_bot': {
                'characteristics': {
                    'sudden_volume_spike': (3.0, 10.0),        # 3-10x normal
                    'price_movement_correlation': (0.8, 1.0),  # high correlation
                    'short_duration_activity': (1.0, 30.0),   # 1-30 seconds
                    'aggressive_market_orders': True,
                    'immediate_reversal': True
                },
                'detection_confidence': 0.0,
                'recent_activity': []
            },
            'wash_trading_bot': {
                'characteristics': {
                    'self_trading_pattern': (0.7, 1.0),       # 70-100% self-trades
                    'volume_inflation': True,                  # artificial volume
                    'price_stability': (0.95, 1.0),          # minimal price impact
                    'coordinated_timing': True,
                    'cross_account_correlation': (0.8, 1.0)
                },
                'detection_confidence': 0.0,
                'recent_activity': []
            },
            'liquidity_provider': {
                'characteristics': {
                    'consistent_two_sided_quotes': (0.8, 1.0), # 80-100% time
                    'tight_spreads': (0.001, 0.005),          # 0.1-0.5% spreads
                    'large_order_sizes': (2.0, 10.0),         # 2-10x retail
                    'inventory_management': True,
                    'mean_reversion_bias': (0.6, 0.9)
                },
                'detection_confidence': 0.0,
                'recent_activity': []
            }
        }
        
        # Real-time market data tracking
        self.market_data = {
            'order_book_snapshots': deque(maxlen=1000),
            'trade_stream': deque(maxlen=5000),
            'volume_profile': defaultdict(float),
            'liquidity_metrics': defaultdict(dict),
            'suspicious_activities': deque(maxlen=500)
        }
        
        # Detection statistics
        self.detection_stats = {
            'total_detections': defaultdict(int),
            'confidence_scores': defaultdict(list),
            'false_positive_rate': defaultdict(float),
            'detection_accuracy': defaultdict(float),
            'last_detection_time': defaultdict(float)
        }
        
        # Alert thresholds
        self.alert_thresholds = {
            'spoofing_confidence': 0.8,
            'wash_trading_confidence': 0.7,
            'momentum_ignition_confidence': 0.9,
            'coordinated_manipulation': 0.85,
            'liquidity_removal_rate': 0.6
        }
        
        # Market manipulation patterns
        self.manipulation_patterns = {
            'pump_and_dump': {
                'phase_1_duration': (300, 1800),    # 5-30 minutes
                'volume_increase': (5.0, 20.0),     # 5-20x normal
                'price_increase': (0.05, 0.3),      # 5-30% pump
                'sharp_reversal': True,
                'social_media_coordination': True
            },
            'bear_raid': {
                'large_sell_orders': True,
                'cascading_stops': True,
                'negative_sentiment_flood': True,
                'short_covering_reversal': True,
                'institutional_coordination': (0.7, 1.0)
            },
            'stop_hunting': {
                'support_resistance_targeting': True,
                'spike_through_levels': (0.002, 0.01), # 0.2-1% spike
                'quick_reversal': (1.0, 10.0),         # 1-10 seconds
                'volume_spike_timing': True,
                'retail_stop_clustering': True
            }
        }
        
        # Initialize market maker detection
        self.initialize_mm_detection()
        
    def initialize_mm_detection(self):
        """Initialize market maker detection system"""
        try:
            # Start real-time order book monitoring
            self.start_order_book_monitoring()
            
            # Start trade stream analysis
            self.start_trade_stream_analysis()
            
            # Start pattern detection
            self.start_pattern_detection()
            
            # Start liquidity analysis
            self.start_liquidity_analysis()
            
            self.logger.info("ü§ñ Market Maker Detection Engine initialized")
            self.logger.info(f"   üéØ Detection patterns: {len(self.mm_signatures)}")
            self.logger.info(f"   üìä Manipulation patterns: {len(self.manipulation_patterns)}")
            self.logger.info(f"   ‚ö° Real-time monitoring: Active")
            
        except Exception as e:
            self.logger.error(f"Market maker detection initialization failed: {e}")
            
    def start_order_book_monitoring(self):
        """Start real-time order book monitoring"""
        def order_book_monitor():
            while True:
                try:
                    # Monitor major trading pairs
                    symbols = ['BTCUSDT', 'ETHUSDT', 'BNBUSDT']
                    
                    for symbol in symbols:
                        order_book = self.get_order_book_snapshot(symbol)
                        if order_book:
                            self.analyze_order_book(symbol, order_book)
                            
                    time.sleep(1)  # 1 second intervals
                except Exception as e:
                    self.logger.error(f"Order book monitoring error: {e}")
                    time.sleep(5)
                    
        threading.Thread(target=order_book_monitor, daemon=True).start()
        
    def start_trade_stream_analysis(self):
        """Start real-time trade stream analysis"""
        def trade_stream_analyzer():
            while True:
                try:
                    # Analyze recent trades for patterns
                    symbols = ['BTCUSDT', 'ETHUSDT', 'BNBUSDT']
                    
                    for symbol in symbols:
                        recent_trades = self.get_recent_trades(symbol)
                        if recent_trades:
                            self.analyze_trading_patterns(symbol, recent_trades)
                            
                    time.sleep(2)  # 2 second intervals
                except Exception as e:
                    self.logger.error(f"Trade stream analysis error: {e}")
                    time.sleep(10)
                    
        threading.Thread(target=trade_stream_analyzer, daemon=True).start()
        
    def start_pattern_detection(self):
        """Start market maker pattern detection"""
        def pattern_detector():
            while True:
                try:
                    self.detect_all_patterns()
                    self.update_confidence_scores()
                    self.check_alert_thresholds()
                    
                    time.sleep(10)  # 10 second intervals
                except Exception as e:
                    self.logger.error(f"Pattern detection error: {e}")
                    time.sleep(30)
                    
        threading.Thread(target=pattern_detector, daemon=True).start()
        
    def start_liquidity_analysis(self):
        """Start liquidity analysis monitoring"""
        def liquidity_analyzer():
            while True:
                try:
                    symbols = ['BTCUSDT', 'ETHUSDT', 'BNBUSDT']
                    
                    for symbol in symbols:
                        self.analyze_liquidity_patterns(symbol)
                        
                    time.sleep(5)  # 5 second intervals
                except Exception as e:
                    self.logger.error(f"Liquidity analysis error: {e}")
                    time.sleep(15)
                    
        threading.Thread(target=liquidity_analyzer, daemon=True).start()
        
    def get_order_book_snapshot(self, symbol):
        """Get order book snapshot for analysis"""
        try:
            # Get order book depth
            order_book = self.binance_client.client.futures_order_book(symbol=symbol, limit=20)
            
            if order_book:
                # Structure order book data
                snapshot = {
                    'symbol': symbol,
                    'timestamp': time.time(),
                    'bids': [(float(bid[0]), float(bid[1])) for bid in order_book['bids']],
                    'asks': [(float(ask[0]), float(ask[1])) for ask in order_book['asks']],
                    'bid_volume': sum(float(bid[1]) for bid in order_book['bids']),
                    'ask_volume': sum(float(ask[1]) for ask in order_book['asks'])
                }
                
                # Store snapshot
                self.market_data['order_book_snapshots'].append(snapshot)
                
                return snapshot
                
            return None
            
        except Exception as e:
            self.logger.error(f"Order book snapshot failed for {symbol}: {e}")
            return None
            
    def get_recent_trades(self, symbol):
        """Get recent trades for analysis"""
        try:
            # Get recent trades
            trades = self.binance_client.client.futures_recent_trades(symbol=symbol, limit=100)
            
            if trades:
                # Structure trade data
                structured_trades = []
                
                for trade in trades:
                    structured_trade = {
                        'symbol': symbol,
                        'price': float(trade['price']),
                        'quantity': float(trade['qty']),
                        'time': int(trade['time']),
                        'is_buyer_maker': trade['isBuyerMaker'],
                        'trade_id': trade['id']
                    }
                    structured_trades.append(structured_trade)
                    
                # Store in trade stream
                self.market_data['trade_stream'].extend(structured_trades)
                
                return structured_trades
                
            return []
            
        except Exception as e:
            self.logger.error(f"Recent trades fetch failed for {symbol}: {e}")
            return []
            
    def analyze_order_book(self, symbol, order_book):
        """Analyze order book for market maker patterns"""
        try:
            # Detect large walls
            large_walls = self.detect_large_walls(order_book)
            
            # Detect spoofing patterns
            spoofing_indicators = self.detect_spoofing(symbol, order_book)
            
            # Analyze bid-ask manipulation
            spread_manipulation = self.analyze_spread_manipulation(order_book)
            
            # Detect iceberg orders
            iceberg_orders = self.detect_iceberg_orders(symbol, order_book)
            
            # Store analysis results
            analysis_result = {
                'timestamp': time.time(),
                'symbol': symbol,
                'large_walls': large_walls,
                'spoofing_indicators': spoofing_indicators,
                'spread_manipulation': spread_manipulation,
                'iceberg_orders': iceberg_orders
            }
            
            # Update market maker signatures
            self.update_mm_signatures(analysis_result)
            
        except Exception as e:
            self.logger.error(f"Order book analysis failed for {symbol}: {e}")
            
    def detect_large_walls(self, order_book):
        """Detect large order walls that may be fake"""
        try:
            large_walls = []
            threshold = self.detection_params['order_book_analysis']['wall_detection_threshold']
            
            # Check bid walls
            for price, volume in order_book['bids']:
                order_value = price * volume
                
                if order_value >= threshold:
                    # Calculate wall characteristics
                    wall_size_ratio = order_value / (order_book['bid_volume'] * price / len(order_book['bids']))
                    
                    large_walls.append({
                        'side': 'bid',
                        'price': price,
                        'volume': volume,
                        'value_usd': order_value,
                        'size_ratio': wall_size_ratio,
                        'distance_from_mid': self.calculate_distance_from_mid(price, order_book)
                    })
                    
            # Check ask walls
            for price, volume in order_book['asks']:
                order_value = price * volume
                
                if order_value >= threshold:
                    wall_size_ratio = order_value / (order_book['ask_volume'] * price / len(order_book['asks']))
                    
                    large_walls.append({
                        'side': 'ask',
                        'price': price,
                        'volume': volume,
                        'value_usd': order_value,
                        'size_ratio': wall_size_ratio,
                        'distance_from_mid': self.calculate_distance_from_mid(price, order_book)
                    })
                    
            return large_walls
            
        except Exception as e:
            self.logger.error(f"Large wall detection failed: {e}")
            return []
            
    def calculate_distance_from_mid(self, price, order_book):
        """Calculate distance from mid price"""
        try:
            best_bid = order_book['bids'][0][0] if order_book['bids'] else 0
            best_ask = order_book['asks'][0][0] if order_book['asks'] else 0
            
            if best_bid > 0 and best_ask > 0:
                mid_price = (best_bid + best_ask) / 2
                return abs(price - mid_price) / mid_price
            else:
                return 0
                
        except Exception as e:
            self.logger.error(f"Distance calculation failed: {e}")
            return 0
            
    def detect_spoofing(self, symbol, order_book):
        """Detect spoofing patterns in order book"""
        try:
            spoofing_indicators = {
                'suspicious_walls': [],
                'spoofing_score': 0,
                'pattern_detected': False
            }
            
            # Get historical order book data for comparison
            recent_snapshots = [
                snapshot for snapshot in self.market_data['order_book_snapshots']
                if snapshot['symbol'] == symbol and 
                time.time() - snapshot['timestamp'] < 60  # Last minute
            ]
            
            if len(recent_snapshots) < 5:
                return spoofing_indicators
                
            # Analyze order placement and cancellation patterns
            large_orders_history = []
            
            for snapshot in recent_snapshots:
                large_walls = self.detect_large_walls(snapshot)
                large_orders_history.append(large_walls)
                
            # Check for orders that appear and disappear quickly
            for i, current_walls in enumerate(large_orders_history[1:], 1):
                prev_walls = large_orders_history[i-1]
                
                # Find walls that disappeared
                for prev_wall in prev_walls:
                    wall_still_exists = any(
                        wall['price'] == prev_wall['price'] and 
                        wall['side'] == prev_wall['side']
                        for wall in current_walls
                    )
                    
                    if not wall_still_exists:
                        # Wall disappeared - potential spoofing
                        time_existed = (recent_snapshots[i]['timestamp'] - 
                                      recent_snapshots[i-1]['timestamp'])
                        
                        if time_existed < self.detection_params['order_book_analysis']['spoofing_time_threshold']:
                            spoofing_indicators['suspicious_walls'].append({
                                'wall': prev_wall,
                                'existence_time': time_existed,
                                'spoofing_likelihood': min(1.0, (5 - time_existed) / 5)
                            })
                            
            # Calculate overall spoofing score
            if spoofing_indicators['suspicious_walls']:
                avg_likelihood = np.mean([
                    wall['spoofing_likelihood'] 
                    for wall in spoofing_indicators['suspicious_walls']
                ])
                
                spoofing_indicators['spoofing_score'] = avg_likelihood
                spoofing_indicators['pattern_detected'] = avg_likelihood > 0.7
                
            return spoofing_indicators
            
        except Exception as e:
            self.logger.error(f"Spoofing detection failed: {e}")
            return {'suspicious_walls': [], 'spoofing_score': 0, 'pattern_detected': False}
            
    def analyze_spread_manipulation(self, order_book):
        """Analyze bid-ask spread for manipulation"""
        try:
            if not order_book['bids'] or not order_book['asks']:
                return {'manipulation_detected': False, 'spread_ratio': 1.0}
                
            best_bid = order_book['bids'][0][0]
            best_ask = order_book['asks'][0][0]
            
            current_spread = (best_ask - best_bid) / best_bid
            
            # Calculate historical average spread
            recent_spreads = []
            
            for snapshot in list(self.market_data['order_book_snapshots'])[-20:]:
                if (snapshot['symbol'] == order_book['symbol'] and 
                    snapshot['bids'] and snapshot['asks']):
                    
                    hist_bid = snapshot['bids'][0][0]
                    hist_ask = snapshot['asks'][0][0]
                    hist_spread = (hist_ask - hist_bid) / hist_bid
                    recent_spreads.append(hist_spread)
                    
            if recent_spreads:
                avg_spread = np.mean(recent_spreads)
                spread_ratio = current_spread / avg_spread if avg_spread > 0 else 1.0
                
                manipulation_threshold = self.detection_params['liquidity_analysis']['bid_ask_manipulation_factor']
                manipulation_detected = spread_ratio > manipulation_threshold
                
                return {
                    'manipulation_detected': manipulation_detected,
                    'spread_ratio': spread_ratio,
                    'current_spread': current_spread,
                    'average_spread': avg_spread
                }
            else:
                return {'manipulation_detected': False, 'spread_ratio': 1.0}
                
        except Exception as e:
            self.logger.error(f"Spread manipulation analysis failed: {e}")
            return {'manipulation_detected': False, 'spread_ratio': 1.0}
            
    def detect_iceberg_orders(self, symbol, order_book):
        """Detect iceberg orders (hidden large orders)"""
        try:
            iceberg_orders = []
            
            # Analyze order size patterns at each price level
            for side in ['bids', 'asks']:
                orders = order_book[side]
                
                for i, (price, volume) in enumerate(orders):
                    # Look for unusually consistent order sizes (iceberg signature)
                    if i > 0:
                        prev_price, prev_volume = orders[i-1]
                        
                        # Check for repeated similar volumes (iceberg chunks)
                        volume_similarity = abs(volume - prev_volume) / max(volume, prev_volume)
                        
                        if volume_similarity < 0.1:  # Very similar volumes
                            iceberg_orders.append({
                                'side': side.rstrip('s'),  # Remove 's'
                                'price': price,
                                'visible_volume': volume,
                                'estimated_total': volume * 5,  # Estimate based on pattern
                                'confidence': 1 - volume_similarity
                            })
                            
            return iceberg_orders
            
        except Exception as e:
            self.logger.error(f"Iceberg order detection failed: {e}")
            return []
            
    def analyze_trading_patterns(self, symbol, trades):
        """Analyze trading patterns for bot behavior"""
        try:
            # Detect ping-pong trading
            ping_pong_score = self.detect_ping_pong_trading(trades)
            
            # Detect wash trading
            wash_trading_score = self.detect_wash_trading(trades)
            
            # Detect momentum ignition
            momentum_ignition = self.detect_momentum_ignition(trades)
            
            # Detect algorithmic patterns
            algo_patterns = self.detect_algorithmic_patterns(trades)
            
            # Store analysis
            pattern_analysis = {
                'symbol': symbol,
                'timestamp': time.time(),
                'ping_pong_score': ping_pong_score,
                'wash_trading_score': wash_trading_score,
                'momentum_ignition': momentum_ignition,
                'algorithmic_patterns': algo_patterns
            }
            
            # Update detection confidence
            self.update_trading_pattern_confidence(pattern_analysis)
            
        except Exception as e:
            self.logger.error(f"Trading pattern analysis failed for {symbol}: {e}")
            
    def detect_ping_pong_trading(self, trades):
        """Detect ping-pong trading patterns"""
        try:
            if len(trades) < 10:
                return 0
                
            # Group trades by price
            price_trades = defaultdict(list)
            
            for trade in trades:
                price_key = round(trade['price'], 4)  # Round to avoid floating point issues
                price_trades[price_key].append(trade)
                
            # Look for alternating buy/sell at same prices
            ping_pong_count = 0
            total_sequences = 0
            
            for price, price_trade_list in price_trades.items():
                if len(price_trade_list) >= 4:  # Minimum for ping-pong
                    # Check for alternating pattern
                    sequence_changes = 0
                    
                    for i in range(1, len(price_trade_list)):
                        curr_side = not price_trade_list[i]['is_buyer_maker']
                        prev_side = not price_trade_list[i-1]['is_buyer_maker']
                        
                        if curr_side != prev_side:
                            sequence_changes += 1
                            
                    alternation_ratio = sequence_changes / (len(price_trade_list) - 1)
                    
                    if alternation_ratio > 0.8:  # 80% alternation
                        ping_pong_count += len(price_trade_list)
                        
                    total_sequences += len(price_trade_list)
                    
            ping_pong_score = ping_pong_count / max(1, total_sequences)
            
            return min(1.0, ping_pong_score * 2)  # Scale to 0-1
            
        except Exception as e:
            self.logger.error(f"Ping-pong detection failed: {e}")
            return 0
            
    def detect_wash_trading(self, trades):
        """Detect wash trading patterns"""
        try:
            if len(trades) < 20:
                return 0
                
            # Analyze volume vs price impact correlation
            volumes = [trade['quantity'] * trade['price'] for trade in trades]
            prices = [trade['price'] for trade in trades]
            
            # Calculate price changes
            price_changes = []
            for i in range(1, len(prices)):
                price_change = abs(prices[i] - prices[i-1]) / prices[i-1]
                price_changes.append(price_change)
                
            if len(price_changes) < 2 or len(volumes) < 2:
                return 0
                
            # High volume with low price impact suggests wash trading
            volume_std = np.std(volumes)
            price_change_std = np.std(price_changes)
            
            if volume_std == 0 or price_change_std == 0:
                return 0
                
            # Calculate volume/impact ratio
            volume_normalized = np.array(volumes[1:]) / volume_std
            price_impact_normalized = np.array(price_changes) / price_change_std
            
            # Inverse correlation suggests wash trading
            correlation = np.corrcoef(volume_normalized, price_impact_normalized)[0, 1]
            
            # Wash trading score (higher when volume doesn't correlate with price impact)
            wash_score = max(0, -correlation)  # Negative correlation = suspicious
            
            return min(1.0, wash_score)
            
        except Exception as e:
            self.logger.error(f"Wash trading detection failed: {e}")
            return 0
            
    def detect_momentum_ignition(self, trades):
        """Detect momentum ignition patterns"""
        try:
            if len(trades) < 10:
                return {'detected': False, 'confidence': 0, 'spike_magnitude': 0}
                
            # Calculate rolling volume in time windows
            window_size = 30  # 30 second windows
            current_time = time.time() * 1000  # Convert to milliseconds
            
            volume_windows = []
            
            for i in range(len(trades)):
                window_start = trades[i]['time']
                window_end = window_start + (window_size * 1000)
                
                window_volume = sum(
                    trade['quantity'] * trade['price']
                    for trade in trades[i:]
                    if window_start <= trade['time'] <= window_end
                )
                
                volume_windows.append(window_volume)
                
            if len(volume_windows) < 3:
                return {'detected': False, 'confidence': 0, 'spike_magnitude': 0}
                
            # Find volume spikes
            avg_volume = np.mean(volume_windows)
            max_volume = max(volume_windows)
            
            spike_magnitude = max_volume / avg_volume if avg_volume > 0 else 1
            
            # Check for rapid price movement during spike
            spike_threshold = self.detection_params['trading_pattern_analysis']['momentum_ignition_volume']
            
            momentum_ignition_detected = (
                max_volume > spike_threshold and
                spike_magnitude > 3.0  # 3x average volume
            )
            
            confidence = min(1.0, spike_magnitude / 10) if momentum_ignition_detected else 0
            
            return {
                'detected': momentum_ignition_detected,
                'confidence': confidence,
                'spike_magnitude': spike_magnitude,
                'max_volume': max_volume,
                'avg_volume': avg_volume
            }
            
        except Exception as e:
            self.logger.error(f"Momentum ignition detection failed: {e}")
            return {'detected': False, 'confidence': 0, 'spike_magnitude': 0}
            
    def detect_algorithmic_patterns(self, trades):
        """Detect algorithmic trading patterns"""
        try:
            algo_indicators = {
                'regular_intervals': 0,
                'round_number_bias': 0,
                'order_size_clustering': 0,
                'response_time_consistency': 0
            }
            
            if len(trades) < 10:
                return algo_indicators
                
            # Regular interval detection
            time_intervals = []
            for i in range(1, len(trades)):
                interval = trades[i]['time'] - trades[i-1]['time']
                time_intervals.append(interval)
                
            if time_intervals:
                # Check for regular patterns
                interval_std = np.std(time_intervals)
                interval_mean = np.mean(time_intervals)
                
                if interval_mean > 0:
                    regularity_score = 1 - (interval_std / interval_mean)
                    algo_indicators['regular_intervals'] = max(0, regularity_score)
                    
            # Round number bias
            round_prices = []
            for trade in trades:
                price = trade['price']
                # Check if price ends in round numbers
                price_str = f"{price:.4f}"
                if price_str.endswith('0000') or price_str.endswith('5000'):
                    round_prices.append(1)
                else:
                    round_prices.append(0)
                    
            if round_prices:
		algo_indicators['round_number_bias'] = np.mean(round_prices)
                
            # Order size clustering
            order_sizes = [trade['quantity'] for trade in trades]
            if len(order_sizes) > 2:
                size_std = np.std(order_sizes)
                size_mean = np.mean(order_sizes)
                
                if size_mean > 0:
                    clustering_score = 1 - (size_std / size_mean)
                    algo_indicators['order_size_clustering'] = max(0, clustering_score)
                    
            return algo_indicators
            
        except Exception as e:
            self.logger.error(f"Algorithmic pattern detection failed: {e}")
            return {'regular_intervals': 0, 'round_number_bias': 0, 'order_size_clustering': 0}
            
    def update_mm_signatures(self, analysis_result):
        """Update market maker signature confidence scores"""
        try:
            symbol = analysis_result['symbol']
            
            # Update spoofing bot signature
            if analysis_result.get('spoofing_indicators', {}).get('pattern_detected', False):
                spoofing_score = analysis_result['spoofing_indicators']['spoofing_score']
                self.mm_signatures['spoofing_bot']['detection_confidence'] = min(1.0, 
                    self.mm_signatures['spoofing_bot']['detection_confidence'] * 0.9 + spoofing_score * 0.1)
                    
                self.mm_signatures['spoofing_bot']['recent_activity'].append({
                    'timestamp': time.time(),
                    'symbol': symbol,
                    'confidence': spoofing_score,
                    'details': analysis_result['spoofing_indicators']
                })
                
            # Update high frequency market maker signature
            if analysis_result.get('iceberg_orders'):
                iceberg_confidence = len(analysis_result['iceberg_orders']) / 10  # Scale by number of orders
                self.mm_signatures['high_frequency_market_maker']['detection_confidence'] = min(1.0,
                    self.mm_signatures['high_frequency_market_maker']['detection_confidence'] * 0.95 + iceberg_confidence * 0.05)
                    
            # Limit recent activity history
            for signature in self.mm_signatures.values():
                if len(signature['recent_activity']) > 50:
                    signature['recent_activity'] = signature['recent_activity'][-50:]
                    
        except Exception as e:
            self.logger.error(f"Market maker signature update failed: {e}")
            
    def update_trading_pattern_confidence(self, pattern_analysis):
        """Update trading pattern confidence scores"""
        try:
            symbol = pattern_analysis['symbol']
            
            # Update wash trading bot signature
            wash_score = pattern_analysis.get('wash_trading_score', 0)
            if wash_score > 0.5:
                self.mm_signatures['wash_trading_bot']['detection_confidence'] = min(1.0,
                    self.mm_signatures['wash_trading_bot']['detection_confidence'] * 0.9 + wash_score * 0.1)
                    
                self.mm_signatures['wash_trading_bot']['recent_activity'].append({
                    'timestamp': time.time(),
                    'symbol': symbol,
                    'wash_score': wash_score
                })
                
            # Update momentum ignition bot signature
            momentum_data = pattern_analysis.get('momentum_ignition', {})
            if momentum_data.get('detected', False):
                momentum_confidence = momentum_data.get('confidence', 0)
                self.mm_signatures['momentum_ignition_bot']['detection_confidence'] = min(1.0,
                    self.mm_signatures['momentum_ignition_bot']['detection_confidence'] * 0.8 + momentum_confidence * 0.2)
                    
                self.mm_signatures['momentum_ignition_bot']['recent_activity'].append({
                    'timestamp': time.time(),
                    'symbol': symbol,
                    'spike_magnitude': momentum_data.get('spike_magnitude', 0),
                    'confidence': momentum_confidence
                })
                
        except Exception as e:
            self.logger.error(f"Trading pattern confidence update failed: {e}")
            
    def detect_all_patterns(self):
        """Run comprehensive pattern detection across all monitored symbols"""
        try:
            symbols = ['BTCUSDT', 'ETHUSDT', 'BNBUSDT']
            
            for symbol in symbols:
                # Detect manipulation patterns
                manipulation_detected = self.detect_manipulation_patterns(symbol)
                
                if manipulation_detected:
                    self.handle_manipulation_detection(symbol, manipulation_detected)
                    
                # Detect coordinated activities
                coordination_detected = self.detect_coordinated_activities(symbol)
                
                if coordination_detected:
                    self.handle_coordination_detection(symbol, coordination_detected)
                    
        except Exception as e:
            self.logger.error(f"Pattern detection failed: {e}")
            
    def detect_manipulation_patterns(self, symbol):
        """Detect market manipulation patterns"""
        try:
            manipulation_signals = {
                'pump_and_dump': self.detect_pump_and_dump(symbol),
                'bear_raid': self.detect_bear_raid(symbol),
                'stop_hunting': self.detect_stop_hunting(symbol)
            }
            
            # Check if any manipulation pattern has high confidence
            for pattern_type, signal in manipulation_signals.items():
                if signal.get('confidence', 0) > 0.7:
                    return {
                        'pattern_type': pattern_type,
                        'confidence': signal['confidence'],
                        'details': signal
                    }
                    
            return None
            
        except Exception as e:
            self.logger.error(f"Manipulation pattern detection failed for {symbol}: {e}")
            return None
            
    def detect_pump_and_dump(self, symbol):
        """Detect pump and dump patterns"""
        try:
            # Get recent price and volume data
            recent_klines = self.binance_client.get_klines(symbol, '1m', 60)  # Last hour
            
            if not recent_klines or len(recent_klines) < 30:
                return {'confidence': 0, 'detected': False}
                
            # Extract price and volume data
            prices = [float(kline[4]) for kline in recent_klines]  # Close prices
            volumes = [float(kline[5]) for kline in recent_klines]  # Volumes
            
            # Calculate price change and volume metrics
            price_start = prices[0]
            price_current = prices[-1]
            price_max = max(prices)
            
            price_increase = (price_max - price_start) / price_start
            current_decline = (price_max - price_current) / price_max if price_max > 0 else 0
            
            # Volume analysis
            avg_volume = np.mean(volumes[:30])  # First half average
            peak_volume = max(volumes)
            volume_spike = peak_volume / avg_volume if avg_volume > 0 else 1
            
            # Pump and dump criteria
            pattern_params = self.manipulation_patterns['pump_and_dump']
            
            pump_detected = (
                price_increase >= pattern_params['price_increase'][0] and  # Min 5% pump
                volume_spike >= pattern_params['volume_increase'][0] and   # Min 5x volume
                current_decline > 0.02  # 2% decline from peak
            )
            
            if pump_detected:
                # Calculate confidence based on pattern strength
                price_confidence = min(1.0, price_increase / pattern_params['price_increase'][1])
                volume_confidence = min(1.0, volume_spike / pattern_params['volume_increase'][1])
                decline_confidence = min(1.0, current_decline / 0.1)  # 10% decline = max confidence
                
                overall_confidence = (price_confidence + volume_confidence + decline_confidence) / 3
                
                return {
                    'detected': True,
                    'confidence': overall_confidence,
                    'price_increase': price_increase,
                    'volume_spike': volume_spike,
                    'current_decline': current_decline,
                    'peak_price': price_max
                }
            else:
                return {'confidence': 0, 'detected': False}
                
        except Exception as e:
            self.logger.error(f"Pump and dump detection failed for {symbol}: {e}")
            return {'confidence': 0, 'detected': False}
            
    def detect_bear_raid(self, symbol):
        """Detect bear raid patterns"""
        try:
            # Get recent order book and trade data
            recent_trades = list(self.market_data['trade_stream'])[-100:]
            symbol_trades = [trade for trade in recent_trades if trade['symbol'] == symbol]
            
            if len(symbol_trades) < 20:
                return {'confidence': 0, 'detected': False}
                
            # Analyze for large coordinated sell orders
            large_sells = []
            total_sell_volume = 0
            total_volume = 0
            
            for trade in symbol_trades:
                volume_usd = trade['price'] * trade['quantity']
                total_volume += volume_usd
                
                if not trade['is_buyer_maker']:  # Sell order
                    total_sell_volume += volume_usd
                    
                    if volume_usd > 50000:  # $50K+ sell order
                        large_sells.append({
                            'price': trade['price'],
                            'volume_usd': volume_usd,
                            'time': trade['time']
                        })
                        
            # Calculate bear raid indicators
            sell_ratio = total_sell_volume / total_volume if total_volume > 0 else 0
            large_sell_count = len(large_sells)
            
            # Check for coordinated timing
            if large_sells:
                sell_times = [sell['time'] for sell in large_sells]
                time_clustering = self.calculate_time_clustering(sell_times)
            else:
                time_clustering = 0
                
            # Bear raid detection criteria
            bear_raid_detected = (
                sell_ratio > 0.8 and  # 80%+ sell pressure
                large_sell_count >= 3 and  # At least 3 large sells
                time_clustering > 0.7  # Coordinated timing
            )
            
            if bear_raid_detected:
                confidence = min(1.0, (sell_ratio + time_clustering + min(1.0, large_sell_count / 10)) / 3)
                
                return {
                    'detected': True,
                    'confidence': confidence,
                    'sell_ratio': sell_ratio,
                    'large_sell_count': large_sell_count,
                    'time_clustering': time_clustering
                }
            else:
                return {'confidence': 0, 'detected': False}
                
        except Exception as e:
            self.logger.error(f"Bear raid detection failed for {symbol}: {e}")
            return {'confidence': 0, 'detected': False}
            
    def detect_stop_hunting(self, symbol):
        """Detect stop hunting patterns"""
        try:
            # Get recent price data with high granularity
            recent_klines = self.binance_client.get_klines(symbol, '1m', 30)
            
            if not recent_klines or len(recent_klines) < 20:
                return {'confidence': 0, 'detected': False}
                
            # Extract OHLC data
            highs = [float(kline[2]) for kline in recent_klines]
            lows = [float(kline[3]) for kline in recent_klines]
            closes = [float(kline[4]) for kline in recent_klines]
            volumes = [float(kline[5]) for kline in recent_klines]
            
            # Identify potential support/resistance levels
            support_levels = []
            resistance_levels = []
            
            # Find local extremes
            for i in range(2, len(lows) - 2):
                # Local low (support)
                if lows[i] <= min(lows[i-2:i]) and lows[i] <= min(lows[i+1:i+3]):
                    support_levels.append(lows[i])
                    
                # Local high (resistance)
                if highs[i] >= max(highs[i-2:i]) and highs[i] >= max(highs[i+1:i+3]):
                    resistance_levels.append(highs[i])
                    
            # Look for spikes through these levels with quick reversals
            stop_hunt_signals = []
            current_price = closes[-1]
            
            # Check recent price action for level breaches
            for i in range(-5, 0):  # Last 5 candles
                candle_high = highs[i]
                candle_low = lows[i]
                candle_close = closes[i]
                candle_volume = volumes[i]
                
                # Check resistance level breaches
                for resistance in resistance_levels:
                    if (candle_high > resistance * 1.002 and  # Spiked through resistance
                        candle_close < resistance and  # But closed below
                        candle_volume > np.mean(volumes) * 1.5):  # With high volume
                        
                        stop_hunt_signals.append({
                            'type': 'resistance_hunt',
                            'level': resistance,
                            'spike_height': (candle_high - resistance) / resistance,
                            'reversal_strength': (candle_high - candle_close) / candle_high
                        })
                        
                # Check support level breaches
                for support in support_levels:
                    if (candle_low < support * 0.998 and  # Spiked through support
                        candle_close > support and  # But closed above
                        candle_volume > np.mean(volumes) * 1.5):  # With high volume
                        
                        stop_hunt_signals.append({
                            'type': 'support_hunt',
                            'level': support,
                            'spike_depth': (support - candle_low) / support,
                            'reversal_strength': (candle_close - candle_low) / candle_close
                        })
                        
            # Calculate stop hunting confidence
            if stop_hunt_signals:
                avg_reversal_strength = np.mean([signal.get('reversal_strength', 0) for signal in stop_hunt_signals])
                signal_count_factor = min(1.0, len(stop_hunt_signals) / 3)
                
                confidence = (avg_reversal_strength + signal_count_factor) / 2
                
                return {
                    'detected': True,
                    'confidence': confidence,
                    'hunt_signals': stop_hunt_signals,
                    'levels_targeted': len(stop_hunt_signals)
                }
            else:
                return {'confidence': 0, 'detected': False}
                
        except Exception as e:
            self.logger.error(f"Stop hunting detection failed for {symbol}: {e}")
            return {'confidence': 0, 'detected': False}
            
    def calculate_time_clustering(self, timestamps):
        """Calculate how clustered timestamps are"""
        try:
            if len(timestamps) < 2:
                return 0
                
            # Convert to seconds and sort
            times = sorted([ts / 1000 for ts in timestamps])
            
            # Calculate intervals between consecutive events
            intervals = []
            for i in range(1, len(times)):
                intervals.append(times[i] - times[i-1])
                
            if not intervals:
                return 0
                
            # High clustering = low variance in intervals
            avg_interval = np.mean(intervals)
            interval_std = np.std(intervals)
            
            if avg_interval == 0:
                return 1.0
                
            # Clustering score (1 = perfectly clustered, 0 = random)
            clustering_score = max(0, 1 - (interval_std / avg_interval))
            
            return clustering_score
            
        except Exception as e:
            self.logger.error(f"Time clustering calculation failed: {e}")
            return 0
            
    def detect_coordinated_activities(self, symbol):
        """Detect coordinated market making activities"""
        try:
            # Analyze cross-exchange coordination
            coordination_score = self.analyze_cross_exchange_coordination(symbol)
            
            # Analyze temporal coordination
            temporal_coordination = self.analyze_temporal_coordination(symbol)
            
            # Analyze volume coordination
            volume_coordination = self.analyze_volume_coordination(symbol)
            
            # Overall coordination assessment
            overall_coordination = (coordination_score + temporal_coordination + volume_coordination) / 3
            
            if overall_coordination > 0.7:
                return {
                    'detected': True,
                    'confidence': overall_coordination,
                    'cross_exchange_score': coordination_score,
                    'temporal_score': temporal_coordination,
                    'volume_score': volume_coordination
                }
            else:
                return None
                
        except Exception as e:
            self.logger.error(f"Coordinated activities detection failed for {symbol}: {e}")
            return None
            
    def analyze_cross_exchange_coordination(self, symbol):
        """Analyze coordination across exchanges (simulated)"""
        try:
            # In production, this would analyze actual cross-exchange data
            # For demo, simulate coordination detection
            
            coordination_indicators = []
            
            # Simulate price movements correlation across exchanges
            binance_price_changes = np.random.normal(0, 0.01, 10)  # 10 recent price changes
            other_exchange_changes = binance_price_changes + np.random.normal(0, 0.002, 10)  # Correlated with noise
            
            # Calculate correlation
            if len(binance_price_changes) == len(other_exchange_changes):
                correlation = np.corrcoef(binance_price_changes, other_exchange_changes)[0, 1]
                coordination_score = max(0, correlation)
            else:
                coordination_score = 0
                
            return coordination_score
            
        except Exception as e:
            self.logger.error(f"Cross-exchange coordination analysis failed: {e}")
            return 0
            
    def analyze_temporal_coordination(self, symbol):
        """Analyze temporal coordination in trading patterns"""
        try:
            # Get recent trades
            recent_trades = [
                trade for trade in self.market_data['trade_stream']
                if trade['symbol'] == symbol and time.time() * 1000 - trade['time'] < 300000  # Last 5 minutes
            ]
            
            if len(recent_trades) < 10:
                return 0
                
            # Analyze timing patterns
            trade_times = [trade['time'] for trade in recent_trades]
            
            # Check for regular intervals (bot behavior)
            intervals = []
            for i in range(1, len(trade_times)):
                interval = trade_times[i] - trade_times[i-1]
                intervals.append(interval)
                
            if not intervals:
                return 0
                
            # Calculate temporal regularity
            interval_std = np.std(intervals)
            interval_mean = np.mean(intervals)
            
            if interval_mean == 0:
                return 0
                
            regularity_score = max(0, 1 - (interval_std / interval_mean))
            
            # High regularity suggests coordination/bots
            return min(1.0, regularity_score * 2)
            
        except Exception as e:
            self.logger.error(f"Temporal coordination analysis failed: {e}")
            return 0
            
    def analyze_volume_coordination(self, symbol):
        """Analyze volume coordination patterns"""
        try:
            # Get recent order book snapshots
            recent_snapshots = [
                snapshot for snapshot in self.market_data['order_book_snapshots']
                if snapshot['symbol'] == symbol and time.time() - snapshot['timestamp'] < 300  # Last 5 minutes
            ]
            
            if len(recent_snapshots) < 5:
                return 0
                
            # Analyze bid/ask volume coordination
            bid_volumes = [snapshot['bid_volume'] for snapshot in recent_snapshots]
            ask_volumes = [snapshot['ask_volume'] for snapshot in recent_snapshots]
            
            # Calculate volume balance consistency
            volume_ratios = []
            for bid_vol, ask_vol in zip(bid_volumes, ask_volumes):
                if ask_vol > 0:
                    ratio = bid_vol / ask_vol
                    volume_ratios.append(ratio)
                    
            if not volume_ratios:
                return 0
                
            # Consistent ratios suggest coordinated market making
            ratio_std = np.std(volume_ratios)
            ratio_mean = np.mean(volume_ratios)
            
            if ratio_mean == 0:
                return 0
                
            consistency_score = max(0, 1 - (ratio_std / ratio_mean))
            
            return min(1.0, consistency_score)
            
        except Exception as e:
            self.logger.error(f"Volume coordination analysis failed: {e}")
            return 0
            
    def analyze_liquidity_patterns(self, symbol):
        """Analyze liquidity provision patterns"""
        try:
            # Get current order book
            order_book = self.get_order_book_snapshot(symbol)
            
            if not order_book:
                return
                
            # Analyze liquidity provision behavior
            liquidity_analysis = {
                'total_bid_liquidity': order_book['bid_volume'],
                'total_ask_liquidity': order_book['ask_volume'],
                'liquidity_imbalance': self.calculate_liquidity_imbalance(order_book),
                'spread_tightness': self.calculate_spread_tightness(order_book),
                'depth_distribution': self.analyze_depth_distribution(order_book)
            }
            
            # Store liquidity metrics
            self.market_data['liquidity_metrics'][symbol] = liquidity_analysis
            
            # Detect liquidity provider patterns
            self.detect_liquidity_provider_behavior(symbol, liquidity_analysis)
            
        except Exception as e:
            self.logger.error(f"Liquidity pattern analysis failed for {symbol}: {e}")
            
    def calculate_liquidity_imbalance(self, order_book):
        """Calculate bid/ask liquidity imbalance"""
        try:
            bid_volume = order_book['bid_volume']
            ask_volume = order_book['ask_volume']
            
            total_volume = bid_volume + ask_volume
            
            if total_volume == 0:
                return 0
                
            # Imbalance: positive = more bids, negative = more asks
            imbalance = (bid_volume - ask_volume) / total_volume
            
            return imbalance
            
        except Exception as e:
            self.logger.error(f"Liquidity imbalance calculation failed: {e}")
            return 0
            
    def calculate_spread_tightness(self, order_book):
        """Calculate bid-ask spread tightness"""
        try:
            if not order_book['bids'] or not order_book['asks']:
                return 0
                
            best_bid = order_book['bids'][0][0]
            best_ask = order_book['asks'][0][0]
            
            spread = (best_ask - best_bid) / best_bid
            
            # Tightness = inverse of spread (higher = tighter)
            tightness = 1 / (1 + spread * 1000)  # Scale for reasonable values
            
            return tightness
            
        except Exception as e:
            self.logger.error(f"Spread tightness calculation failed: {e}")
            return 0
            
    def analyze_depth_distribution(self, order_book):
        """Analyze order book depth distribution"""
        try:
            # Calculate cumulative volume at different price levels
            depth_levels = [1, 5, 10, 20]  # Price levels to analyze
            depth_distribution = {}
            
            for level in depth_levels:
                if len(order_book['bids']) >= level and len(order_book['asks']) >= level:
                    bid_depth = sum(volume for _, volume in order_book['bids'][:level])
                    ask_depth = sum(volume for _, volume in order_book['asks'][:level])
                    
                    depth_distribution[f'level_{level}'] = {
                        'bid_depth': bid_depth,
                        'ask_depth': ask_depth,
                        'total_depth': bid_depth + ask_depth
                    }
                    
            return depth_distribution
            
        except Exception as e:
            self.logger.error(f"Depth distribution analysis failed: {e}")
            return {}
            
    def detect_liquidity_provider_behavior(self, symbol, liquidity_analysis):
        """Detect legitimate liquidity provider behavior"""
        try:
            # Characteristics of legitimate liquidity providers
            characteristics = {
                'consistent_two_sided_quotes': False,
                'tight_spreads': False,
                'balanced_depth': False,
                'stable_provision': False
            }
            
            # Check spread tightness
            spread_tightness = liquidity_analysis.get('spread_tightness', 0)
            characteristics['tight_spreads'] = spread_tightness > 0.8
            
            # Check liquidity balance
            imbalance = abs(liquidity_analysis.get('liquidity_imbalance', 0))
            characteristics['balanced_depth'] = imbalance < 0.2  # Less than 20% imbalance
            
            # Update liquidity provider signature
            legitimate_indicators = sum(characteristics.values())
            lp_confidence = legitimate_indicators / len(characteristics)
            
            self.mm_signatures['liquidity_provider']['detection_confidence'] = min(1.0,
                self.mm_signatures['liquidity_provider']['detection_confidence'] * 0.9 + lp_confidence * 0.1)
                
        except Exception as e:
            self.logger.error(f"Liquidity provider behavior detection failed: {e}")
            
    def update_confidence_scores(self):
        """Update overall confidence scores for all signatures"""
        try:
            current_time = time.time()
            
            for signature_name, signature_data in self.mm_signatures.items():
                # Apply time decay to confidence scores
                time_since_last_activity = current_time - signature_data.get('last_activity', current_time)
                decay_factor = max(0.5, 1 - (time_since_last_activity / 3600))  # Decay over 1 hour
                
                signature_data['detection_confidence'] *= decay_factor
                
                # Update statistics
                confidence = signature_data['detection_confidence']
                self.detection_stats['confidence_scores'][signature_name].append(confidence)
                
                # Limit confidence history
                if len(self.detection_stats['confidence_scores'][signature_name]) > 100:
                    self.detection_stats['confidence_scores'][signature_name].pop(0)
                    
        except Exception as e:
            self.logger.error(f"Confidence score update failed: {e}")
            
    def check_alert_thresholds(self):
        """Check if any detection confidence exceeds alert thresholds"""
        try:
            for signature_name, signature_data in self.mm_signatures.items():
                confidence = signature_data['detection_confidence']
                
                # Check against thresholds
                if signature_name == 'spoofing_bot' and confidence > self.alert_thresholds['spoofing_confidence']:
                    self.generate_detection_alert('spoofing', confidence, signature_data)
                    
                elif signature_name == 'wash_trading_bot' and confidence > self.alert_thresholds['wash_trading_confidence']:
                    self.generate_detection_alert('wash_trading', confidence, signature_data)
                    
                elif signature_name == 'momentum_ignition_bot' and confidence > self.alert_thresholds['momentum_ignition_confidence']:
                    self.generate_detection_alert('momentum_ignition', confidence, signature_data)
                    
        except Exception as e:
            self.logger.error(f"Alert threshold check failed: {e}")
            
    def generate_detection_alert(self, detection_type, confidence, signature_data):
        """Generate alert for market maker detection"""
        try:
            alert_data = {
                'type': 'market_maker_detection',
                'detection_type': detection_type,
                'confidence': confidence,
                'timestamp': time.time(),
                'recent_activity': signature_data.get('recent_activity', [])[-3:],  # Last 3 activities
                'recommendation': self.get_detection_recommendation(detection_type, confidence)
            }
            
            # Store alert
            self.market_data['suspicious_activities'].append(alert_data)
            
            # Log alert
            self.logger.warning(f"ü§ñ MARKET MAKER DETECTED: {detection_type.upper()} (confidence: {confidence:.1%})")
            
            # Update detection statistics
            self.detection_stats['total_detections'][detection_type] += 1
            self.detection_stats['last_detection_time'][detection_type] = time.time()
            
        except Exception as e:
            self.logger.error(f"Detection alert generation failed: {e}")
            
    def get_detection_recommendation(self, detection_type, confidence):
        """Get trading recommendation based on detection"""
        try:
            recommendations = {
                'spoofing': {
                    'high_confidence': 'Avoid trading near large walls, wait for genuine breakouts',
                    'medium_confidence': 'Exercise caution with large orders in order book',
                    'action': 'ignore_fake_walls'
                },
                'wash_trading': {
                    'high_confidence': 'Ignore artificial volume spikes, focus on genuine price action',
                    'medium_confidence': 'Verify volume with multiple indicators',
                    'action': 'discount_volume'
                },
                'momentum_ignition': {
                    'high_confidence': 'Avoid FOMO entries, wait for genuine momentum confirmation',
                    'medium_confidence': 'Require additional confirmation before following moves',
                    'action': 'wait_for_confirmation'
                }
            }
            
            if detection_type in recommendations:
                if confidence > 0.8:
                    return recommendations[detection_type]['high_confidence']
                else:
                    return recommendations[detection_type]['medium_confidence']
            else:
                return 'Monitor market conditions closely'
                
        except Exception as e:
            self.logger.error(f"Detection recommendation failed: {e}")
            return 'Exercise general caution'
            
    def handle_manipulation_detection(self, symbol, manipulation_data):
        """Handle detected market manipulation"""
        try:
            pattern_type = manipulation_data['pattern_type']
            confidence = manipulation_data['confidence']
            
            self.logger.warning(f"üö® MANIPULATION DETECTED: {pattern_type.upper()} in {symbol} (confidence: {confidence:.1%})")
            
            # Store manipulation event
            manipulation_event = {
                'symbol': symbol,
                'pattern_type': pattern_type,
                'confidence': confidence,
                'timestamp': time.time(),
                'details': manipulation_data['details']
            }
            
	    self.market_data['suspicious_activities'].append(manipulation_event)
            
            # Generate trading recommendation
            recommendation = self.get_manipulation_recommendation(pattern_type, confidence)
            
            # Update bot behavior if needed
            self.adapt_to_manipulation(symbol, pattern_type, recommendation)
            
        except Exception as e:
            self.logger.error(f"Manipulation detection handling failed: {e}")
            
    def handle_coordination_detection(self, symbol, coordination_data):
        """Handle detected coordinated activities"""
        try:
            confidence = coordination_data['confidence']
            
            self.logger.warning(f"ü§ù COORDINATED ACTIVITY DETECTED in {symbol} (confidence: {confidence:.1%})")
            
            # Store coordination event
            coordination_event = {
                'symbol': symbol,
                'confidence': confidence,
                'timestamp': time.time(),
                'cross_exchange_score': coordination_data['cross_exchange_score'],
                'temporal_score': coordination_data['temporal_score'],
                'volume_score': coordination_data['volume_score']
            }
            
            self.market_data['suspicious_activities'].append(coordination_event)
            
        except Exception as e:
            self.logger.error(f"Coordination detection handling failed: {e}")
            
    def get_manipulation_recommendation(self, pattern_type, confidence):
        """Get recommendation for detected manipulation"""
        try:
            recommendations = {
                'pump_and_dump': {
                    'avoid_fomo': True,
                    'wait_for_stabilization': True,
                    'reduce_position_size': True,
                    'action': 'avoid_trading'
                },
                'bear_raid': {
                    'avoid_panic_selling': True,
                    'look_for_oversold_bounce': True,
                    'wait_for_volume_normalization': True,
                    'action': 'wait_or_counter_trade'
                },
                'stop_hunting': {
                    'avoid_tight_stops': True,
                    'use_wider_stop_losses': True,
                    'wait_for_genuine_breakout': True,
                    'action': 'adjust_risk_management'
                }
            }
            
            return recommendations.get(pattern_type, {'action': 'exercise_caution'})
            
        except Exception as e:
            self.logger.error(f"Manipulation recommendation failed: {e}")
            return {'action': 'exercise_caution'}
            
    def adapt_to_manipulation(self, symbol, pattern_type, recommendation):
        """Adapt trading behavior based on detected manipulation"""
        try:
            # This would integrate with trading systems to adjust behavior
            # For now, log the adaptation
            
            action = recommendation.get('action', 'exercise_caution')
            
            adaptation_message = f"Adapting to {pattern_type} in {symbol}: {action}"
            self.logger.info(f"üîß {adaptation_message}")
            
            # Store adaptation for trading system
            adaptation_data = {
                'symbol': symbol,
                'pattern_type': pattern_type,
                'action': action,
                'timestamp': time.time(),
                'duration': 3600  # 1 hour adaptation period
            }
            
            # This would be used by trading systems to modify behavior
            self.memory_manager.store_manipulation_adaptation(adaptation_data)
            
        except Exception as e:
            self.logger.error(f"Manipulation adaptation failed: {e}")
            
    def get_market_maker_summary(self):
        """Get comprehensive market maker detection summary"""
        try:
            summary = {
                'detection_overview': {
                    'total_signatures': len(self.mm_signatures),
                    'active_detections': len([s for s in self.mm_signatures.values() if s['detection_confidence'] > 0.5]),
                    'high_confidence_detections': len([s for s in self.mm_signatures.values() if s['detection_confidence'] > 0.8])
                },
                'signature_confidence': {
                    name: {
                        'confidence': data['detection_confidence'],
                        'recent_activity_count': len(data['recent_activity']),
                        'last_detection': max([activity.get('timestamp', 0) for activity in data['recent_activity']]) if data['recent_activity'] else 0
                    }
                    for name, data in self.mm_signatures.items()
                },
                'recent_suspicious_activities': list(self.market_data['suspicious_activities'])[-10:],
                'detection_statistics': {
                    'total_detections_by_type': dict(self.detection_stats['total_detections']),
                    'average_confidence_by_type': {
                        sig_type: np.mean(confidences) if confidences else 0
                        for sig_type, confidences in self.detection_stats['confidence_scores'].items()
                    }
                },
                'market_health_assessment': self.assess_market_health()
            }
            
            return summary
            
        except Exception as e:
            self.logger.error(f"Market maker summary generation failed: {e}")
            return {}
            
    def assess_market_health(self):
        """Assess overall market health based on detections"""
        try:
            # Calculate overall manipulation risk
            high_confidence_detections = [
                s for s in self.mm_signatures.values() 
                if s['detection_confidence'] > 0.7
            ]
            
            recent_suspicious_activities = [
                activity for activity in self.market_data['suspicious_activities']
                if time.time() - activity.get('timestamp', 0) < 3600  # Last hour
            ]
            
            # Market health scoring
            manipulation_risk = min(1.0, len(high_confidence_detections) / 3)  # 3+ = high risk
            recent_activity_risk = min(1.0, len(recent_suspicious_activities) / 5)  # 5+ = high risk
            
            overall_risk = (manipulation_risk + recent_activity_risk) / 2
            
            # Health assessment
            if overall_risk < 0.3:
                health_status = 'healthy'
                recommendation = 'Normal trading conditions'
            elif overall_risk < 0.6:
                health_status = 'caution'
                recommendation = 'Exercise increased caution'
            else:
                health_status = 'unhealthy'
                recommendation = 'High manipulation risk - consider reducing activity'
                
            return {
                'status': health_status,
                'risk_score': overall_risk,
                'manipulation_risk': manipulation_risk,
                'recent_activity_risk': recent_activity_risk,
                'recommendation': recommendation,
                'active_threats': len(high_confidence_detections),
                'recent_incidents': len(recent_suspicious_activities)
            }
            
        except Exception as e:
            self.logger.error(f"Market health assessment failed: {e}")
            return {'status': 'unknown', 'risk_score': 0.5}
            
    def get_symbol_risk_assessment(self, symbol):
        """Get risk assessment for specific symbol"""
        try:
            # Get symbol-specific suspicious activities
            symbol_activities = [
                activity for activity in self.market_data['suspicious_activities']
                if activity.get('symbol') == symbol and 
                time.time() - activity.get('timestamp', 0) < 1800  # Last 30 minutes
            ]
            
            # Get current market maker confidence for symbol
            mm_risks = []
            
            for signature_name, signature_data in self.mm_signatures.items():
                recent_symbol_activity = [
                    activity for activity in signature_data['recent_activity']
                    if activity.get('symbol') == symbol and
                    time.time() - activity.get('timestamp', 0) < 1800
                ]
                
                if recent_symbol_activity:
                    avg_confidence = np.mean([activity.get('confidence', 0) for activity in recent_symbol_activity])
                    mm_risks.append(avg_confidence)
                    
            # Calculate symbol risk
            activity_risk = min(1.0, len(symbol_activities) / 3)
            mm_risk = np.mean(mm_risks) if mm_risks else 0
            
            overall_symbol_risk = (activity_risk + mm_risk) / 2
            
            # Risk categorization
            if overall_symbol_risk < 0.3:
                risk_level = 'low'
                trading_advice = 'Normal trading'
            elif overall_symbol_risk < 0.6:
                risk_level = 'medium'
                trading_advice = 'Exercise caution'
            else:
                risk_level = 'high'
                trading_advice = 'Avoid or reduce exposure'
                
            return {
                'symbol': symbol,
                'risk_level': risk_level,
                'risk_score': overall_symbol_risk,
                'trading_advice': trading_advice,
                'recent_incidents': len(symbol_activities),
                'mm_detection_risk': mm_risk,
                'last_incident': max([a.get('timestamp', 0) for a in symbol_activities]) if symbol_activities else 0
            }
            
        except Exception as e:
            self.logger.error(f"Symbol risk assessment failed for {symbol}: {e}")
            return {'symbol': symbol, 'risk_level': 'unknown', 'risk_score': 0.5}
            
    def get_trading_recommendations(self):
        """Get trading recommendations based on current detections"""
        try:
            recommendations = []
            
            # General market recommendations
            market_health = self.assess_market_health()
            
            if market_health['status'] == 'unhealthy':
                recommendations.append({
                    'type': 'market_wide',
                    'priority': 'high',
                    'message': 'High manipulation risk detected - reduce overall trading activity',
                    'specific_actions': [
                        'Reduce position sizes by 50%',
                        'Increase stop-loss distances',
                        'Avoid FOMO entries',
                        'Wait for cleaner setups'
                    ]
                })
                
            # Symbol-specific recommendations
            symbols = ['BTCUSDT', 'ETHUSDT', 'BNBUSDT']
            
            for symbol in symbols:
                symbol_risk = self.get_symbol_risk_assessment(symbol)
                
                if symbol_risk['risk_level'] == 'high':
                    recommendations.append({
                        'type': 'symbol_specific',
                        'symbol': symbol,
                        'priority': 'medium',
                        'message': f'High manipulation risk in {symbol}',
                        'specific_actions': [
                            f'Avoid new {symbol} positions',
                            'Close existing positions if profitable',
                            'Wait for risk reduction'
                        ]
                    })
                    
            # Pattern-specific recommendations
            for signature_name, signature_data in self.mm_signatures.items():
                if signature_data['detection_confidence'] > 0.8:
                    
                    if signature_name == 'spoofing_bot':
                        recommendations.append({
                            'type': 'pattern_specific',
                            'pattern': 'spoofing',
                            'priority': 'medium',
                            'message': 'High spoofing activity detected',
                            'specific_actions': [
                                'Ignore large walls in order book',
                                'Wait for wall removal before trading',
                                'Use market orders carefully'
                            ]
                        })
                        
                    elif signature_name == 'wash_trading_bot':
                        recommendations.append({
                            'type': 'pattern_specific',
                            'pattern': 'wash_trading',
                            'priority': 'low',
                            'message': 'Wash trading detected - artificial volume',
                            'specific_actions': [
                                'Discount recent volume spikes',
                                'Focus on price action over volume',
                                'Verify breakouts with multiple confirmations'
                            ]
                        })
                        
            return recommendations
            
        except Exception as e:
            self.logger.error(f"Trading recommendations generation failed: {e}")
            return []
            
    def export_detection_data(self, filename=None):
        """Export market maker detection data"""
        try:
            if filename is None:
                filename = f"mm_detection_data_{int(time.time())}.json"
                
            export_data = {
                'export_timestamp': time.time(),
                'detection_parameters': self.detection_params,
                'mm_signatures': self.mm_signatures,
                'manipulation_patterns': self.manipulation_patterns,
                'detection_statistics': dict(self.detection_stats),
                'recent_suspicious_activities': list(self.market_data['suspicious_activities']),
                'market_health_assessment': self.assess_market_health(),
                'trading_recommendations': self.get_trading_recommendations()
            }
            
            # Store via memory manager
            self.memory_manager.export_mm_detection_data(export_data, filename)
            
            self.logger.info(f"üìÅ Market maker detection data exported: {filename}")
            return filename
            
        except Exception as e:
            self.logger.error(f"MM detection data export failed: {e}")
            return None
            
    def update_detection_accuracy(self, detection_type, was_accurate):
        """Update detection accuracy statistics"""
        try:
            # Track accuracy for machine learning improvement
            current_accuracy = self.detection_stats['detection_accuracy'].get(detection_type, 0.5)
            
            # Simple moving average update
            update_factor = 0.1
            new_accuracy = current_accuracy * (1 - update_factor) + (1.0 if was_accurate else 0.0) * update_factor
            
            self.detection_stats['detection_accuracy'][detection_type] = new_accuracy
            
            # Adjust detection thresholds based on accuracy
            if new_accuracy < 0.6:  # Poor accuracy
                # Increase thresholds to reduce false positives
                if detection_type in self.alert_thresholds:
                    threshold_key = f"{detection_type}_confidence"
                    if threshold_key in self.alert_thresholds:
                        self.alert_thresholds[threshold_key] = min(0.95, 
                            self.alert_thresholds[threshold_key] * 1.05)
                            
            elif new_accuracy > 0.8:  # High accuracy
                # Lower thresholds to catch more patterns
                if detection_type in self.alert_thresholds:
                    threshold_key = f"{detection_type}_confidence"
                    if threshold_key in self.alert_thresholds:
                        self.alert_thresholds[threshold_key] = max(0.5,
                            self.alert_thresholds[threshold_key] * 0.95)
                            
        except Exception as e:
            self.logger.error(f"Detection accuracy update failed: {e}")
            
    def cleanup_old_detection_data(self, hours_to_keep=24):
        """Clean up old detection data to manage memory"""
        try:
            cutoff_time = time.time() - (hours_to_keep * 3600)
            
            # Clean order book snapshots
            self.market_data['order_book_snapshots'] = deque([
                snapshot for snapshot in self.market_data['order_book_snapshots']
                if snapshot.get('timestamp', time.time()) > cutoff_time
            ], maxlen=1000)
            
            # Clean trade stream
            self.market_data['trade_stream'] = deque([
                trade for trade in self.market_data['trade_stream']
                if trade.get('time', time.time() * 1000) > cutoff_time * 1000
            ], maxlen=5000)
            
            # Clean suspicious activities
            self.market_data['suspicious_activities'] = deque([
                activity for activity in self.market_data['suspicious_activities']
                if activity.get('timestamp', time.time()) > cutoff_time
            ], maxlen=500)
            
            # Clean recent activity from signatures
            for signature_data in self.mm_signatures.values():
                signature_data['recent_activity'] = [
                    activity for activity in signature_data['recent_activity']
                    if activity.get('timestamp', time.time()) > cutoff_time
                ]
                
            self.logger.info("üßπ Cleaned up old market maker detection data")
            
        except Exception as e:
            self.logger.error(f"MM detection data cleanup failed: {e}")
            
    def get_real_time_detection_status(self):
        """Get real-time detection status for dashboard"""
        try:
            return {
                'timestamp': time.time(),
                'monitoring_status': 'active',
                'symbols_monitored': ['BTCUSDT', 'ETHUSDT', 'BNBUSDT'],
                'detection_confidence': {
                    name: data['detection_confidence']
                    for name, data in self.mm_signatures.items()
                },
                'recent_alerts': len([
                    activity for activity in self.market_data['suspicious_activities']
                    if time.time() - activity.get('timestamp', 0) < 300  # Last 5 minutes
                ]),
                'market_health': self.assess_market_health()['status'],
                'data_points': {
                    'order_book_snapshots': len(self.market_data['order_book_snapshots']),
                    'trade_stream_size': len(self.market_data['trade_stream']),
                    'suspicious_activities': len(self.market_data['suspicious_activities'])
                }
            }
            
        except Exception as e:
            self.logger.error(f"Real-time detection status failed: {e}")
            return {'monitoring_status': 'error', 'timestamp': time.time()}

class FundingRateArbitrageEngine:
    """Advanced funding rate arbitrage engine for capturing risk-free returns from perpetual futures"""
    
    def __init__(self, config, binance_client, memory_manager, performance_analytics):
        self.config = config
        self.binance_client = binance_client
        self.memory_manager = memory_manager
        self.performance_analytics = performance_analytics
        self.logger = logging.getLogger('MonsterBot.FundingRateArbitrage')
        
        # Funding rate arbitrage configuration
        self.arbitrage_config = {
            'target_symbols': [
                'BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'ADAUSDT', 'SOLUSDT', 
                'AVAXUSDT', 'MATICUSDT', 'DOTUSDT', 'LINKUSDT', 'UNIUSDT'
            ],
            'min_funding_rate': 0.0001,      # 0.01% minimum rate for consideration
            'max_funding_rate': 0.0075,      # 0.75% maximum rate (risk management)
            'funding_frequency': 8,          # Hours between funding payments
            'annual_target_return': 0.15,    # 15% annual target from funding
            'max_position_percentage': 0.4,  # 40% of capital per position
            'hedge_threshold': 0.95,         # 95% hedge ratio minimum
            'rebalancing_threshold': 0.05,   # 5% threshold for rebalancing
            'funding_prediction_lookback': 168  # 1 week of hourly data
        }
        
        # Exchange configurations for arbitrage
        self.exchanges = {
            'binance_futures': {
                'client': self.binance_client,
                'funding_intervals': 8,  # Every 8 hours
                'funding_times': [0, 8, 16],  # UTC hours
                'position_limits': {'BTCUSDT': 1000000, 'ETHUSDT': 500000},  # USD limits
                'maker_fee': 0.0002,
                'taker_fee': 0.0004,
                'enabled': True
            },
            'binance_spot': {
                'client': self.binance_client,
                'funding_intervals': 0,  # No funding on spot
                'position_limits': {'BTCUSDT': 500000, 'ETHUSDT': 250000},
                'maker_fee': 0.001,
                'taker_fee': 0.001,
                'enabled': True
            }
            # Additional exchanges would be configured here in production
        }
        
        # Funding rate tracking
        self.funding_data = defaultdict(lambda: {
            'current_rate': 0,
            'predicted_rate': 0,
            'rate_history': deque(maxlen=168),  # 1 week hourly
            'next_funding_time': 0,
            'annual_rate': 0,
            'volatility': 0,
            'trend': 'neutral'
        })
        
        # Active arbitrage positions
        self.active_positions = defaultdict(lambda: {
            'symbol': '',
            'entry_time': 0,
            'position_size_usd': 0,
            'funding_rate_entry': 0,
            'expected_daily_return': 0,
            'hedge_ratio': 0,
            'total_funding_received': 0,
            'position_pnl': 0,
            'status': 'inactive'
        })
        
        # Arbitrage opportunities tracking
        self.opportunities = defaultdict(lambda: {
            'rate_differential': 0,
            'annual_yield': 0,
            'risk_score': 0,
            'confidence': 0,
            'optimal_size': 0,
            'entry_urgency': 'low',
            'last_updated': 0
        })
        
        # Performance tracking
        self.performance_tracking = {
            'total_funding_earned': 0,
            'total_positions_opened': 0,
            'average_daily_return': 0,
            'total_fees_paid': 0,
            'net_profit': 0,
            'sharpe_ratio': 0,
            'max_drawdown': 0,
            'win_rate': 0
        }
        
        # Risk management parameters
        self.risk_params = {
            'max_total_exposure': 0.8,        # 80% of capital max
            'correlation_limit': 0.7,         # Max correlation between positions
            'funding_rate_volatility_limit': 0.002,  # 0.2% max volatility
            'position_duration_limit': 30,    # 30 days max position hold
            'emergency_exit_threshold': -0.05, # 5% loss triggers exit
            'minimum_hedge_ratio': 0.9        # 90% minimum hedge
        }
        
        # Funding rate prediction models
        self.prediction_models = {
            'time_series': {
                'enabled': True,
                'lookback_periods': 168,
                'model_accuracy': 0.7,
                'prediction_horizon': 24  # 24 hours ahead
            },
            'market_sentiment': {
                'enabled': True,
                'sentiment_weight': 0.3,
                'volatility_weight': 0.4,
                'volume_weight': 0.3
            },
            'cross_exchange': {
                'enabled': False,  # Would be enabled with multiple exchanges
                'spread_threshold': 0.0005,
                'execution_delay': 2.0
            }
        }
        
        # Initialize funding rate arbitrage system
        self.initialize_funding_arbitrage()
        
    def initialize_funding_arbitrage(self):
        """Initialize funding rate arbitrage system"""
        try:
            # Load historical funding rates
            self.load_historical_funding_rates()
            
            # Start funding rate monitoring
            self.start_funding_rate_monitoring()
            
            # Start opportunity scanning
            self.start_opportunity_scanning()
            
            # Start position management
            self.start_position_management()
            
            # Start funding rate prediction
            self.start_funding_prediction()
            
            self.logger.info("üí∞ Funding Rate Arbitrage Engine initialized")
            self.logger.info(f"   üìä Monitoring {len(self.arbitrage_config['target_symbols'])} symbols")
            self.logger.info(f"   üéØ Target annual return: {self.arbitrage_config['annual_target_return']:.1%}")
            self.logger.info(f"   ‚ö° Funding frequency: Every {self.arbitrage_config['funding_frequency']} hours")
            
        except Exception as e:
            self.logger.error(f"Funding arbitrage initialization failed: {e}")
            
    def load_historical_funding_rates(self):
        """Load historical funding rate data"""
        try:
            for symbol in self.arbitrage_config['target_symbols']:
                # Load from memory manager
                historical_rates = self.memory_manager.get_funding_rate_history(symbol)
                
                if historical_rates:
                    self.funding_data[symbol]['rate_history'].extend(historical_rates)
                    self.logger.info(f"üìö Loaded {len(historical_rates)} historical funding rates for {symbol}")
                else:
                    # Fetch fresh data
                    self.fetch_historical_funding_rates(symbol)
                    
        except Exception as e:
            self.logger.error(f"Historical funding rate loading failed: {e}")
            
    def fetch_historical_funding_rates(self, symbol):
        """Fetch historical funding rates from exchange"""
        try:
            # Get funding rate history
            funding_history = self.binance_client.client.futures_funding_rate(
                symbol=symbol, 
                limit=500  # Last 500 funding periods
            )
            
            if funding_history:
                for funding_record in funding_history:
                    rate_data = {
                        'timestamp': int(funding_record['fundingTime']),
                        'funding_rate': float(funding_record['fundingRate']),
                        'symbol': symbol
                    }
                    
                    self.funding_data[symbol]['rate_history'].append(rate_data)
                    
                self.logger.info(f"üìà Fetched {len(funding_history)} funding rates for {symbol}")
                
                # Store in memory manager
                self.memory_manager.store_funding_rate_history(symbol, funding_history)
                
        except Exception as e:
            self.logger.error(f"Historical funding rate fetch failed for {symbol}: {e}")
            
    def start_funding_rate_monitoring(self):
        """Start real-time funding rate monitoring"""
        def funding_monitor():
            while True:
                try:
                    for symbol in self.arbitrage_config['target_symbols']:
                        self.update_current_funding_rate(symbol)
                        
                    time.sleep(300)  # Update every 5 minutes
                except Exception as e:
                    self.logger.error(f"Funding rate monitoring error: {e}")
                    time.sleep(600)
                    
        threading.Thread(target=funding_monitor, daemon=True).start()
        
    def start_opportunity_scanning(self):
        """Start opportunity scanning for funding arbitrage"""
        def opportunity_scanner():
            while True:
                try:
                    self.scan_funding_opportunities()
                    self.evaluate_cross_exchange_opportunities()
                    time.sleep(60)  # Scan every minute
                except Exception as e:
                    self.logger.error(f"Opportunity scanning error: {e}")
                    time.sleep(300)
                    
        threading.Thread(target=opportunity_scanner, daemon=True).start()
        
    def start_position_management(self):
        """Start position management and monitoring"""
        def position_manager():
            while True:
                try:
                    self.monitor_active_positions()
                    self.check_funding_payments()
                    self.rebalance_positions()
                    time.sleep(30)  # Check every 30 seconds
                except Exception as e:
                    self.logger.error(f"Position management error: {e}")
                    time.sleep(120)
                    
        threading.Thread(target=position_manager, daemon=True).start()
        
    def start_funding_prediction(self):
        """Start funding rate prediction system"""
        def funding_predictor():
            while True:
                try:
                    for symbol in self.arbitrage_config['target_symbols']:
                        self.predict_next_funding_rate(symbol)
                        
                    time.sleep(1800)  # Update predictions every 30 minutes
                except Exception as e:
                    self.logger.error(f"Funding prediction error: {e}")
                    time.sleep(3600)
                    
        threading.Thread(target=funding_predictor, daemon=True).start()
        
    def update_current_funding_rate(self, symbol):
        """Update current funding rate for symbol"""
        try:
            # Get current funding rate
            funding_info = self.binance_client.client.futures_funding_rate(symbol=symbol, limit=1)
            
            if funding_info:
                current_rate = float(funding_info[0]['fundingRate'])
                funding_time = int(funding_info[0]['fundingTime'])
                
                # Update funding data
                self.funding_data[symbol]['current_rate'] = current_rate
                self.funding_data[symbol]['next_funding_time'] = funding_time + (8 * 3600 * 1000)  # Next funding in 8 hours
                
                # Calculate annual rate
                annual_rate = current_rate * 3 * 365  # 3 times daily
                self.funding_data[symbol]['annual_rate'] = annual_rate
                
                # Add to history
                rate_record = {
                    'timestamp': funding_time,
                    'funding_rate': current_rate,
                    'symbol': symbol
                }
                
                # Avoid duplicates
                if (not self.funding_data[symbol]['rate_history'] or 
                    self.funding_data[symbol]['rate_history'][-1]['timestamp'] != funding_time):
                    self.funding_data[symbol]['rate_history'].append(rate_record)
                    
                # Calculate trend and volatility
                self.calculate_funding_metrics(symbol)
                
        except Exception as e:
            self.logger.error(f"Funding rate update failed for {symbol}: {e}")
            
    def calculate_funding_metrics(self, symbol):
        """Calculate funding rate metrics (trend, volatility)"""
        try:
            rate_history = self.funding_data[symbol]['rate_history']
            
            if len(rate_history) < 10:
                return
                
            # Extract recent rates
            recent_rates = [r['funding_rate'] for r in list(rate_history)[-24:]]  # Last 24 periods
            
            # Calculate volatility
            volatility = np.std(recent_rates) if len(recent_rates) > 1 else 0
            self.funding_data[symbol]['volatility'] = volatility
            
            # Calculate trend
            if len(recent_rates) >= 5:
                # Simple linear trend
                x = np.arange(len(recent_rates))
                slope = np.polyfit(x, recent_rates, 1)[0]
                
                if slope > 0.00005:  # 0.005% threshold
                    trend = 'increasing'
                elif slope < -0.00005:
                    trend = 'decreasing'
                else:
                    trend = 'stable'
                    
                self.funding_data[symbol]['trend'] = trend
                
        except Exception as e:
            self.logger.error(f"Funding metrics calculation failed for {symbol}: {e}")
            
    def predict_next_funding_rate(self, symbol):
        """Predict next funding rate using various models"""
        try:
            rate_history = self.funding_data[symbol]['rate_history']
            
            if len(rate_history) < 24:  # Need minimum history
                return
                
            predictions = []
            
            # Time series prediction
            if self.prediction_models['time_series']['enabled']:
                ts_prediction = self.time_series_prediction(symbol)
                if ts_prediction is not None:
                    predictions.append(ts_prediction)
                    
            # Market sentiment prediction
            if self.prediction_models['market_sentiment']['enabled']:
                sentiment_prediction = self.sentiment_based_prediction(symbol)
                if sentiment_prediction is not None:
                    predictions.append(sentiment_prediction)
                    
            # Combine predictions
            if predictions:
                predicted_rate = np.mean(predictions)
                self.funding_data[symbol]['predicted_rate'] = predicted_rate
                
                self.logger.debug(f"üîÆ {symbol} funding prediction: {predicted_rate:.4%}")
                
        except Exception as e:
            self.logger.error(f"Funding rate prediction failed for {symbol}: {e}")
            
    def time_series_prediction(self, symbol):
        """Simple time series prediction for funding rates"""
        try:
            rate_history = self.funding_data[symbol]['rate_history']
            lookback = min(self.prediction_models['time_series']['lookback_periods'], len(rate_history))
            
            if lookback < 10:
                return None
                
            # Extract recent rates
            recent_rates = [r['funding_rate'] for r in list(rate_history)[-lookback:]]
            
            # Simple moving average with trend adjustment
            sma = np.mean(recent_rates)
            
            # Calculate trend component
            x = np.arange(len(recent_rates))
            slope, intercept = np.polyfit(x, recent_rates, 1)
            
            # Predict next period
            next_x = len(recent_rates)
            trend_component = slope * next_x + intercept
            
            # Weighted combination
            predicted_rate = 0.6 * sma + 0.4 * trend_component
            
            return predicted_rate
            
        except Exception as e:
            self.logger.error(f"Time series prediction failed for {symbol}: {e}")
            return None
            
    def sentiment_based_prediction(self, symbol):
        """Predict funding rate based on market sentiment"""
        try:
            # Get recent price data for sentiment analysis
            klines = self.binance_client.get_klines(symbol, '1h', 24)
            
            if not klines or len(klines) < 10:
                return None
                
            # Extract price and volume data
            closes = [float(k[4]) for k in klines]
            volumes = [float(k[5]) for k in klines]
            
            # Calculate sentiment indicators
            price_momentum = (closes[-1] - closes[0]) / closes[0]  # 24h momentum
            volume_trend = np.mean(volumes[-6:]) / np.mean(volumes[:-6])  # Recent vs older volume
            
            # Calculate volatility
            returns = [(closes[i] - closes[i-1]) / closes[i-1] for i in range(1, len(closes))]
            volatility = np.std(returns)
            
            # Sentiment scoring
            sentiment_weights = self.prediction_models['market_sentiment']
            
            # Positive momentum + high volume = bullish = higher funding rates
            momentum_score = np.tanh(price_momentum * 10)  # Scale and bound
            volume_score = min(2.0, volume_trend) - 1  # Center around 0
            volatility_score = -min(2.0, volatility * 100)  # High vol = negative
            
            # Combine sentiment factors
            sentiment_score = (
                momentum_score * sentiment_weights['sentiment_weight'] +
                volume_score * sentiment_weights['volume_weight'] +
                volatility_score * sentiment_weights['volatility_weight']
            )
            
            # Convert sentiment to funding rate prediction
            current_rate = self.funding_data[symbol]['current_rate']
            sentiment_adjustment = sentiment_score * 0.0002  # Max 0.02% adjustment
            
            predicted_rate = current_rate + sentiment_adjustment
            
            return predicted_rate
            
        except Exception as e:
            self.logger.error(f"Sentiment prediction failed for {symbol}: {e}")
            return None
            
    def scan_funding_opportunities(self):
        """Scan for funding rate arbitrage opportunities"""
        try:
            for symbol in self.arbitrage_config['target_symbols']:
                opportunity = self.evaluate_funding_opportunity(symbol)
                
                if opportunity:
                    self.opportunities[symbol] = opportunity
                    
                    # Log significant opportunities
                    if opportunity['annual_yield'] > 0.1:  # 10%+ annual yield
                        self.logger.info(
                            f"üí∞ Funding opportunity: {symbol} "
                            f"{opportunity['annual_yield']:.1%} annual yield "
                            f"(confidence: {opportunity['confidence']:.1%})"
                        )
                        
                        # Auto-execute if conditions are met
                        if self.should_auto_execute_opportunity(symbol, opportunity):
                            self.execute_funding_arbitrage(symbol, opportunity)
                            
        except Exception as e:
            self.logger.error(f"Opportunity scanning failed: {e}")
            
    def evaluate_funding_opportunity(self, symbol):
        """Evaluate funding arbitrage opportunity for symbol"""
        try:
            funding_data = self.funding_data[symbol]
            current_rate = funding_data['current_rate']
            predicted_rate = funding_data.get('predicted_rate', current_rate)
            
            # Check minimum rate threshold
            if abs(current_rate) < self.arbitrage_config['min_funding_rate']:
                return None
                
            # Calculate expected returns
            annual_yield = abs(current_rate) * 3 * 365  # 3 times daily
            
            # Risk assessment
            volatility = funding_data['volatility']
            risk_score = min(1.0, volatility / 0.001)  # 0.1% vol = 1.0 risk
            
            # Confidence based on prediction accuracy and rate stability
            rate_stability = 1 - min(1.0, volatility / abs(current_rate)) if current_rate != 0 else 0
            prediction_confidence = 0.7  # Base confidence
            
            overall_confidence = (rate_stability + prediction_confidence) / 2
            
            # Calculate optimal position size
            optimal_size = self.calculate_optimal_position_size(symbol, annual_yield, risk_score)
            
            # Determine entry urgency
            time_to_funding = self.get_time_to_next_funding(symbol)
            
            if time_to_funding < 1800:  # Less than 30 minutes
                urgency = 'very_high'
            elif time_to_funding < 3600:  # Less than 1 hour
                urgency = 'high'
            elif time_to_funding < 7200:  # Less than 2 hours
                urgency = 'medium'
            else:
                urgency = 'low'
                
            return {
                'rate_differential': current_rate,
                'annual_yield': annual_yield,
                'risk_score': risk_score,
                'confidence': overall_confidence,
                'optimal_size': optimal_size,
                'entry_urgency': urgency,
                'time_to_funding': time_to_funding,
                'predicted_rate': predicted_rate,
                'last_updated': time.time()
            }
            
        except Exception as e:
            self.logger.error(f"Opportunity evaluation failed for {symbol}: {e}")
            return None
            
    def calculate_optimal_position_size(self, symbol, annual_yield, risk_score):
        """Calculate optimal position size for funding arbitrage"""
        try:
            # Get account balance
            account_balance = self.binance_client.get_account_balance()
            
            # Base position size from config
            max_position_pct = self.arbitrage_config['max_position_percentage']
            base_size = account_balance * max_position_pct
            
            # Adjust for yield (higher yield = larger position)
            yield_multiplier = min(2.0, annual_yield / 0.1)  # 10% yield = 1x, 20% = 2x
            
            # Adjust for risk (higher risk = smaller position)
            risk_multiplier = max(0.3, 1 - risk_score)
            
            # Check current exposure
            current_exposure = self.get_current_total_exposure()
            max_total_exposure = account_balance * self.risk_params['max_total_exposure']
            available_capacity = max_total_exposure - current_exposure
            
            # Calculate optimal size
            optimal_size = min(
                base_size * yield_multiplier * risk_multiplier,
                available_capacity,
                base_size
            )
            
            return max(0, optimal_size)
            
        except Exception as e:
            self.logger.error(f"Optimal position size calculation failed for {symbol}: {e}")
            return 0
            
    def get_time_to_next_funding(self, symbol):
        """Get time until next funding payment"""
        try:
            next_funding_time = self.funding_data[symbol]['next_funding_time']
            current_time = time.time() * 1000  # Convert to milliseconds
            
            time_to_funding = (next_funding_time - current_time) / 1000  # Convert back to seconds
            
            return max(0, time_to_funding)
            
        except Exception as e:
            self.logger.error(f"Time to funding calculation failed for {symbol}: {e}")
            return 0
            
    def should_auto_execute_opportunity(self, symbol, opportunity):
        """Determine if opportunity should be auto-executed"""
        try:
            # Auto-execution criteria
            min_annual_yield = 0.08  # 8% minimum
            min_confidence = 0.6     # 60% minimum confidence
            max_risk_score = 0.7     # Maximum acceptable risk
            
            # Check if we already have a position in this symbol
            if self.active_positions[symbol]['status'] == 'active':
                return False
                
            # Check criteria
            meets_yield_requirement = opportunity['annual_yield'] >= min_annual_yield
            meets_confidence_requirement = opportunity['confidence'] >= min_confidence
            meets_risk_requirement = opportunity['risk_score'] <= max_risk_score
            has_sufficient_size = opportunity['optimal_size'] > 1000  # Minimum $1000
            
            return (meets_yield_requirement and meets_confidence_requirement and 
                   meets_risk_requirement and has_sufficient_size)
                   
        except Exception as e:
            self.logger.error(f"Auto-execution check failed for {symbol}: {e}")
            return False
            
    def execute_funding_arbitrage(self, symbol, opportunity):
        """Execute funding rate arbitrage position"""
        try:
            position_size = opportunity['optimal_size']
            funding_rate = self.funding_data[symbol]['current_rate']
            
            self.logger.info(f"üöÄ Executing funding arbitrage: {symbol}")
            self.logger.info(f"   üí∞ Position size: ${position_size:,.0f}")
            self.logger.info(f"   üìä Funding rate: {funding_rate:.4%}")
            self.logger.info(f"   üéØ Expected annual yield: {opportunity['annual_yield']:.1%}")
            
            # Determine position direction based on funding rate
            if funding_rate > 0:
                # Positive funding rate: longs pay shorts
                # Open SHORT position to receive funding
                position_side = 'SHORT'
                hedge_side = 'LONG'
            else:
                # Negative funding rate: shorts pay longs  
                # Open LONG position to receive funding
                position_side = 'LONG'
                hedge_side = 'SHORT'
                
            # Calculate position quantity
            current_price = self.get_current_price(symbol)
            if not current_price:
                self.logger.error(f"Failed to get current price for {symbol}")
                return False
                
            position_quantity = position_size / current_price
            
            # Execute the futures position
            futures_order = self.binance_client.place_market_order(
                symbol=symbol,
                side='SELL' if position_side == 'SHORT' else 'BUY',
                quantity=position_quantity
            )
            
            if not futures_order:
                self.logger.error(f"Failed to execute futures order for {symbol}")
                return False
                
            # Execute hedge position (simplified - in production would use spot or other exchange)
            hedge_success = self.execute_hedge_position(symbol, hedge_side, position_quantity)
            
            if hedge_success:
                # Record the position
                self.active_positions[symbol] = {
                    'symbol': symbol,
                    'entry_time': time.time(),
                    'position_size_usd': position_size,
                    'position_side': position_side,
                    'position_quantity': position_quantity,
                    'entry_price': current_price,
                    'funding_rate_entry': funding_rate,
                    'expected_daily_return': opportunity['annual_yield'] / 365,
                    'hedge_ratio': 0.95,  # Simplified
                    'total_funding_received': 0,
                    'position_pnl': 0,
                    'status': 'active',
                    'futures_order_id': futures_order.get('orderId'),
                    'next_funding_time': self.funding_data[symbol]['next_funding_time']
                }
                
                # Update performance tracking
                self.performance_tracking['total_positions_opened'] += 1
                
                self.logger.info(f"‚úÖ Funding arbitrage position opened: {symbol}")
                return True
            else:
                # Failed to hedge - close futures position
                self.close_futures_position(symbol, position_quantity, position_side)
                return False
                
        except Exception as e:
            self.logger.error(f"Funding arbitrage execution failed for {symbol}: {e}")
            return False
            
    def execute_hedge_position(self, symbol, hedge_side, quantity):
        """Execute hedge position (simplified implementation)"""
        try:
            # In production, this would open a spot position or position on another exchange
            # For demo purposes, we'll simulate a successful hedge
            
            self.logger.info(f"üìà Hedging with {hedge_side} position: {quantity:.6f} {symbol}")
            
            # Simulate hedge execution
            hedge_success = True  # In production, execute actual hedge order
            
            return hedge_success
            
        except Exception as e:
            self.logger.error(f"Hedge execution failed for {symbol}: {e}")
            return False
            
    def get_current_price(self, symbol):
        """Get current price for symbol"""
        try:
            ticker = self.binance_client.client.futures_symbol_ticker(symbol=symbol)
            return float(ticker['price']) if ticker else None
        except Exception as e:
            self.logger.error(f"Price fetch failed for {symbol}: {e}")
            return None
            
    def monitor_active_positions(self):
        """Monitor active funding arbitrage positions"""
        try:
            for symbol, position in self.active_positions.items():
                if position['status'] != 'active':
                    continue
                    
                # Update position PnL
                self.update_position_pnl(symbol, position)
                
                # Check for exit conditions
                if self.should_exit_position(symbol, position):
                    self.close_arbitrage_position(symbol, position)
                    
                # Check for rebalancing needs
                elif self.needs_rebalancing(symbol, position):
                    self.rebalance_position(symbol, position)
                    
        except Exception as e:
            self.logger.error(f"Position monitoring failed: {e}")
            
    def update_position_pnl(self, symbol, position):
        """Update position PnL and metrics"""
        try:
            current_price = self.get_current_price(symbol)
            if not current_price:
                return
                
            entry_price = position['entry_price']
            quantity = position['position_quantity']
            position_side = position['position_side']
            
            # Calculate unrealized PnL
            if position_side == 'LONG':
                price_pnl = (current_price - entry_price) * quantity
            else:  # SHORT
                price_pnl = (entry_price - current_price) * quantity
                
            # Add hedge PnL (simplified - opposite of futures PnL)
            hedge_pnl = -price_pnl * position['hedge_ratio']
            
            # Net PnL from price movements (should be near zero if well hedged)
            net_price_pnl = price_pnl + hedge_pnl
            
            # Total PnL including funding received
            total_pnl = net_price_pnl + position['total_funding_received']
            
            # Update position
            position['position_pnl'] = total_pnl
            position['price_pnl'] = net_price_pnl
            position['last_update'] = time.time()
            
        except Exception as e:
            self.logger.error(f"Position PnL update failed for {symbol}: {e}")
            
    def check_funding_payments(self):
	"""Check and record funding payments for active positions"""
        try:
            current_time = time.time() * 1000  # Convert to milliseconds
            
            for symbol, position in self.active_positions.items():
                if position['status'] != 'active':
                    continue
                    
                next_funding_time = position.get('next_funding_time', 0)
                
                # Check if funding payment occurred
                if current_time >= next_funding_time:
                    funding_payment = self.calculate_funding_payment(symbol, position)
                    
                    if funding_payment != 0:
                        # Record funding payment
                        position['total_funding_received'] += funding_payment
                        position['next_funding_time'] = next_funding_time + (8 * 3600 * 1000)  # Next funding in 8 hours
                        
                        # Update performance tracking
                        self.performance_tracking['total_funding_earned'] += funding_payment
                        
                        self.logger.info(
                            f"üí∞ Funding payment received: {symbol} "
                            f"${funding_payment:.2f} "
                            f"(Total: ${position['total_funding_received']:.2f})"
                        )
                        
        except Exception as e:
            self.logger.error(f"Funding payment check failed: {e}")
            
    def calculate_funding_payment(self, symbol, position):
        """Calculate funding payment for position"""
        try:
            current_rate = self.funding_data[symbol]['current_rate']
            position_size_usd = position['position_size_usd']
            position_side = position['position_side']
            
            # Calculate funding payment
            # If we're SHORT and rate is positive, we receive funding
            # If we're LONG and rate is negative, we receive funding
            if (position_side == 'SHORT' and current_rate > 0) or (position_side == 'LONG' and current_rate < 0):
                funding_payment = abs(current_rate) * position_size_usd
            else:
                funding_payment = -abs(current_rate) * position_size_usd
                
            return funding_payment
            
        except Exception as e:
            self.logger.error(f"Funding payment calculation failed for {symbol}: {e}")
            return 0
            
    def should_exit_position(self, symbol, position):
        """Determine if position should be closed"""
        try:
            # Exit conditions
            current_time = time.time()
            position_age = (current_time - position['entry_time']) / 86400  # Days
            
            # Maximum hold period
            if position_age > self.risk_params['position_duration_limit']:
                self.logger.info(f"üïê Closing {symbol} position: Maximum hold period reached")
                return True
                
            # Emergency exit on large losses
            position_pnl = position.get('position_pnl', 0)
            position_size = position['position_size_usd']
            
            if position_pnl / position_size < self.risk_params['emergency_exit_threshold']:
                self.logger.warning(f"üö® Emergency exit {symbol}: Large loss detected")
                return True
                
            # Exit if funding rate becomes unfavorable
            current_rate = self.funding_data[symbol]['current_rate']
            entry_rate = position['funding_rate_entry']
            position_side = position['position_side']
            
            # Check if funding rate direction changed significantly
            if position_side == 'SHORT':
                # We want positive rates (we receive funding)
                if current_rate < 0 and abs(current_rate) > abs(entry_rate) * 0.5:
                    self.logger.info(f"üìâ Closing {symbol} SHORT: Funding rate turned negative")
                    return True
            else:  # LONG position
                # We want negative rates (we receive funding)
                if current_rate > 0 and current_rate > abs(entry_rate) * 0.5:
                    self.logger.info(f"üìà Closing {symbol} LONG: Funding rate turned positive")
                    return True
                    
            # Exit if hedge ratio deteriorated
            if position.get('hedge_ratio', 1.0) < self.risk_params['minimum_hedge_ratio']:
                self.logger.warning(f"‚ö†Ô∏è Closing {symbol}: Hedge ratio below minimum")
                return True
                
            return False
            
        except Exception as e:
            self.logger.error(f"Exit condition check failed for {symbol}: {e}")
            return False
            
    def close_arbitrage_position(self, symbol, position):
        """Close funding arbitrage position"""
        try:
            self.logger.info(f"üîö Closing funding arbitrage position: {symbol}")
            
            position_quantity = position['position_quantity']
            position_side = position['position_side']
            
            # Close futures position
            close_side = 'BUY' if position_side == 'SHORT' else 'SELL'
            
            close_order = self.binance_client.place_market_order(
                symbol=symbol,
                side=close_side,
                quantity=position_quantity,
                reduce_only=True
            )
            
            if close_order:
                # Close hedge position (simplified)
                self.close_hedge_position(symbol, position)
                
                # Calculate final PnL
                final_pnl = position.get('position_pnl', 0)
                total_funding = position.get('total_funding_received', 0)
                
                # Update performance tracking
                self.performance_tracking['net_profit'] += final_pnl
                
                if final_pnl > 0:
                    win_rate = self.performance_tracking.get('win_rate', 0)
                    total_positions = self.performance_tracking['total_positions_opened']
                    self.performance_tracking['win_rate'] = (win_rate * (total_positions - 1) + 1) / total_positions
                    
                # Mark position as closed
                position['status'] = 'closed'
                position['close_time'] = time.time()
                position['final_pnl'] = final_pnl
                
                self.logger.info(
                    f"‚úÖ Position closed: {symbol} "
                    f"PnL: ${final_pnl:.2f} "
                    f"Funding: ${total_funding:.2f}"
                )
                
                return True
            else:
                self.logger.error(f"Failed to close futures position for {symbol}")
                return False
                
        except Exception as e:
            self.logger.error(f"Position closing failed for {symbol}: {e}")
            return False
            
    def close_hedge_position(self, symbol, position):
        """Close hedge position (simplified)"""
        try:
            # In production, this would close the actual hedge position
            self.logger.info(f"üìâ Closing hedge position for {symbol}")
            return True
            
        except Exception as e:
            self.logger.error(f"Hedge position closing failed for {symbol}: {e}")
            return False
            
    def close_futures_position(self, symbol, quantity, position_side):
        """Close futures position (utility method)"""
        try:
            close_side = 'BUY' if position_side == 'SHORT' else 'SELL'
            
            close_order = self.binance_client.place_market_order(
                symbol=symbol,
                side=close_side,
                quantity=quantity,
                reduce_only=True
            )
            
            return close_order is not None
            
        except Exception as e:
            self.logger.error(f"Futures position closing failed for {symbol}: {e}")
            return False
            
    def needs_rebalancing(self, symbol, position):
        """Check if position needs rebalancing"""
        try:
            # Check hedge ratio drift
            current_hedge_ratio = position.get('hedge_ratio', 1.0)
            target_hedge_ratio = self.arbitrage_config['hedge_threshold']
            
            ratio_drift = abs(current_hedge_ratio - target_hedge_ratio)
            
            return ratio_drift > self.arbitrage_config['rebalancing_threshold']
            
        except Exception as e:
            self.logger.error(f"Rebalancing check failed for {symbol}: {e}")
            return False
            
    def rebalance_position(self, symbol, position):
        """Rebalance position to maintain hedge ratio"""
        try:
            self.logger.info(f"‚öñÔ∏è Rebalancing position: {symbol}")
            
            # In production, this would adjust hedge positions
            # For demo, just log the rebalancing
            target_ratio = self.arbitrage_config['hedge_threshold']
            position['hedge_ratio'] = target_ratio
            position['last_rebalance'] = time.time()
            
            self.logger.info(f"‚úÖ Position rebalanced: {symbol}")
            
        except Exception as e:
            self.logger.error(f"Position rebalancing failed for {symbol}: {e}")
            
    def rebalance_positions(self):
        """Rebalance all positions if needed"""
        try:
            for symbol, position in self.active_positions.items():
                if position['status'] == 'active' and self.needs_rebalancing(symbol, position):
                    self.rebalance_position(symbol, position)
                    
        except Exception as e:
            self.logger.error(f"Positions rebalancing failed: {e}")
            
    def evaluate_cross_exchange_opportunities(self):
        """Evaluate cross-exchange funding arbitrage opportunities"""
        try:
            # This would compare funding rates across different exchanges
            # For demo purposes, we'll simulate cross-exchange analysis
            
            if not self.prediction_models['cross_exchange']['enabled']:
                return
                
            for symbol in self.arbitrage_config['target_symbols']:
                # Simulate funding rate differences between exchanges
                binance_rate = self.funding_data[symbol]['current_rate']
                
                # Simulate other exchange rates (would be real data in production)
                other_exchange_rate = binance_rate + np.random.uniform(-0.0005, 0.0005)
                
                rate_spread = abs(binance_rate - other_exchange_rate)
                
                if rate_spread > self.prediction_models['cross_exchange']['spread_threshold']:
                    self.logger.info(
                        f"üîÑ Cross-exchange opportunity: {symbol} "
                        f"Spread: {rate_spread:.4%}"
                    )
                    
        except Exception as e:
            self.logger.error(f"Cross-exchange evaluation failed: {e}")
            
    def get_current_total_exposure(self):
        """Get current total exposure across all positions"""
        try:
            total_exposure = 0
            
            for position in self.active_positions.values():
                if position['status'] == 'active':
                    total_exposure += position.get('position_size_usd', 0)
                    
            return total_exposure
            
        except Exception as e:
            self.logger.error(f"Total exposure calculation failed: {e}")
            return 0
            
    def get_funding_arbitrage_summary(self):
        """Get comprehensive funding arbitrage summary"""
        try:
            # Active positions summary
            active_positions = [p for p in self.active_positions.values() if p['status'] == 'active']
            
            total_exposure = sum(p['position_size_usd'] for p in active_positions)
            total_funding_earned = sum(p['total_funding_received'] for p in active_positions)
            total_unrealized_pnl = sum(p.get('position_pnl', 0) for p in active_positions)
            
            # Current opportunities
            high_yield_opportunities = [
                {'symbol': symbol, **opp} 
                for symbol, opp in self.opportunities.items()
                if opp.get('annual_yield', 0) > 0.08  # 8%+ yield
            ]
            
            # Performance metrics
            performance = self.performance_tracking.copy()
            
            # Calculate current daily return rate
            if total_exposure > 0:
                daily_funding_rate = (total_funding_earned / total_exposure) if total_exposure > 0 else 0
                annualized_return = daily_funding_rate * 365
            else:
                annualized_return = 0
                
            return {
                'active_positions': {
                    'count': len(active_positions),
                    'total_exposure_usd': total_exposure,
                    'total_funding_earned': total_funding_earned,
                    'total_unrealized_pnl': total_unrealized_pnl,
                    'positions': [
                        {
                            'symbol': p['symbol'],
                            'side': p['position_side'],
                            'size_usd': p['position_size_usd'],
                            'funding_rate': p['funding_rate_entry'],
                            'funding_earned': p['total_funding_received'],
                            'days_held': (time.time() - p['entry_time']) / 86400
                        }
                        for p in active_positions
                    ]
                },
                'opportunities': {
                    'high_yield_count': len(high_yield_opportunities),
                    'opportunities': sorted(high_yield_opportunities, 
                                          key=lambda x: x['annual_yield'], reverse=True)[:5]
                },
                'performance': {
                    **performance,
                    'current_annualized_return': annualized_return,
                    'total_positions': len(self.active_positions),
                    'success_rate': performance.get('win_rate', 0)
                },
                'funding_rates': {
                    'current_rates': {
                        symbol: {
                            'rate': data['current_rate'],
                            'annual_rate': data['annual_rate'],
                            'trend': data['trend']
                        }
                        for symbol, data in self.funding_data.items()
                        if abs(data['current_rate']) > 0.0001
                    }
                }
            }
            
        except Exception as e:
            self.logger.error(f"Funding arbitrage summary failed: {e}")
            return {}
            
    def get_top_funding_opportunities(self, limit=5):
        """Get top funding arbitrage opportunities"""
        try:
            # Filter and sort opportunities
            valid_opportunities = []
            
            for symbol, opportunity in self.opportunities.items():
                if (opportunity.get('annual_yield', 0) > 0.05 and  # 5%+ yield
                    opportunity.get('confidence', 0) > 0.5 and     # 50%+ confidence
                    opportunity.get('optimal_size', 0) > 500):      # $500+ position
                    
                    valid_opportunities.append({
                        'symbol': symbol,
                        **opportunity
                    })
                    
            # Sort by yield
            valid_opportunities.sort(key=lambda x: x['annual_yield'], reverse=True)
            
            return valid_opportunities[:limit]
            
        except Exception as e:
            self.logger.error(f"Top opportunities retrieval failed: {e}")
            return []
            
    def calculate_portfolio_metrics(self):
        """Calculate portfolio-level metrics"""
        try:
            active_positions = [p for p in self.active_positions.values() if p['status'] == 'active']
            
            if not active_positions:
                return {}
                
            # Portfolio exposure
            total_exposure = sum(p['position_size_usd'] for p in active_positions)
            
            # Portfolio funding yield
            total_funding = sum(p['total_funding_received'] for p in active_positions)
            portfolio_yield = (total_funding / total_exposure) if total_exposure > 0 else 0
            
            # Portfolio risk metrics
            position_correlations = self.calculate_position_correlations(active_positions)
            avg_correlation = np.mean(list(position_correlations.values())) if position_correlations else 0
            
            # Portfolio Sharpe ratio (simplified)
            funding_rates = [self.funding_data[p['symbol']]['current_rate'] for p in active_positions]
            portfolio_volatility = np.std(funding_rates) if len(funding_rates) > 1 else 0
            sharpe_ratio = (portfolio_yield / portfolio_volatility) if portfolio_volatility > 0 else 0
            
            return {
                'total_exposure': total_exposure,
                'portfolio_yield': portfolio_yield,
                'average_correlation': avg_correlation,
                'portfolio_volatility': portfolio_volatility,
                'sharpe_ratio': sharpe_ratio,
                'diversification_score': 1 - avg_correlation,
                'position_count': len(active_positions)
            }
            
        except Exception as e:
            self.logger.error(f"Portfolio metrics calculation failed: {e}")
            return {}
            
    def calculate_position_correlations(self, positions):
        """Calculate correlations between positions"""
        try:
            correlations = {}
            
            if len(positions) < 2:
                return correlations
                
            # Get funding rate histories for correlation calculation
            for i, pos1 in enumerate(positions):
                for j, pos2 in enumerate(positions[i+1:], i+1):
                    symbol1 = pos1['symbol']
                    symbol2 = pos2['symbol']
                    
                    # Get recent funding rates
                    rates1 = [r['funding_rate'] for r in list(self.funding_data[symbol1]['rate_history'])[-50:]]
                    rates2 = [r['funding_rate'] for r in list(self.funding_data[symbol2]['rate_history'])[-50:]]
                    
                    if len(rates1) >= 10 and len(rates2) >= 10:
                        min_length = min(len(rates1), len(rates2))
                        correlation = np.corrcoef(rates1[-min_length:], rates2[-min_length:])[0, 1]
                        correlations[f"{symbol1}-{symbol2}"] = correlation
                        
            return correlations
            
        except Exception as e:
            self.logger.error(f"Position correlation calculation failed: {e}")
            return {}
            
    def optimize_portfolio_allocation(self):
        """Optimize portfolio allocation across funding opportunities"""
        try:
            # Get current opportunities
            opportunities = self.get_top_funding_opportunities(10)
            
            if not opportunities:
                return
                
            # Calculate optimal allocation using simplified portfolio theory
            yields = [opp['annual_yield'] for opp in opportunities]
            risks = [opp['risk_score'] for opp in opportunities]
            
            # Risk-adjusted scores
            risk_adjusted_scores = [y / (1 + r) for y, r in zip(yields, risks)]
            
            # Normalize to get allocation weights
            total_score = sum(risk_adjusted_scores)
            weights = [score / total_score for score in risk_adjusted_scores] if total_score > 0 else []
            
            # Update optimal sizes based on portfolio optimization
            account_balance = self.binance_client.get_account_balance()
            max_total_allocation = account_balance * self.risk_params['max_total_exposure']
            
            for i, opp in enumerate(opportunities):
                if i < len(weights):
                    optimal_allocation = max_total_allocation * weights[i]
                    self.opportunities[opp['symbol']]['optimal_size'] = optimal_allocation
                    
            self.logger.info("üìä Portfolio allocation optimized")
            
        except Exception as e:
            self.logger.error(f"Portfolio optimization failed: {e}")
            
    def export_funding_data(self, filename=None):
        """Export funding arbitrage data"""
        try:
            if filename is None:
                filename = f"funding_arbitrage_data_{int(time.time())}.json"
                
            export_data = {
                'export_timestamp': time.time(),
                'arbitrage_config': self.arbitrage_config,
                'funding_data': {
                    symbol: {
                        'current_rate': data['current_rate'],
                        'annual_rate': data['annual_rate'],
                        'volatility': data['volatility'],
                        'trend': data['trend'],
                        'rate_history': list(data['rate_history'])[-100:]  # Last 100 records
                    }
                    for symbol, data in self.funding_data.items()
                },
                'active_positions': dict(self.active_positions),
                'opportunities': dict(self.opportunities),
                'performance_tracking': self.performance_tracking,
                'portfolio_metrics': self.calculate_portfolio_metrics()
            }
            
            # Store via memory manager
            self.memory_manager.export_funding_arbitrage_data(export_data, filename)
            
            self.logger.info(f"üìÅ Funding arbitrage data exported: {filename}")
            return filename
            
        except Exception as e:
            self.logger.error(f"Funding arbitrage data export failed: {e}")
            return None
            
    def cleanup_old_data(self, days_to_keep=30):
        """Clean up old funding arbitrage data"""
        try:
            cutoff_time = time.time() - (days_to_keep * 86400)
            
            # Clean up closed positions
            closed_positions = {
                symbol: position for symbol, position in self.active_positions.items()
                if (position['status'] == 'closed' and 
                    position.get('close_time', time.time()) < cutoff_time)
            }
            
            for symbol in closed_positions:
                del self.active_positions[symbol]
                
            # Clean up old opportunities
            for symbol in list(self.opportunities.keys()):
                if self.opportunities[symbol].get('last_updated', time.time()) < cutoff_time:
                    del self.opportunities[symbol]
                    
            removed_count = len(closed_positions)
            
            if removed_count > 0:
                self.logger.info(f"üßπ Cleaned up {removed_count} old funding arbitrage records")
                
        except Exception as e:
            self.logger.error(f"Funding arbitrage cleanup failed: {e}")

class LiquidationHeatMapsEngine:
    """Advanced liquidation heat maps engine for detecting leverage clusters and cascade risk"""
    
    def __init__(self, config, binance_client, memory_manager, performance_analytics):
        self.config = config
        self.binance_client = binance_client
        self.memory_manager = memory_manager
        self.performance_analytics = performance_analytics
        self.logger = logging.getLogger('MonsterBot.LiquidationHeatMaps')
        
        # Liquidation analysis configuration
        self.liquidation_config = {
            'target_symbols': [
                'BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'ADAUSDT', 'SOLUSDT',
                'AVAXUSDT', 'MATICUSDT', 'DOTUSDT', 'LINKUSDT', 'UNIUSDT'
            ],
            'leverage_ranges': [5, 10, 20, 25, 50, 75, 100, 125],
            'price_range_percentage': 0.10,  # ¬±10% from current price
            'heat_map_resolution': 100,      # 100 price levels
            'liquidation_threshold': 1000000, # $1M+ for significant level
            'cascade_detection_threshold': 5000000,  # $5M+ for cascade risk
            'update_frequency': 30,          # 30 seconds
            'historical_analysis_days': 7    # 1 week of historical data
        }
        
        # Leverage analysis parameters
        self.leverage_analysis = {
            'position_estimation_methods': {
                'open_interest_analysis': True,
                'funding_rate_correlation': True,
                'volume_profile_analysis': True,
                'order_book_imbalance': True,
                'liquidation_event_tracking': True
            },
            'crowd_behavior_indicators': {
                'leverage_clustering': 0.8,     # 80% correlation threshold
                'momentum_chasing': 0.7,        # 70% correlation with price
                'fomo_detection': 0.9,          # 90% confidence threshold
                'fear_selling': 0.8,            # 80% confidence threshold
                'whale_vs_retail_ratio': 0.3    # 30% whale dominance threshold
            }
        }
        
        # Heat map data structures
        self.heat_maps = defaultdict(lambda: {
            'long_liquidations': defaultdict(float),
            'short_liquidations': defaultdict(float),
            'total_liquidations': defaultdict(float),
            'price_levels': [],
            'max_liquidation_level': 0,
            'cascade_risk_zones': [],
            'last_update': 0
        })
        
        # Liquidation cluster tracking
        self.liquidation_clusters = defaultdict(lambda: {
            'clusters': [],
            'total_at_risk': 0,
            'cascade_probability': 0,
            'next_major_level': 0,
            'time_to_cascade': 0,
            'risk_score': 0
        })
        
        # Historical liquidation events
        self.liquidation_events = defaultdict(lambda: deque(maxlen=1000))
        
        # Market pressure metrics
        self.market_pressure = defaultdict(lambda: {
            'long_pressure': 0,
            'short_pressure': 0,
            'net_pressure': 0,
            'leverage_weighted_pressure': 0,
            'retail_vs_whale_pressure': 0,
            'momentum_pressure': 0,
            'fear_greed_pressure': 0
        })
        
        # Cascade detection system
        self.cascade_detection = {
            'active_cascade_risk': defaultdict(bool),
            'cascade_triggers': defaultdict(list),
            'cascade_probability': defaultdict(float),
            'estimated_cascade_size': defaultdict(float),
            'cascade_price_targets': defaultdict(list)
        }
        
        # Real-time position estimation
        self.position_estimates = defaultdict(lambda: {
            'estimated_long_oi': 0,
            'estimated_short_oi': 0,
            'average_long_leverage': 0,
            'average_short_leverage': 0,
            'retail_long_percentage': 0,
            'retail_short_percentage': 0,
            'whale_long_percentage': 0,
            'whale_short_percentage': 0
        })
        
        # Initialize liquidation heat maps system
        self.initialize_liquidation_system()
        
    def initialize_liquidation_system(self):
        """Initialize liquidation heat maps system"""
        try:
            # Load historical liquidation data
            self.load_historical_liquidations()
            
            # Start real-time heat map generation
            self.start_heat_map_generation()
            
            # Start liquidation cluster detection
            self.start_cluster_detection()
            
            # Start cascade risk monitoring
            self.start_cascade_monitoring()
            
            # Start position estimation
            self.start_position_estimation()
            
            self.logger.info("üî• Liquidation Heat Maps Engine initialized")
            self.logger.info(f"   üìä Monitoring {len(self.liquidation_config['target_symbols'])} symbols")
            self.logger.info(f"   ‚ö° Leverage ranges: {self.liquidation_config['leverage_ranges']}")
            self.logger.info(f"   üéØ Cascade threshold: ${self.liquidation_config['cascade_detection_threshold']:,}")
            
        except Exception as e:
            self.logger.error(f"Liquidation heat maps initialization failed: {e}")
            
    def load_historical_liquidations(self):
        """Load historical liquidation data"""
        try:
            for symbol in self.liquidation_config['target_symbols']:
                # Load from memory manager
                historical_liquidations = self.memory_manager.get_liquidation_history(symbol)
                
                if historical_liquidations:
                    self.liquidation_events[symbol].extend(historical_liquidations)
                    self.logger.info(f"üìö Loaded {len(historical_liquidations)} liquidation events for {symbol}")
                else:
                    # Initialize with estimated data
                    self.estimate_historical_liquidations(symbol)
                    
        except Exception as e:
            self.logger.error(f"Historical liquidation loading failed: {e}")
            
    def estimate_historical_liquidations(self, symbol):
        """Estimate historical liquidations from price movements"""
        try:
            # Get recent price data
            klines = self.binance_client.get_klines(symbol, '5m', 2000)  # ~1 week of 5min data
            
            if not klines:
                return
                
            # Analyze price movements for potential liquidations
            for i in range(1, len(klines)):
                prev_candle = klines[i-1]
                curr_candle = klines[i]
                
                prev_close = float(prev_candle[4])
                curr_low = float(curr_candle[3])
                curr_high = float(curr_candle[2])
                volume = float(curr_candle[5])
                
                # Detect potential liquidation events
                # Sharp down moves with high volume = long liquidations
                downward_move = (prev_close - curr_low) / prev_close
                upward_move = (curr_high - prev_close) / prev_close
                
                if downward_move > 0.02 and volume > 0:  # 2%+ drop
                    # Estimate long liquidations
                    estimated_liquidation_volume = volume * downward_move * 1000000  # Scale estimate
                    
                    liquidation_event = {
                        'timestamp': int(curr_candle[0]),
                        'symbol': symbol,
                        'liquidation_price': curr_low,
                        'side': 'LONG',
                        'estimated_volume_usd': estimated_liquidation_volume,
                        'price_drop_percentage': downward_move,
                        'trigger_volume': volume
                    }
                    
                    self.liquidation_events[symbol].append(liquidation_event)
                    
                elif upward_move > 0.02 and volume > 0:  # 2%+ pump
                    # Estimate short liquidations
                    estimated_liquidation_volume = volume * upward_move * 1000000
                    
                    liquidation_event = {
                        'timestamp': int(curr_candle[0]),
                        'symbol': symbol,
                        'liquidation_price': curr_high,
                        'side': 'SHORT',
                        'estimated_volume_usd': estimated_liquidation_volume,
                        'price_pump_percentage': upward_move,
                        'trigger_volume': volume
                    }
                    
                    self.liquidation_events[symbol].append(liquidation_event)
                    
            self.logger.info(f"üìà Estimated {len(self.liquidation_events[symbol])} liquidation events for {symbol}")
            
        except Exception as e:
            self.logger.error(f"Historical liquidation estimation failed for {symbol}: {e}")
            
    def start_heat_map_generation(self):
        """Start real-time heat map generation"""
        def heat_map_generator():
            while True:
                try:
                    for symbol in self.liquidation_config['target_symbols']:
                        self.generate_liquidation_heat_map(symbol)
                        
                    time.sleep(self.liquidation_config['update_frequency'])
                except Exception as e:
                    self.logger.error(f"Heat map generation error: {e}")
                    time.sleep(60)
                    
        threading.Thread(target=heat_map_generator, daemon=True).start()
        
    def start_cluster_detection(self):
        """Start liquidation cluster detection"""
        def cluster_detector():
            while True:
                try:
                    for symbol in self.liquidation_config['target_symbols']:
                        self.detect_liquidation_clusters(symbol)
                        
                    time.sleep(60)  # Update every minute
                except Exception as e:
                    self.logger.error(f"Cluster detection error: {e}")
                    time.sleep(120)
                    
        threading.Thread(target=cluster_detector, daemon=True).start()
        
    def start_cascade_monitoring(self):
        """Start cascade risk monitoring"""
        def cascade_monitor():
            while True:
                try:
                    for symbol in self.liquidation_config['target_symbols']:
                        self.monitor_cascade_risk(symbol)
                        
                    time.sleep(30)  # Update every 30 seconds
                except Exception as e:
                    self.logger.error(f"Cascade monitoring error: {e}")
                    time.sleep(90)
                    
        threading.Thread(target=cascade_monitor, daemon=True).start()
        
    def start_position_estimation(self):
        """Start position estimation system"""
        def position_estimator():
            while True:
                try:
                    for symbol in self.liquidation_config['target_symbols']:
                        self.estimate_market_positions(symbol)
                        
                    time.sleep(120)  # Update every 2 minutes
                except Exception as e:
                    self.logger.error(f"Position estimation error: {e}")
                    time.sleep(300)
                    
        threading.Thread(target=position_estimator, daemon=True).start()
        
    def generate_liquidation_heat_map(self, symbol):
        """Generate liquidation heat map for symbol"""
        try:
            # Get current price
            current_price = self.get_current_price(symbol)
            if not current_price:
                return
                
            # Calculate price range
            price_range = self.liquidation_config['price_range_percentage']
            price_min = current_price * (1 - price_range)
            price_max = current_price * (1 + price_range)
            
            # Generate price levels
            resolution = self.liquidation_config['heat_map_resolution']
            price_step = (price_max - price_min) / resolution
            price_levels = [price_min + i * price_step for i in range(resolution)]
            
            # Calculate liquidation volumes at each price level
            long_liquidations = {}
            short_liquidations = {}
            
            for price_level in price_levels:
                long_liq_volume = self.estimate_long_liquidations_at_price(symbol, price_level, current_price)
                short_liq_volume = self.estimate_short_liquidations_at_price(symbol, price_level, current_price)
                
                long_liquidations[price_level] = long_liq_volume
                short_liquidations[price_level] = short_liq_volume
                
            # Update heat map
            heat_map = self.heat_maps[symbol]
            heat_map['long_liquidations'] = long_liquidations
            heat_map['short_liquidations'] = short_liquidations
            heat_map['total_liquidations'] = {
                price: long_liquidations[price] + short_liquidations[price]
                for price in price_levels
            }
            heat_map['price_levels'] = price_levels
            heat_map['current_price'] = current_price
            heat_map['last_update'] = time.time()
            
            # Find maximum liquidation level
            max_liquidation = max(heat_map['total_liquidations'].values())
            heat_map['max_liquidation_level'] = max_liquidation
            
            # Identify significant levels
            self.identify_cascade_risk_zones(symbol, heat_map)
            
        except Exception as e:
            self.logger.error(f"Heat map generation failed for {symbol}: {e}")
            
    def estimate_long_liquidations_at_price(self, symbol, target_price, current_price):
        """Estimate long liquidation volume at target price"""
        try:
            if target_price >= current_price:
                return 0  # Longs liquidate on downward moves
                
            # Calculate price drop percentage
            price_drop = (current_price - target_price) / current_price
            
            # Estimate liquidations based on leverage and open interest
            position_data = self.position_estimates[symbol]
            estimated_long_oi = position_data['estimated_long_oi']
            avg_long_leverage = position_data['average_long_leverage']
            
            if avg_long_leverage == 0:
                avg_long_leverage = 20  # Default assumption
                
            # Calculate liquidation threshold
            liquidation_threshold = 1 / avg_long_leverage  # e.g., 5% drop liquidates 20x longs
            
            if price_drop >= liquidation_threshold:
                # Estimate what percentage of longs get liquidated
                liquidation_percentage = min(1.0, price_drop / liquidation_threshold)
                
                # Adjust for retail vs whale positions (retail more likely to get liquidated)
                retail_percentage = position_data['retail_long_percentage']
                liquidation_multiplier = 1 + (retail_percentage * 0.5)  # 50% higher liquidation for retail
                
                estimated_liquidation_volume = (
                    estimated_long_oi * liquidation_percentage * liquidation_multiplier
                )
                
                return estimated_liquidation_volume
            else:
                return 0
                
        except Exception as e:
            self.logger.error(f"Long liquidation estimation failed for {symbol}: {e}")
            return 0
            
    def estimate_short_liquidations_at_price(self, symbol, target_price, current_price):
        """Estimate short liquidation volume at target price"""
        try:
            if target_price <= current_price:
                return 0  # Shorts liquidate on upward moves
                
            # Calculate price pump percentage
            price_pump = (target_price - current_price) / current_price
            
            # Estimate liquidations based on leverage and open interest
            position_data = self.position_estimates[symbol]
            estimated_short_oi = position_data['estimated_short_oi']
            avg_short_leverage = position_data['average_short_leverage']
            
            if avg_short_leverage == 0:
                avg_short_leverage = 15  # Default assumption (shorts typically use less leverage)
                
            # Calculate liquidation threshold
            liquidation_threshold = 1 / avg_short_leverage
            
            if price_pump >= liquidation_threshold:
                # Estimate what percentage of shorts get liquidated
                liquidation_percentage = min(1.0, price_pump / liquidation_threshold)
                
                # Adjust for retail vs whale positions
                retail_percentage = position_data['retail_short_percentage']
                liquidation_multiplier = 1 + (retail_percentage * 0.5)
                
                estimated_liquidation_volume = (
                    estimated_short_oi * liquidation_percentage * liquidation_multiplier
                )
                
                return estimated_liquidation_volume
            else:
                return 0
                
        except Exception as e:
            self.logger.error(f"Short liquidation estimation failed for {symbol}: {e}")
            return 0
            
    def identify_cascade_risk_zones(self, symbol, heat_map):
        """Identify areas with high cascade risk"""
        try:
            cascade_zones = []
            cascade_threshold = self.liquidation_config['cascade_detection_threshold']
            
            total_liquidations = heat_map['total_liquidations']
            price_levels = heat_map['price_levels']
            
            # Find zones with high liquidation concentrations
            for i, price in enumerate(price_levels):
                liquidation_volume = total_liquidations[price]
                
                if liquidation_volume > cascade_threshold:
                    # Check for clustering with nearby levels
                    cluster_volume = liquidation_volume
                    cluster_start = price
                    cluster_end = price
                    
                    # Look for adjacent high-liquidation levels
                    for j in range(max(0, i-5), min(len(price_levels), i+6)):
                        if j != i and total_liquidations[price_levels[j]] > cascade_threshold * 0.5:
                            cluster_volume += total_liquidations[price_levels[j]]
                            cluster_start = min(cluster_start, price_levels[j])
                            cluster_end = max(cluster_end, price_levels[j])
                            
                    if cluster_volume > cascade_threshold:
                        cascade_zones.append({
                            'price_range': (cluster_start, cluster_end),
                            'center_price': (cluster_start + cluster_end) / 2,
                            'total_volume': cluster_volume,
                            'risk_level': min(1.0, cluster_volume / (cascade_threshold * 3)),
                            'direction': 'down' if price < heat_map['current_price'] else 'up'
                        })
                        
            # Sort by volume and remove overlaps
            cascade_zones.sort(key=lambda x: x['total_volume'], reverse=True)
            heat_map['cascade_risk_zones'] = cascade_zones[:5]  # Top 5 zones
            
        except Exception as e:
            self.logger.error(f"Cascade risk zone identification failed for {symbol}: {e}")
            
    def detect_liquidation_clusters(self, symbol):
        """Detect liquidation clusters and analyze risk"""
        try:
            heat_map = self.heat_maps[symbol]
            
            if not heat_map['price_levels']:
                return
                
            clusters = []
            total_liquidations = heat_map['total_liquidations']
            current_price = heat_map['current_price']
            
            # Cluster detection using moving windows
            window_size = 10  # Price levels
            price_levels = heat_map['price_levels']
            
            for i in range(0, len(price_levels) - window_size, window_size // 2):
                window_prices = price_levels[i:i + window_size]
                window_volumes = [total_liquidations[p] for p in window_prices]
                
                total_volume = sum(window_volumes)
                max_volume = max(window_volumes)
                avg_price = sum(window_prices) / len(window_prices)
                
                if total_volume > self.liquidation_config['liquidation_threshold']:
                    cluster = {
                        'price_center': avg_price,
                        'price_range': (min(window_prices), max(window_prices)),
                        'total_volume': total_volume,
                        'max_volume': max_volume,
                        'distance_from_current': abs(avg_price - current_price) / current_price,
                        'side': 'short' if avg_price > current_price else 'long',
                        'urgency': self.calculate_cluster_urgency(avg_price, current_price, total_volume)
                    }
                    
                    clusters.append(cluster)
                    
            # Sort clusters by urgency and volume
            clusters.sort(key=lambda x: (x['urgency'], x['total_volume']), reverse=True)
            
            # Calculate overall cluster metrics
            total_at_risk = sum(c['total_volume'] for c in clusters)
            cascade_probability = self.calculate_cascade_probability(symbol, clusters)
            
            # Find next major liquidation level
            next_major_level = self.find_next_major_level(symbol, clusters, current_price)
            
            # Update cluster data
            self.liquidation_clusters[symbol] = {
                'clusters': clusters[:10],  # Top 10 clusters
                'total_at_risk': total_at_risk,
                'cascade_probability': cascade_probability,
                'next_major_level': next_major_level,
                'time_to_cascade': self.estimate_time_to_cascade(symbol, next_major_level, current_price),
                'risk_score': min(1.0, total_at_risk / (self.liquidation_config['cascade_detection_threshold'] * 5))
            }
            
        except Exception as e:
            self.logger.error(f"Liquidation cluster detection failed for {symbol}: {e}")
            
    def calculate_cluster_urgency(self, cluster_price, current_price, volume):
        """Calculate urgency score for liquidation cluster"""
        try:
            # Distance factor (closer = more urgent)
            distance = abs(cluster_price - current_price) / current_price
            distance_score = max(0, 1 - (distance / 0.05))  # Max urgency within 5%
            
            # Volume factor (larger = more urgent)
            volume_score = min(1.0, volume / (self.liquidation_config['cascade_detection_threshold'] * 2))
            
            # Combine factors
            urgency = (distance_score + volume_score) / 2
            
            return urgency
            
        except Exception as e:
            self.logger.error(f"Cluster urgency calculation failed: {e}")
            return 0
            
    def calculate_cascade_probability(self, symbol, clusters):
        """Calculate probability of liquidation cascade"""
        try:
            if not clusters:
                return 0
                
            # Factors affecting cascade probability
            factors = {
                'cluster_density': 0,
                'total_volume': 0,
                'proximity_to_current': 0,
                'market_volatility': 0,
                'leverage_concentration': 0
            }
            
            # Cluster density factor
            high_volume_clusters = [c for c in clusters if c['total_volume'] > self.liquidation_config['liquidation_threshold']]
            factors['cluster_density'] = min(1.0, len(high_volume_clusters) / 5)
            
            # Total volume factor
            total_volume = sum(c['total_volume'] for c in clusters)
            factors['total_volume'] = min(1.0, total_volume / (self.liquidation_config['cascade_detection_threshold'] * 3))
            
            # Proximity factor (closer clusters more likely to cascade)
            nearby_clusters = [c for c in clusters if c['distance_from_current'] < 0.05]  # Within 5%
            factors['proximity_to_current'] = min(1.0, len(nearby_clusters) / 3)
            
            # Market volatility factor
            volatility = self.calculate_recent_volatility(symbol)
            factors['market_volatility'] = min(1.0, volatility / 0.05)  # 5% daily vol = max factor
            
            # Leverage concentration factor
            position_data = self.position_estimates[symbol]
            avg_leverage = (position_data['average_long_leverage'] + position_data['average_short_leverage']) / 2
            factors['leverage_concentration'] = min(1.0, avg_leverage / 50)  # 50x leverage = max factor
            
            # Weight the factors
            weights = {
                'cluster_density': 0.25,
                'total_volume': 0.30,
                'proximity_to_current': 0.25,
                'market_volatility': 0.10,
                'leverage_concentration': 0.10
            }
            
            cascade_probability = sum(factors[f] * weights[f] for f in factors)
            
            return min(1.0, cascade_probability)
            
        except Exception as e:
            self.logger.error(f"Cascade probability calculation failed for {symbol}: {e}")
            return 0
            
    def find_next_major_level(self, symbol, clusters, current_price):
        """Find the next major liquidation level"""
        try:
            if not clusters:
                return current_price
                
            # Find the closest high-volume cluster
            sorted_clusters = sorted(clusters, key=lambda x: x['distance_from_current'])
            
            for cluster in sorted_clusters:
                if cluster['total_volume'] > self.liquidation_config['liquidation_threshold']:
                    return cluster['price_center']
                    
            return current_price
            
        except Exception as e:
            self.logger.error(f"Next major level calculation failed for {symbol}: {e}")
            return current_price
            
    def estimate_time_to_cascade(self, symbol, target_price, current_price):
        """Estimate time until potential cascade"""
        try:
            if target_price == current_price:
                return float('inf')
                
            # Calculate required price movement
            price_movement_required = abs(target_price - current_price) / current_price
            
            # Get recent volatility to estimate time
            recent_volatility = self.calculate_recent_volatility(symbol)
            
            if recent_volatility == 0:
                return float('inf')
                
            # Estimate time based on volatility (assuming random walk)
            # Time = (price_movement / volatility)^2
            estimated_time_hours = (price_movement_required / recent_volatility) ** 2 * 24
            
            return min(168, max(0.1, estimated_time_hours))  # Cap at 1 week, min 6 minutes
            
        except Exception as e:
            self.logger.error(f"Time to cascade estimation failed for {symbol}: {e}")
            return float('inf')
            
    def calculate_recent_volatility(self, symbol):
        """Calculate recent volatility for symbol"""
        try:
            # Get recent price data
            klines = self.binance_client.get_klines(symbol, '1h', 24)  # Last 24 hours
            
            if not klines or len(klines) < 2:
                return 0.02  # Default 2% volatility
                
            # Calculate hourly returns
            returns = []
            for i in range(1, len(klines)):
                prev_close = float(klines[i-1][4])
                curr_close = float(klines[i][4])
                
                if prev_close > 0:
                    hourly_return = (curr_close - prev_close) / prev_close
                    returns.append(hourly_return)
                    
            if not returns:
                return 0.02
                
            # Calculate daily volatility (annualized)
            hourly_volatility = np.std(returns)
            daily_volatility = hourly_volatility * np.sqrt(24)
            
            return daily_volatility
            
        except Exception as e:
            self.logger.error(f"Volatility calculation failed for {symbol}: {e}")
            return 0.02
            
    def monitor_cascade_risk(self, symbol):
        """Monitor and alert on cascade risk"""
        try:
            cluster_data = self.liquidation_clusters[symbol]
            cascade_probability = cluster_data['cascade_probability']
            
            # Update cascade detection
            self.cascade_detection['cascade_probability'][symbol] = cascade_probability
            
            # Check for high cascade risk
            if cascade_probability > 0.7:  # 70% cascade probability
                if not self.cascade_detection['active_cascade_risk'][symbol]:
                    self.generate_cascade_alert(symbol, cluster_data)
                    self.cascade_detection['active_cascade_risk'][symbol] = True
                    
            elif cascade_probability < 0.4:  # Risk reduced
                self.cascade_detection['active_cascade_risk'][symbol] = False
                
            # Identify cascade triggers
            self.identify_cascade_triggers(symbol, cluster_data)
            
        except Exception as e:
            self.logger.error(f"Cascade risk monitoring failed for {symbol}: {e}")
            
    def generate_cascade_alert(self, symbol, cluster_data):
        """Generate cascade risk alert"""
        try:
            cascade_probability = cluster_data['cascade_probability']
            total_at_risk = cluster_data['total_at_risk']
            next_major_level = cluster_data['next_major_level']
            time_to_cascade = cluster_data['time_to_cascade']
            
            alert_data = {
                'type': 'liquidation_cascade_risk',
                'symbol': symbol,
                'cascade_probability': cascade_probability,
                'total_volume_at_risk': total_at_risk,
                'next_major_level': next_major_level,
                'estimated_time_to_cascade': time_to_cascade,
                'recommendation': self.get_cascade_recommendation(symbol, cluster_data),
                'timestamp': time.time()
            }
            
            self.logger.warning(
                f"üî• CASCADE RISK ALERT: {symbol} "
                f"Probability: {cascade_probability:.1%} "
                f"At Risk: ${total_at_risk:,.0f} "
                f"Level: ${next_major_level:.2f}"
            )
            
            # Store alert for other systems
            self.memory_manager.store_cascade_alert(alert_data)
            
        except Exception as e:
            self.logger.error(f"Cascade alert generation failed for {symbol}: {e}")
            
    def get_cascade_recommendation(self, symbol, cluster_data):
        """Get trading recommendation based on cascade risk"""
        try:
            cascade_probability = cluster_data['cascade_probability']
            next_major_level = cluster_data['next_major_level']
            current_price = self.heat_maps[symbol]['current_price']
            
            recommendations = []
            
            if cascade_probability > 0.8:  # Very high risk
                if next_major_level < current_price:
                    recommendations.extend([
                        'Avoid long positions',
                        'Consider short positions with tight stops',
                        'Reduce leverage significantly',
                        'Wait for cascade to complete before buying'
                    ])
                else:
                    recommendations.extend([
                        'Avoid short positions',
                        'Consider long positions with tight stops',
			'Reduce leverage significantly',
                    'Wait for cascade to complete before selling'
                ])
                    
            elif cascade_probability > 0.6:  # High risk
                recommendations.extend([
                    'Use maximum 5x leverage',
                    'Set very tight stop losses',
                    'Monitor price action closely',
                    'Consider reducing position sizes'
                ])
                
            elif cascade_probability > 0.4:  # Medium risk
                recommendations.extend([
                    'Use conservative leverage (10x max)',
                    'Set reasonable stop losses',
                    'Avoid adding to losing positions'
                ])
                
            else:  # Low risk
                recommendations.extend([
                    'Normal trading conditions',
                    'Standard risk management applies'
                ])
                
            return recommendations
            
        except Exception as e:
            self.logger.error(f"Cascade recommendation generation failed for {symbol}: {e}")
            return ['Error generating recommendations']
            
    def identify_cascade_triggers(self, symbol, cluster_data):
        """Identify potential cascade trigger points"""
        try:
            current_price = self.heat_maps[symbol]['current_price']
            clusters = cluster_data['clusters']
            
            triggers = []
            
            for cluster in clusters:
                # Calculate trigger distance
                trigger_distance = abs(cluster['price_center'] - current_price) / current_price
                
                if trigger_distance < 0.03:  # Within 3%
                    trigger = {
                        'price': cluster['price_center'],
                        'volume': cluster['total_volume'],
                        'distance': trigger_distance,
                        'side': cluster['side'],
                        'urgency': cluster['urgency'],
                        'trigger_probability': 1 - trigger_distance * 10  # Closer = higher probability
                    }
                    
                    triggers.append(trigger)
                    
            # Sort by urgency and probability
            triggers.sort(key=lambda x: (x['urgency'], x['trigger_probability']), reverse=True)
            
            self.cascade_detection['cascade_triggers'][symbol] = triggers[:5]  # Top 5 triggers
            
        except Exception as e:
            self.logger.error(f"Cascade trigger identification failed for {symbol}: {e}")
            
    def estimate_market_positions(self, symbol):
        """Estimate current market positions and leverage distribution"""
        try:
            # Get market data
            ticker = self.binance_client.get_ticker(symbol)
            oi_data = self.get_open_interest_data(symbol)
            funding_rate = self.get_funding_rate(symbol)
            
            if not ticker or not oi_data:
                return
                
            current_price = float(ticker['lastPrice'])
            open_interest = oi_data.get('openInterest', 0)
            
            # Estimate long/short distribution based on funding rate
            if funding_rate > 0.01:  # High positive funding = more longs
                long_oi_percentage = 0.6 + min(0.3, funding_rate * 20)
            elif funding_rate < -0.01:  # High negative funding = more shorts
                long_oi_percentage = 0.4 - min(0.3, abs(funding_rate) * 20)
            else:
                long_oi_percentage = 0.5  # Balanced
                
            short_oi_percentage = 1 - long_oi_percentage
            
            # Estimate average leverage based on market conditions
            volatility = self.calculate_recent_volatility(symbol)
            
            # Higher volatility = lower average leverage (people are more cautious)
            if volatility > 0.05:  # >5% daily vol
                avg_long_leverage = 15
                avg_short_leverage = 12
            elif volatility > 0.03:  # 3-5% daily vol
                avg_long_leverage = 20
                avg_short_leverage = 18
            else:  # <3% daily vol
                avg_long_leverage = 25
                avg_short_leverage = 22
                
            # Estimate retail vs whale distribution
            volume_24h = float(ticker['volume'])
            large_trade_threshold = current_price * 10000  # $10k+ trades considered whale
            
            # Estimate based on order size distribution (approximate)
            whale_percentage = min(0.4, volume_24h / (open_interest * 50))  # Max 40% whale
            retail_percentage = 1 - whale_percentage
            
            # Update position estimates
            position_data = self.position_estimates[symbol]
            position_data.update({
                'estimated_long_oi': open_interest * long_oi_percentage,
                'estimated_short_oi': open_interest * short_oi_percentage,
                'average_long_leverage': avg_long_leverage,
                'average_short_leverage': avg_short_leverage,
                'retail_long_percentage': retail_percentage * long_oi_percentage,
                'retail_short_percentage': retail_percentage * short_oi_percentage,
                'whale_long_percentage': whale_percentage * long_oi_percentage,
                'whale_short_percentage': whale_percentage * short_oi_percentage,
                'last_update': time.time()
            })
            
            # Calculate market pressure
            self.calculate_market_pressure(symbol, position_data, funding_rate, volatility)
            
        except Exception as e:
            self.logger.error(f"Position estimation failed for {symbol}: {e}")
            
    def calculate_market_pressure(self, symbol, position_data, funding_rate, volatility):
        """Calculate various market pressure metrics"""
        try:
            pressure_data = self.market_pressure[symbol]
            
            # Long pressure (based on long OI and leverage)
            long_oi = position_data['estimated_long_oi']
            avg_long_lev = position_data['average_long_leverage']
            long_pressure = (long_oi * avg_long_lev) / 1000000  # Normalize to millions
            
            # Short pressure
            short_oi = position_data['estimated_short_oi']
            avg_short_lev = position_data['average_short_leverage']
            short_pressure = (short_oi * avg_short_lev) / 1000000
            
            # Net pressure (positive = more long pressure)
            net_pressure = long_pressure - short_pressure
            
            # Leverage-weighted pressure
            total_leverage_weighted = (long_oi * avg_long_lev) + (short_oi * avg_short_lev)
            leverage_weighted_pressure = total_leverage_weighted / 1000000
            
            # Retail vs whale pressure
            retail_long = position_data['retail_long_percentage'] * long_oi
            retail_short = position_data['retail_short_percentage'] * short_oi
            whale_long = position_data['whale_long_percentage'] * long_oi
            whale_short = position_data['whale_short_percentage'] * short_oi
            
            retail_net_pressure = (retail_long - retail_short) / 1000000
            whale_net_pressure = (whale_long - whale_short) / 1000000
            retail_vs_whale_pressure = retail_net_pressure - whale_net_pressure
            
            # Momentum pressure (based on funding rate trend)
            momentum_pressure = funding_rate * 100  # Scale for visibility
            
            # Fear/greed pressure (based on volatility and funding)
            if funding_rate > 0.005 and volatility < 0.03:
                fear_greed_pressure = 0.8  # Greed (high funding, low vol)
            elif funding_rate < -0.005 and volatility > 0.05:
                fear_greed_pressure = -0.8  # Fear (negative funding, high vol)
            else:
                fear_greed_pressure = funding_rate * 50  # Neutral scaling
                
            # Update pressure metrics
            pressure_data.update({
                'long_pressure': long_pressure,
                'short_pressure': short_pressure,
                'net_pressure': net_pressure,
                'leverage_weighted_pressure': leverage_weighted_pressure,
                'retail_vs_whale_pressure': retail_vs_whale_pressure,
                'momentum_pressure': momentum_pressure,
                'fear_greed_pressure': fear_greed_pressure,
                'last_update': time.time()
            })
            
        except Exception as e:
            self.logger.error(f"Market pressure calculation failed for {symbol}: {e}")
            
    def get_open_interest_data(self, symbol):
        """Get open interest data from Binance"""
        try:
            oi_data = self.binance_client.get_open_interest(symbol)
            return oi_data
        except Exception as e:
            self.logger.error(f"Open interest data fetch failed for {symbol}: {e}")
            return None
            
    def get_funding_rate(self, symbol):
        """Get current funding rate"""
        try:
            funding_data = self.binance_client.get_funding_rate(symbol)
            if funding_data:
                return float(funding_data.get('fundingRate', 0))
            return 0
        except Exception as e:
            self.logger.error(f"Funding rate fetch failed for {symbol}: {e}")
            return 0
            
    def get_current_price(self, symbol):
        """Get current price for symbol"""
        try:
            ticker = self.binance_client.get_ticker(symbol)
            if ticker:
                return float(ticker['lastPrice'])
            return None
        except Exception as e:
            self.logger.error(f"Price fetch failed for {symbol}: {e}")
            return None
            
    def get_liquidation_opportunity_score(self, symbol):
        """Calculate liquidation opportunity score for trading"""
        try:
            heat_map = self.heat_maps[symbol]
            cluster_data = self.liquidation_clusters[symbol]
            pressure_data = self.market_pressure[symbol]
            
            if not heat_map.get('price_levels'):
                return 0
                
            score_factors = {
                'cascade_probability': cluster_data['cascade_probability'],
                'total_volume_at_risk': min(1.0, cluster_data['total_at_risk'] / 10000000),  # Cap at $10M
                'pressure_imbalance': abs(pressure_data['net_pressure']) / 100,  # Normalize
                'volatility_factor': min(1.0, self.calculate_recent_volatility(symbol) / 0.05),
                'proximity_factor': 1 / (1 + cluster_data.get('next_major_level', 0))
            }
            
            # Weight the factors
            weights = {
                'cascade_probability': 0.35,
                'total_volume_at_risk': 0.25,
                'pressure_imbalance': 0.20,
                'volatility_factor': 0.15,
                'proximity_factor': 0.05
            }
            
            opportunity_score = sum(score_factors[f] * weights[f] for f in score_factors)
            
            return min(1.0, opportunity_score)
            
        except Exception as e:
            self.logger.error(f"Opportunity score calculation failed for {symbol}: {e}")
            return 0
            
    def get_liquidation_trade_signal(self, symbol):
        """Generate trade signal based on liquidation analysis"""
        try:
            opportunity_score = self.get_liquidation_opportunity_score(symbol)
            cluster_data = self.liquidation_clusters[symbol]
            pressure_data = self.market_pressure[symbol]
            heat_map = self.heat_maps[symbol]
            
            if opportunity_score < 0.3:  # Low opportunity
                return None
                
            current_price = heat_map.get('current_price')
            next_major_level = cluster_data.get('next_major_level')
            cascade_probability = cluster_data.get('cascade_probability', 0)
            
            if not current_price or not next_major_level:
                return None
                
            # Determine signal direction
            if next_major_level < current_price:  # Liquidations below = short signal
                signal_type = 'SHORT'
                entry_price = current_price
                target_price = next_major_level
                stop_price = current_price * 1.02  # 2% stop loss
                
            else:  # Liquidations above = long signal
                signal_type = 'LONG'
                entry_price = current_price
                target_price = next_major_level
                stop_price = current_price * 0.98  # 2% stop loss
                
            # Calculate position size based on risk
            risk_percentage = min(0.05, 0.01 + (opportunity_score * 0.04))  # 1-5% risk
            
            # Adjust leverage based on cascade probability
            base_leverage = 10
            if cascade_probability > 0.8:
                leverage = min(25, base_leverage * 2)  # High confidence = higher leverage
            elif cascade_probability > 0.6:
                leverage = min(20, base_leverage * 1.5)
            else:
                leverage = base_leverage
                
            signal = {
                'symbol': symbol,
                'type': signal_type,
                'entry_price': entry_price,
                'target_price': target_price,
                'stop_loss': stop_price,
                'leverage': leverage,
                'risk_percentage': risk_percentage,
                'opportunity_score': opportunity_score,
                'cascade_probability': cascade_probability,
                'reasoning': f"Liquidation cascade expected at ${target_price:.2f}",
                'confidence': opportunity_score,
                'timestamp': time.time(),
                'strategy': 'liquidation_cascade'
            }
            
            return signal
            
        except Exception as e:
            self.logger.error(f"Trade signal generation failed for {symbol}: {e}")
            return None
            
    def get_liquidation_analytics(self, symbol=None):
        """Get comprehensive liquidation analytics"""
        try:
            if symbol:
                symbols = [symbol]
            else:
                symbols = self.liquidation_config['target_symbols']
                
            analytics = {}
            
            for sym in symbols:
                heat_map = self.heat_maps[sym]
                cluster_data = self.liquidation_clusters[sym]
                pressure_data = self.market_pressure[sym]
                
                analytics[sym] = {
                    'heat_map_summary': {
                        'current_price': heat_map.get('current_price'),
                        'max_liquidation_level': heat_map.get('max_liquidation_level'),
                        'cascade_risk_zones': len(heat_map.get('cascade_risk_zones', [])),
                        'last_update': heat_map.get('last_update')
                    },
                    'cluster_analysis': {
                        'total_clusters': len(cluster_data['clusters']),
                        'total_at_risk': cluster_data['total_at_risk'],
                        'cascade_probability': cluster_data['cascade_probability'],
                        'next_major_level': cluster_data['next_major_level'],
                        'risk_score': cluster_data['risk_score']
                    },
                    'market_pressure': {
                        'net_pressure': pressure_data['net_pressure'],
                        'leverage_weighted_pressure': pressure_data['leverage_weighted_pressure'],
                        'retail_vs_whale_pressure': pressure_data['retail_vs_whale_pressure'],
                        'fear_greed_pressure': pressure_data['fear_greed_pressure']
                    },
                    'opportunity_score': self.get_liquidation_opportunity_score(sym),
                    'active_cascade_risk': self.cascade_detection['active_cascade_risk'][sym],
                    'trade_signal': self.get_liquidation_trade_signal(sym)
                }
                
            return analytics
            
        except Exception as e:
            self.logger.error(f"Analytics generation failed: {e}")
            return {}
            
    def save_liquidation_state(self):
        """Save current liquidation analysis state"""
        try:
            state_data = {
                'heat_maps': dict(self.heat_maps),
                'liquidation_clusters': dict(self.liquidation_clusters),
                'market_pressure': dict(self.market_pressure),
                'cascade_detection': dict(self.cascade_detection),
                'position_estimates': dict(self.position_estimates),
                'timestamp': time.time()
            }
            
            self.memory_manager.save_liquidation_state(state_data)
            self.logger.info("üíæ Liquidation state saved successfully")
            
        except Exception as e:
            self.logger.error(f"Liquidation state save failed: {e}")
            
    def load_liquidation_state(self):
        """Load previous liquidation analysis state"""
        try:
            state_data = self.memory_manager.load_liquidation_state()
            
            if state_data:
                self.heat_maps.update(state_data.get('heat_maps', {}))
                self.liquidation_clusters.update(state_data.get('liquidation_clusters', {}))
                self.market_pressure.update(state_data.get('market_pressure', {}))
                self.cascade_detection.update(state_data.get('cascade_detection', {}))
                self.position_estimates.update(state_data.get('position_estimates', {}))
                
                self.logger.info("üìö Liquidation state loaded successfully")
                return True
            else:
                self.logger.info("üì≠ No previous liquidation state found")
                return False
                
        except Exception as e:
            self.logger.error(f"Liquidation state load failed: {e}")
            return False

class AutoCompoundingEngine:
    """Advanced auto-compounding system for aggressive capital growth"""
    
    def __init__(self, config, binance_client, memory_manager, performance_analytics, risk_manager):
        self.config = config
        self.binance_client = binance_client
        self.memory_manager = memory_manager
        self.performance_analytics = performance_analytics
        self.risk_manager = risk_manager
        self.logger = logging.getLogger('MonsterBot.AutoCompounding')
        
        # Compounding configuration
        self.compounding_config = {
            'target_daily_returns': {
                (0, 1000): 0.30,        # 30% daily for $0-$1k (PSYCHO MODE)
                (1000, 10000): 0.25,    # 25% daily for $1k-$10k (HUNTER MODE)
                (10000, 100000): 0.20,  # 20% daily for $10k-$100k (ACCUMULATOR MODE)
                (100000, 1000000): 0.15, # 15% daily for $100k-$1M (WHALE MODE)
                (1000000, 10000000): 0.10, # 10% daily for $1M-$10M
                (10000000, 100000000): 0.05, # 5% daily for $10M-$100M
                (100000000, float('inf')): 0.01  # 1% daily for $100M+
            },
            'compounding_frequencies': {
                'continuous': True,      # Compound after every profitable trade
                'daily': True,          # Daily compounding calculations
                'weekly': True,         # Weekly optimization
                'monthly': True         # Monthly strategy adjustments
            },
            'capital_allocation_modes': {
                'aggressive_growth': 0.90,    # 90% of capital actively trading
                'balanced_growth': 0.70,      # 70% trading, 30% reserved
                'conservative_growth': 0.50,  # 50% trading, 50% reserved
                'preservation': 0.30          # 30% trading, 70% preserved
            },
            'risk_scaling_factors': {
                'winning_streak': 1.5,        # Increase risk on wins
                'losing_streak': 0.5,         # Decrease risk on losses
                'high_volatility': 0.8,       # Reduce risk in high vol
                'low_volatility': 1.2,        # Increase risk in low vol
                'market_trending': 1.3,       # Increase risk in trends
                'market_ranging': 0.9         # Reduce risk in ranges
            }
        }
        
        # Capital growth tracking
        self.capital_growth = {
            'initial_capital': 0,
            'current_capital': 0,
            'daily_targets': {},
            'daily_performance': deque(maxlen=30),  # 30 days of performance
            'weekly_performance': deque(maxlen=12), # 12 weeks of performance
            'monthly_performance': deque(maxlen=6), # 6 months of performance
            'growth_milestones': [],
            'compound_interest_earned': 0,
            'total_trades_count': 0,
            'profitable_trades_count': 0
        }
        
        # Dynamic position sizing
        self.position_sizing = {
            'base_risk_percentage': 0.02,      # 2% base risk per trade
            'max_risk_percentage': 0.10,       # 10% maximum risk per trade
            'min_risk_percentage': 0.005,      # 0.5% minimum risk per trade
            'leverage_scaling': {
                'psycho_mode': (50, 100),       # 50x-100x leverage
                'hunter_mode': (20, 50),        # 20x-50x leverage
                'accumulator_mode': (10, 25),   # 10x-25x leverage
                'whale_mode': (5, 15)           # 5x-15x leverage
            },
            'position_correlation_limit': 0.70, # Max 70% correlation between positions
            'max_open_positions': 10,           # Maximum concurrent positions
            'capital_per_position_limit': 0.20  # Max 20% capital per position
        }
        
        # Profit reinvestment strategies
        self.reinvestment_strategies = {
            'immediate_reinvest': {
                'enabled': True,
                'profit_threshold': 0.05,      # 5% profit triggers reinvestment
                'reinvest_percentage': 0.80,   # Reinvest 80% of profits
                'reserve_percentage': 0.20     # Keep 20% as reserves
            },
            'milestone_reinvest': {
                'enabled': True,
                'milestone_percentages': [0.25, 0.50, 1.0, 2.0],  # 25%, 50%, 100%, 200% gains
                'reinvest_multiplier': 1.2      # Increase base position size by 20%
            },
            'streak_based_reinvest': {
                'enabled': True,
                'winning_streak_threshold': 5,  # 5 wins in a row
                'reinvest_bonus': 0.10,         # 10% bonus reinvestment
                'losing_streak_protection': 3   # Stop reinvesting after 3 losses
            },
            'volatility_based_reinvest': {
                'enabled': True,
                'high_vol_threshold': 0.05,     # 5% daily volatility
                'low_vol_threshold': 0.02,      # 2% daily volatility
                'high_vol_reduction': 0.5,      # Reduce reinvestment by 50% in high vol
                'low_vol_increase': 1.5         # Increase reinvestment by 50% in low vol
            }
        }
        
        # Capital allocation matrix
        self.capital_allocation = {
            'active_trading_capital': 0,
            'reserved_capital': 0,
            'emergency_fund': 0,
            'strategy_allocations': defaultdict(float),
            'symbol_allocations': defaultdict(float),
            'leverage_allocations': defaultdict(float),
            'timeframe_allocations': defaultdict(float)
        }
        
        # Growth optimization algorithms
        self.optimization_algorithms = {
            'kelly_criterion': {
                'enabled': True,
                'lookback_trades': 50,          # Last 50 trades for calculation
                'max_kelly_percentage': 0.25,   # Cap Kelly at 25%
                'confidence_threshold': 0.60    # 60% win rate minimum
            },
            'sharpe_optimization': {
                'enabled': True,
                'target_sharpe': 3.0,           # Target Sharpe ratio
                'risk_free_rate': 0.02,         # 2% risk-free rate
                'lookback_period': 30           # 30 days for calculation
            },
            'drawdown_protection': {
                'max_drawdown_threshold': 0.20, # 20% max drawdown
                'drawdown_recovery_factor': 2,  # 2x recovery multiplier
                'protection_mode_threshold': 0.15 # 15% drawdown triggers protection
            },
            'volatility_targeting': {
                'target_volatility': 0.30,     # 30% annual volatility target
                'vol_adjustment_factor': 0.5,   # 50% adjustment speed
                'min_vol_threshold': 0.10,      # 10% minimum volatility
                'max_vol_threshold': 0.60       # 60% maximum volatility
            }
        }
        
        # Performance tracking
        self.performance_metrics = {
            'daily_returns': deque(maxlen=365),
            'weekly_returns': deque(maxlen=52),
            'monthly_returns': deque(maxlen=12),
            'rolling_sharpe': deque(maxlen=30),
            'rolling_max_drawdown': deque(maxlen=30),
            'rolling_win_rate': deque(maxlen=30),
            'rolling_profit_factor': deque(maxlen=30),
            'compound_growth_rate': 0,
            'total_return': 0,
            'annualized_return': 0
        }
        
        # Current trading state
        self.trading_state = {
            'current_mode': 'psycho_mode',      # Start aggressive
            'winning_streak': 0,
            'losing_streak': 0,
            'daily_trades_count': 0,
            'daily_profit_target': 0,
            'daily_profit_achieved': 0,
            'last_milestone_hit': 0,
            'next_milestone_target': 0,
            'risk_adjustment_factor': 1.0,
            'leverage_multiplier': 1.0
        }
        
        # Initialize auto-compounding system
        self.initialize_compounding_system()
        
    def initialize_compounding_system(self):
        """Initialize the auto-compounding system"""
        try:
            # Load previous state
            self.load_compounding_state()
            
            # Get current account balance
            self.update_current_capital()
            
            # Determine trading mode based on capital
            self.update_trading_mode()
            
            # Calculate daily targets
            self.calculate_daily_targets()
            
            # Start compounding threads
            self.start_compounding_monitors()
            
            self.logger.info("üí∞ Auto-Compounding Engine initialized")
            self.logger.info(f"   üéØ Current Capital: ${self.capital_growth['current_capital']:,.2f}")
            self.logger.info(f"   üî• Trading Mode: {self.trading_state['current_mode']}")
            self.logger.info(f"   üìà Daily Target: {self.trading_state['daily_profit_target']:.1%}")
            
        except Exception as e:
            self.logger.error(f"Auto-compounding initialization failed: {e}")
            
    def update_current_capital(self):
        """Update current capital from account balance"""
        try:
            account_info = self.binance_client.get_account_balance()
            
            if account_info:
                # Calculate total USDT value
                total_usdt = 0
                for balance in account_info['balances']:
                    asset = balance['asset']
                    free = float(balance['free'])
                    locked = float(balance['locked'])
                    total_balance = free + locked
                    
                    if asset == 'USDT':
                        total_usdt += total_balance
                    elif total_balance > 0:
                        # Convert to USDT value
                        try:
                            ticker = self.binance_client.get_ticker(f"{asset}USDT")
                            if ticker:
                                price = float(ticker['lastPrice'])
                                total_usdt += total_balance * price
                        except:
                            pass  # Skip if conversion fails
                            
                # Update capital tracking
                if self.capital_growth['initial_capital'] == 0:
                    self.capital_growth['initial_capital'] = total_usdt
                    
                self.capital_growth['current_capital'] = total_usdt
                
                self.logger.info(f"üí∞ Capital updated: ${total_usdt:,.2f}")
                
        except Exception as e:
            self.logger.error(f"Capital update failed: {e}")
            
    def update_trading_mode(self):
        """Update trading mode based on current capital"""
        try:
            current_capital = self.capital_growth['current_capital']
            
            if current_capital < 1000:
                new_mode = 'psycho_mode'
            elif current_capital < 10000:
                new_mode = 'hunter_mode'
            elif current_capital < 100000:
                new_mode = 'accumulator_mode'
            else:
                new_mode = 'whale_mode'
                
            if new_mode != self.trading_state['current_mode']:
                old_mode = self.trading_state['current_mode']
                self.trading_state['current_mode'] = new_mode
                
                self.logger.info(f"üéØ Trading mode changed: {old_mode} ‚Üí {new_mode}")
                
                # Adjust strategies for new mode
                self.adjust_strategies_for_mode(new_mode)
                
        except Exception as e:
            self.logger.error(f"Trading mode update failed: {e}")
            
    def calculate_daily_targets(self):
        """Calculate daily profit targets based on capital and mode"""
        try:
            current_capital = self.capital_growth['current_capital']
            
            # Find target return rate for current capital level
            target_return = 0.10  # Default 10%
            for (min_cap, max_cap), return_rate in self.compounding_config['target_daily_returns'].items():
                if min_cap <= current_capital < max_cap:
                    target_return = return_rate
                    break
                    
            # Calculate target profit in USDT
            daily_profit_target = current_capital * target_return
            
            # Adjust based on recent performance
            recent_performance = self.get_recent_performance()
            if recent_performance < target_return * 0.8:  # Underperforming
                target_return *= 1.2  # Increase target by 20%
            elif recent_performance > target_return * 1.2:  # Overperforming
                target_return *= 0.9  # Slightly reduce target
                
            self.trading_state['daily_profit_target'] = target_return
            
            # Store in daily targets
            today = datetime.now().strftime('%Y-%m-%d')
            self.capital_growth['daily_targets'][today] = {
                'capital': current_capital,
                'target_return': target_return,
                'target_profit_usdt': daily_profit_target,
                'actual_return': 0,
                'actual_profit_usdt': 0
            }
            
            self.logger.info(f"üéØ Daily target set: {target_return:.1%} (${daily_profit_target:,.2f})")
            
        except Exception as e:
            self.logger.error(f"Daily target calculation failed: {e}")
            
    def get_recent_performance(self):
        """Get recent performance metrics"""
        try:
            if len(self.performance_metrics['daily_returns']) < 7:
                return 0
                
            recent_returns = list(self.performance_metrics['daily_returns'])[-7:]
            return np.mean(recent_returns)
            
        except Exception as e:
            self.logger.error(f"Recent performance calculation failed: {e}")
            return 0
            
    def calculate_optimal_position_size(self, symbol, strategy, signal_confidence):
        """Calculate optimal position size using multiple factors"""
        try:
            current_capital = self.capital_growth['current_capital']
            current_mode = self.trading_state['current_mode']
            
            # Base risk percentage
            base_risk = self.position_sizing['base_risk_percentage']
            
            # Adjust for trading mode
            if current_mode == 'psycho_mode':
                base_risk *= 2.5  # 5% base risk
            elif current_mode == 'hunter_mode':
                base_risk *= 2.0  # 4% base risk
            elif current_mode == 'accumulator_mode':
                base_risk *= 1.5  # 3% base risk
            # whale_mode uses default 2%
            
            # Adjust for signal confidence
            confidence_multiplier = 0.5 + (signal_confidence * 1.5)  # 0.5x to 2.0x
            base_risk *= confidence_multiplier
            
            # Adjust for winning/losing streaks
            streak_multiplier = self.calculate_streak_multiplier()
            base_risk *= streak_multiplier
            
            # Adjust for market volatility
            volatility_multiplier = self.calculate_volatility_multiplier(symbol)
            base_risk *= volatility_multiplier
            
            # Adjust for Kelly criterion
            kelly_multiplier = self.calculate_kelly_multiplier(strategy)
            base_risk *= kelly_multiplier
            
            # Apply limits
            base_risk = max(self.position_sizing['min_risk_percentage'], base_risk)
            base_risk = min(self.position_sizing['max_risk_percentage'], base_risk)
            
            # Calculate position size in USDT
            position_size_usdt = current_capital * base_risk
            
            # Apply capital per position limit
            max_position_size = current_capital * self.position_sizing['capital_per_position_limit']
            position_size_usdt = min(position_size_usdt, max_position_size)
            
            # Calculate optimal leverage
            optimal_leverage = self.calculate_optimal_leverage(current_mode, signal_confidence, base_risk)
            
            return {
                'position_size_usdt': position_size_usdt,
                'risk_percentage': base_risk,
                'leverage': optimal_leverage,
                'confidence_multiplier': confidence_multiplier,
                'streak_multiplier': streak_multiplier,
                'volatility_multiplier': volatility_multiplier,
                'kelly_multiplier': kelly_multiplier
            }
            
        except Exception as e:
            self.logger.error(f"Position size calculation failed: {e}")
            return {
                'position_size_usdt': current_capital * 0.01,
                'risk_percentage': 0.01,
                'leverage': 10,
                'confidence_multiplier': 1.0,
                'streak_multiplier': 1.0,
                'volatility_multiplier': 1.0,
                'kelly_multiplier': 1.0
            }
            
    def calculate_streak_multiplier(self):
        """Calculate position size multiplier based on winning/losing streaks"""
        try:
            winning_streak = self.trading_state['winning_streak']
            losing_streak = self.trading_state['losing_streak']
            
            if winning_streak >= 3:
                # Increase size on winning streaks (but cap the increase)
                multiplier = min(2.0, 1 + (winning_streak * 0.15))
            elif losing_streak >= 2:
                # Decrease size on losing streaks
                multiplier = max(0.3, 1 - (losing_streak * 0.20))
            else:
                multiplier = 1.0
                
            return multiplier
            
        except Exception as e:
            self.logger.error(f"Streak multiplier calculation failed: {e}")
            return 1.0
            
    def calculate_volatility_multiplier(self, symbol):
        """Calculate position size multiplier based on market volatility"""
        try:
            # Get recent volatility from performance analytics
            volatility = self.performance_analytics.get_symbol_volatility(symbol)
            
            if volatility is None:
                return 1.0
                
            target_vol = self.optimization_algorithms['volatility_targeting']['target_volatility']
            
            if volatility > target_vol * 1.5:  # High volatility
                multiplier = 0.6  # Reduce position size
            elif volatility > target_vol:
                multiplier = 0.8
            elif volatility < target_vol * 0.5:  # Low volatility
                multiplier = 1.4  # Increase position size
            elif volatility < target_vol:
                multiplier = 1.2
            else:
                multiplier = 1.0
                
            return multiplier
            
        except Exception as e:
            self.logger.error(f"Volatility multiplier calculation failed: {e}")
            return 1.0
            
    def calculate_kelly_multiplier(self, strategy):
        """Calculate Kelly criterion multiplier"""
        try:
            # Get strategy performance data
            strategy_stats = self.performance_analytics.get_strategy_stats(strategy)
            
            if not strategy_stats or strategy_stats['total_trades'] < 20:
                return 1.0  # Not enough data
                
            win_rate = strategy_stats['win_rate']
            avg_win = strategy_stats['avg_win_percentage']
            avg_loss = strategy_stats['avg_loss_percentage']
            
            if avg_loss == 0:
                return 1.0
                
            # Kelly formula: f = (bp - q) / b
            # Where: b = odds (avg_win/avg_loss), p = win_rate, q = 1-p
            b = avg_win / abs(avg_loss)
            p = win_rate
            q = 1 - p
            
            kelly_fraction = (b * p - q) / b
            
            # Cap Kelly at maximum percentage
            max_kelly = self.optimization_algorithms['kelly_criterion']['max_kelly_percentage']
            kelly_fraction = max(0, min(max_kelly, kelly_fraction))
            
            # Convert to multiplier (base risk is 2%, so if Kelly suggests 10%, multiplier is 5)
            base_risk = self.position_sizing['base_risk_percentage']
            kelly_multiplier = kelly_fraction / base_risk if base_risk > 0 else 1.0
            kelly_multiplier = max(0.2, min(3.0, kelly_multiplier))  # Limit to 0.2x - 3.0x
            
            return kelly_multiplier
            
        except Exception as e:
            self.logger.error(f"Kelly multiplier calculation failed: {e}")
            return 1.0
            
    def calculate_optimal_leverage(self, trading_mode, signal_confidence, risk_percentage):
        """Calculate optimal leverage based on multiple factors"""
        try:
            # Get base leverage range for trading mode
            leverage_range = self.position_sizing['leverage_scaling'][trading_mode]
            min_leverage, max_leverage = leverage_range
            
            # Start with base leverage (middle of range)
            base_leverage = (min_leverage + max_leverage) / 2
            
            # Adjust for signal confidence
            confidence_adjustment = 0.5 + signal_confidence  # 0.5x to 1.5x
            leverage = base_leverage * confidence_adjustment
            
            # Adjust for risk percentage (higher risk = lower leverage to maintain same exposure)
            risk_adjustment = self.position_sizing['base_risk_percentage'] / risk_percentage
            leverage *= risk_adjustment
            
            # Adjust for recent performance
            if self.trading_state['winning_streak'] >= 5:
                leverage *= 1.3  # Increase leverage on winning streaks
            elif self.trading_state['losing_streak'] >= 3:
                leverage *= 0.7  # Decrease leverage on losing streaks
                
            # Apply mode limits
            leverage = max(min_leverage, min(max_leverage, leverage))
            
            # Round to nearest integer
            leverage = round(leverage)
            
            return leverage
            
        except Exception as e:
            self.logger.error(f"Optimal leverage calculation failed: {e}")
            return 10  # Default leverage
            
    def process_trade_result(self, trade_result):
        """Process completed trade result for compounding"""
        try:
            symbol = trade_result['symbol']
            profit_usdt = trade_result['profit_usdt']
            profit_percentage = trade_result['profit_percentage']
            strategy = trade_result['strategy']
            
            # Update capital
            self.capital_growth['current_capital'] += profit_usdt
            self.capital_growth['total_trades_count'] += 1
            
            # Update streaks
            if profit_usdt > 0:
                self.capital_growth['profitable_trades_count'] += 1
                self.trading_state['winning_streak'] += 1
                self.trading_state['losing_streak'] = 0
            else:
                self.trading_state['winning_streak'] = 0
                self.trading_state['losing_streak'] += 1
                
            # Update daily progress
            self.trading_state['daily_profit_achieved'] += profit_percentage
            self.trading_state['daily_trades_count'] += 1
            
            # Update performance metrics
            self.performance_metrics['daily_returns'].append(profit_percentage)
            
            # Check for milestones
            self.check_growth_milestones()
            
            # Process profit reinvestment
            if profit_usdt > 0:
                self.process_profit_reinvestment(profit_usdt, profit_percentage, strategy)
                
            # Update trading mode if capital changed significantly
            self.update_trading_mode()
            
            # Log progress
            win_rate = self.capital_growth['profitable_trades_count'] / self.capital_growth['total_trades_count']
            self.logger.info(
                f"üí∞ Trade processed: {symbol} "
                f"P&L: ${profit_usdt:,.2f} ({profit_percentage:+.2%}) "
                f"Capital: ${self.capital_growth['current_capital']:,.2f} "
                f"Streak: W{self.trading_state['winning_streak']}/L{self.trading_state['losing_streak']} "
                f"Win Rate: {win_rate:.1%}"
            )
            
        except Exception as e:
            self.logger.error(f"Trade result processing failed: {e}")
            
    def process_profit_reinvestment(self, profit_usdt, profit_percentage, strategy):
        """Process profit reinvestment based on configured strategies"""
        try:
            total_reinvestment = 0
            
            # Immediate reinvestment
            if (self.reinvestment_strategies['immediate_reinvest']['enabled'] and 
                profit_percentage >= self.reinvestment_strategies['immediate_reinvest']['profit_threshold']):
                
                reinvest_amount = profit_usdt * self.reinvestment_strategies['immediate_reinvest']['reinvest_percentage']
                self.capital_allocation['active_trading_capital'] += reinvest_amount
                total_reinvestment += reinvest_amount
                
                reserve_amount = profit_usdt * self.reinvestment_strategies['immediate_reinvest']['reserve_percentage']
                self.capital_allocation['reserved_capital'] += reserve_amount
                
            # Milestone reinvestment
            if self.reinvestment_strategies['milestone_reinvest']['enabled']:
                milestone_bonus = self.calculate_milestone_reinvestment_bonus()
                if milestone_bonus > 0:
                    bonus_amount = profit_usdt * milestone_bonus
                    self.capital_allocation['active_trading_capital'] += bonus_amount
                    total_reinvestment += bonus_amount
                    
            # Streak-based reinvestment
            if (self.reinvestment_strategies['streak_based_reinvest']['enabled'] and
                self.trading_state['winning_streak'] >= self.reinvestment_strategies['streak_based_reinvest']['winning_streak_threshold']):
                
                streak_bonus = profit_usdt * self.reinvestment_strategies['streak_based_reinvest']['reinvest_bonus']
                self.capital_allocation['active_trading_capital'] += streak_bonus
                total_reinvestment += streak_bonus
                
            # Update compound interest tracking
            self.capital_growth['compound_interest_earned'] += total_reinvestment
            
            if total_reinvestment > 0:
                self.logger.info(f"üîÑ Reinvested ${total_reinvestment:,.2f} for continued growth")
                
        except Exception as e:
            self.logger.error(f"Profit reinvestment processing failed: {e}")
            
    def calculate_milestone_reinvestment_bonus(self):
        """Calculate reinvestment bonus based on growth milestones"""
        try:
            initial_capital = self.capital_growth['initial_capital']
            current_capital = self.capital_growth['current_capital']
            
            if initial_capital <= 0:
                return 0
                
            growth_multiple = current_capital / initial_capital
            milestone_percentages = self.reinvestment_strategies['milestone_reinvest']['milestone_percentages']
            
            # Find the highest milestone achieved
            highest_milestone = 0
            for milestone in milestone_percentages:
                if growth_multiple >= (1 + milestone):
                    highest_milestone = milestone
                    
            # Check if this is a new milestone
            if highest_milestone > self.trading_state['last_milestone_hit']:
                self.trading_state['last_milestone_hit'] = highest_milestone
                multiplier = self.reinvestment_strategies['milestone_reinvest']['reinvest_multiplier']
                return highest_milestone * multiplier
                
            return 0
            
        except Exception as e:
            self.logger.error(f"Milestone bonus calculation failed: {e}")
            return 0
            
    def check_growth_milestones(self):
        """Check and celebrate growth milestones"""
        try:
            current_capital = self.capital_growth['current_capital']
            
            # Define milestone levels
            milestones = [1000, 5000, 10000, 25000, 50000, 100000, 250000, 500000, 1000000]
            
            for milestone in milestones:
                if (current_capital >= milestone and 
                    milestone not in self.capital_growth['growth_milestones']):
                    
                    self.capital_growth['growth_milestones'].append(milestone)
                    initial_capital = self.capital_growth['initial_capital']
                    
                    if initial_capital > 0:
                        growth_multiple = current_capital / initial_capital
                        self.logger.info(
                            f"üéâ MILESTONE ACHIEVED! ${milestone:,} "
                            f"({growth_multiple:.1f}x growth from ${initial_capital:,.2f})"
                        )
                        
                        # Store milestone achievement
                        milestone_data = {
                            'milestone': milestone,
                            'timestamp': time.time(),
                            'initial_capital': initial_capital,
                            'growth_multiple': growth_multiple,
                            'days_to_achieve': self.calculate_days_since_start(),
                            'trades_to_achieve': self.capital_growth['total_trades_count']
                        }
                        
                        self.memory_manager.store_growth_milestone(milestone_data)
                        
        except Exception as e:
            self.logger.error(f"Milestone checking failed: {e}")
            
    def calculate_days_since_start(self):
        """Calculate days since bot started trading"""
        try:
            # This would typically come from stored start date
            # For now, estimate based on trade count
            trades_count = self.capital_growth['total_trades_count']
            estimated_trades_per_day = 10  # Estimate
            return max(1, trades_count / estimated_trades_per_day)
        except:
            return 1
            
    def adjust_strategies_for_mode(self, new_mode):
        """Adjust trading strategies based on new trading mode"""
        try:
            if new_mode == 'psycho_mode':
                # Ultra-aggressive settings
                adjustments = {
                    'max_positions': 15,
                    'risk_per_trade': 0.05,
                    'leverage_range': (50, 100),
                    'profit_target_multiplier': 2.0,
                    'strategy_aggressiveness': 'maximum'
                }
            elif new_mode == 'hunter_mode':
                # Aggressive but more selective
                adjustments = {
                    'max_positions': 12,
                    'risk_per_trade': 0.04,
                    'leverage_range': (25, 50),
                    'profit_target_multiplier': 1.5,
                    'strategy_aggressiveness': 'high'
                }
            elif new_mode == 'accumulator_mode':
                # Balanced growth approach
                adjustments = {
                    'max_positions': 10,
                    'risk_per_trade': 0.03,
                    'leverage_range': (15, 30),
                    'profit_target_multiplier': 1.2,
                    'strategy_aggressiveness': 'medium'
                }
            else:  # whale_mode
                # Conservative capital preservation
                adjustments = {
                    'max_positions': 8,
                    'risk_per_trade': 0.02,
                    'leverage_range': (5, 15),
                    'profit_target_multiplier': 1.0,
                    'strategy_aggressiveness': 'low'
                }
                
            # Apply adjustments to position sizing
            self.position_sizing['max_open_positions'] = adjustments['max_positions']
            self.position_sizing['base_risk_percentage'] = adjustments['risk_per_trade']
            self.position_sizing['leverage_scaling'][new_mode] = adjustments['leverage_range']
            
            self.logger.info(f"‚öôÔ∏è Strategies adjusted for {new_mode}: {adjustments}")
            
        except Exception as e:
            self.logger.error(f"Strategy adjustment failed: {e}")
            
    def start_compounding_monitors(self):
        """Start background monitoring threads"""
        def daily_target_monitor():
            while True:
                try:
                    self.monitor_daily_progress()
                    time.sleep(3600)  # Check every hour
                except Exception as e:
                    self.logger.error(f"Daily monitor error: {e}")
                    time.sleep(1800)
                    
        def capital_allocation_optimizer():
            while True:
                try:
                    self.optimize_capital_allocation()
                    time.sleep(1800)  # Optimize every 30 minutes
                except Exception as e:
                    self.logger.error(f"Capital allocation error: {e}")
                    time.sleep(900)
                    
        def performance_tracker():
            while True:
                try:
                    self.update_performance_metrics()
                    time.sleep(900)  # Update every 15 minutes
                except Exception as e:
                    self.logger.error(f"Performance tracking error: {e}")
                    time.sleep(600)
                    
        threading.Thread(target=daily_target_monitor, daemon=True).start()
        threading.Thread(target=capital_allocation_optimizer, daemon=True).start()
        threading.Thread(target=performance_tracker, daemon=True).start()
        
    def monitor_daily_progress(self):
        """Monitor progress toward daily targets"""
        try:
            today = datetime.now().strftime('%Y-%m-%d')
            daily_target = self.capital_growth['daily_targets'].get(today)
            
            if not daily_target:
                self.calculate_daily_targets()
                return
                
            target_return = daily_target['target_return']
            achieved_return = self.trading_state['daily_profit_achieved']
            progress_percentage = achieved_return / target_return if target_return > 0 else 0
            
            # Update daily target with actual performance
            daily_target['actual_return'] = achieved_return
            daily_target['actual_profit_usdt'] = self.capital_growth['current_capital'] * achieved_return

	    # Log progress
            if progress_percentage >= 1.0:
                self.logger.info(f"üéØ Daily target ACHIEVED! {achieved_return:.1%} / {target_return:.1%}")
            elif progress_percentage >= 0.8:
                self.logger.info(f"üìà Daily progress: {achieved_return:.1%} / {target_return:.1%} ({progress_percentage:.0%})")
            elif progress_percentage < 0.5:
                self.logger.warning(f"‚ö†Ô∏è Daily progress behind: {achieved_return:.1%} / {target_return:.1%} ({progress_percentage:.0%})")
                
            # Adjust strategy aggressiveness based on progress
            if progress_percentage < 0.3:  # Way behind target
                self.trading_state['risk_adjustment_factor'] = 1.5  # Increase risk
                self.trading_state['leverage_multiplier'] = 1.3     # Increase leverage
            elif progress_percentage > 1.2:  # Way ahead of target
                self.trading_state['risk_adjustment_factor'] = 0.8  # Reduce risk
                self.trading_state['leverage_multiplier'] = 0.9     # Reduce leverage
            else:
                self.trading_state['risk_adjustment_factor'] = 1.0  # Normal risk
                self.trading_state['leverage_multiplier'] = 1.0     # Normal leverage
                
        except Exception as e:
            self.logger.error(f"Daily progress monitoring failed: {e}")
            
    def optimize_capital_allocation(self):
        """Optimize capital allocation across strategies and symbols"""
        try:
            current_capital = self.capital_growth['current_capital']
            current_mode = self.trading_state['current_mode']
            
            # Determine allocation percentages based on mode
            if current_mode == 'psycho_mode':
                active_percentage = self.compounding_config['capital_allocation_modes']['aggressive_growth']
            elif current_mode == 'hunter_mode':
                active_percentage = self.compounding_config['capital_allocation_modes']['balanced_growth']
            elif current_mode == 'accumulator_mode':
                active_percentage = self.compounding_config['capital_allocation_modes']['balanced_growth']
            else:  # whale_mode
                active_percentage = self.compounding_config['capital_allocation_modes']['conservative_growth']
                
            # Calculate allocations
            active_capital = current_capital * active_percentage
            reserved_capital = current_capital * (1 - active_percentage) * 0.8
            emergency_fund = current_capital * (1 - active_percentage) * 0.2
            
            # Update capital allocation
            self.capital_allocation.update({
                'active_trading_capital': active_capital,
                'reserved_capital': reserved_capital,
                'emergency_fund': emergency_fund
            })
            
            # Allocate capital across strategies based on performance
            self.allocate_capital_to_strategies(active_capital)
            
            # Allocate capital across symbols based on opportunity
            self.allocate_capital_to_symbols(active_capital)
            
        except Exception as e:
            self.logger.error(f"Capital allocation optimization failed: {e}")
            
    def allocate_capital_to_strategies(self, active_capital):
        """Allocate capital across different trading strategies"""
        try:
            # Get strategy performance data
            strategy_stats = self.performance_analytics.get_all_strategy_stats()
            
            if not strategy_stats:
                # Equal allocation if no performance data
                strategies = ['breakout', 'momentum', 'mean_reversion', 'liquidation_cascade', 'whale_tracking']
                allocation_per_strategy = active_capital / len(strategies)
                for strategy in strategies:
                    self.capital_allocation['strategy_allocations'][strategy] = allocation_per_strategy
                return
                
            # Calculate allocation weights based on performance
            total_score = 0
            strategy_scores = {}
            
            for strategy, stats in strategy_stats.items():
                if stats['total_trades'] >= 10:  # Minimum trades for consideration
                    # Score based on profit factor, win rate, and Sharpe ratio
                    profit_factor = stats.get('profit_factor', 1.0)
                    win_rate = stats.get('win_rate', 0.5)
                    sharpe_ratio = stats.get('sharpe_ratio', 0.0)
                    
                    # Composite score
                    score = (profit_factor * 0.4) + (win_rate * 0.3) + (max(0, sharpe_ratio) * 0.3)
                    strategy_scores[strategy] = score
                    total_score += score
                    
            # Allocate capital proportionally
            if total_score > 0:
                for strategy, score in strategy_scores.items():
                    allocation_percentage = score / total_score
                    allocation_amount = active_capital * allocation_percentage
                    self.capital_allocation['strategy_allocations'][strategy] = allocation_amount
                    
            self.logger.info(f"üí∞ Capital allocated across {len(strategy_scores)} strategies")
            
        except Exception as e:
            self.logger.error(f"Strategy capital allocation failed: {e}")
            
    def allocate_capital_to_symbols(self, active_capital):
        """Allocate capital across different symbols based on opportunity"""
        try:
            # Get opportunity scores for all symbols
            symbols = ['BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'ADAUSDT', 'SOLUSDT']
            symbol_opportunities = {}
            
            for symbol in symbols:
                # Get various opportunity indicators
                volatility = self.performance_analytics.get_symbol_volatility(symbol)
                volume = self.performance_analytics.get_symbol_volume(symbol)
                trend_strength = self.performance_analytics.get_trend_strength(symbol)
                
                if volatility and volume and trend_strength:
                    # Calculate opportunity score
                    vol_score = min(1.0, volatility / 0.05)  # Normalize to 5% daily vol
                    volume_score = min(1.0, volume / 1000000000)  # Normalize to $1B daily volume
                    trend_score = abs(trend_strength)  # Absolute trend strength
                    
                    opportunity_score = (vol_score * 0.4) + (volume_score * 0.3) + (trend_score * 0.3)
                    symbol_opportunities[symbol] = opportunity_score
                    
            # Allocate capital based on opportunity scores
            total_opportunity = sum(symbol_opportunities.values())
            
            if total_opportunity > 0:
                for symbol, opportunity in symbol_opportunities.items():
                    allocation_percentage = opportunity / total_opportunity
                    allocation_amount = active_capital * allocation_percentage
                    self.capital_allocation['symbol_allocations'][symbol] = allocation_amount
                    
            self.logger.info(f"üí∞ Capital allocated across {len(symbol_opportunities)} symbols")
            
        except Exception as e:
            self.logger.error(f"Symbol capital allocation failed: {e}")
            
    def update_performance_metrics(self):
        """Update comprehensive performance metrics"""
        try:
            current_capital = self.capital_growth['current_capital']
            initial_capital = self.capital_growth['initial_capital']
            
            if initial_capital <= 0:
                return
                
            # Calculate total return
            total_return = (current_capital - initial_capital) / initial_capital
            self.performance_metrics['total_return'] = total_return
            
            # Calculate compound growth rate
            days_trading = self.calculate_days_since_start()
            if days_trading > 0:
                compound_growth_rate = (current_capital / initial_capital) ** (1/days_trading) - 1
                self.performance_metrics['compound_growth_rate'] = compound_growth_rate
                
                # Calculate annualized return
                annualized_return = (1 + compound_growth_rate) ** 365 - 1
                self.performance_metrics['annualized_return'] = annualized_return
                
            # Calculate rolling metrics
            self.calculate_rolling_sharpe()
            self.calculate_rolling_max_drawdown()
            self.calculate_rolling_win_rate()
            self.calculate_rolling_profit_factor()
            
        except Exception as e:
            self.logger.error(f"Performance metrics update failed: {e}")
            
    def calculate_rolling_sharpe(self):
        """Calculate rolling Sharpe ratio"""
        try:
            if len(self.performance_metrics['daily_returns']) < 7:
                return
                
            daily_returns = list(self.performance_metrics['daily_returns'])
            
            if len(daily_returns) >= 30:
                recent_returns = daily_returns[-30:]
            else:
                recent_returns = daily_returns
                
            if len(recent_returns) < 2:
                return
                
            mean_return = np.mean(recent_returns)
            std_return = np.std(recent_returns)
            risk_free_rate = self.optimization_algorithms['sharpe_optimization']['risk_free_rate'] / 365
            
            if std_return > 0:
                sharpe_ratio = (mean_return - risk_free_rate) / std_return
                self.performance_metrics['rolling_sharpe'].append(sharpe_ratio)
                
        except Exception as e:
            self.logger.error(f"Rolling Sharpe calculation failed: {e}")
            
    def calculate_rolling_max_drawdown(self):
        """Calculate rolling maximum drawdown"""
        try:
            if len(self.performance_metrics['daily_returns']) < 2:
                return
                
            daily_returns = list(self.performance_metrics['daily_returns'])
            
            # Calculate cumulative returns
            cumulative_returns = []
            cumulative = 1.0
            for ret in daily_returns:
                cumulative *= (1 + ret)
                cumulative_returns.append(cumulative)
                
            # Calculate running maximum
            running_max = []
            current_max = cumulative_returns[0]
            for cum_ret in cumulative_returns:
                current_max = max(current_max, cum_ret)
                running_max.append(current_max)
                
            # Calculate drawdowns
            drawdowns = []
            for i in range(len(cumulative_returns)):
                drawdown = (cumulative_returns[i] - running_max[i]) / running_max[i]
                drawdowns.append(drawdown)
                
            # Get maximum drawdown
            max_drawdown = min(drawdowns) if drawdowns else 0
            self.performance_metrics['rolling_max_drawdown'].append(abs(max_drawdown))
            
        except Exception as e:
            self.logger.error(f"Rolling drawdown calculation failed: {e}")
            
    def calculate_rolling_win_rate(self):
        """Calculate rolling win rate"""
        try:
            if len(self.performance_metrics['daily_returns']) < 7:
                return
                
            daily_returns = list(self.performance_metrics['daily_returns'])
            recent_returns = daily_returns[-30:] if len(daily_returns) >= 30 else daily_returns
            
            winning_days = sum(1 for ret in recent_returns if ret > 0)
            total_days = len(recent_returns)
            
            win_rate = winning_days / total_days if total_days > 0 else 0
            self.performance_metrics['rolling_win_rate'].append(win_rate)
            
        except Exception as e:
            self.logger.error(f"Rolling win rate calculation failed: {e}")
            
    def calculate_rolling_profit_factor(self):
        """Calculate rolling profit factor"""
        try:
            if len(self.performance_metrics['daily_returns']) < 7:
                return
                
            daily_returns = list(self.performance_metrics['daily_returns'])
            recent_returns = daily_returns[-30:] if len(daily_returns) >= 30 else daily_returns
            
            gross_profit = sum(ret for ret in recent_returns if ret > 0)
            gross_loss = abs(sum(ret for ret in recent_returns if ret < 0))
            
            profit_factor = gross_profit / gross_loss if gross_loss > 0 else float('inf')
            self.performance_metrics['rolling_profit_factor'].append(profit_factor)
            
        except Exception as e:
            self.logger.error(f"Rolling profit factor calculation failed: {e}")
            
    def get_compounding_analytics(self):
        """Get comprehensive compounding analytics"""
        try:
            current_capital = self.capital_growth['current_capital']
            initial_capital = self.capital_growth['initial_capital']
            
            analytics = {
                'capital_growth': {
                    'initial_capital': initial_capital,
                    'current_capital': current_capital,
                    'total_return': self.performance_metrics.get('total_return', 0),
                    'compound_growth_rate': self.performance_metrics.get('compound_growth_rate', 0),
                    'annualized_return': self.performance_metrics.get('annualized_return', 0),
                    'growth_milestones': self.capital_growth['growth_milestones']
                },
                'trading_performance': {
                    'total_trades': self.capital_growth['total_trades_count'],
                    'profitable_trades': self.capital_growth['profitable_trades_count'],
                    'win_rate': self.capital_growth['profitable_trades_count'] / max(1, self.capital_growth['total_trades_count']),
                    'winning_streak': self.trading_state['winning_streak'],
                    'losing_streak': self.trading_state['losing_streak'],
                    'compound_interest_earned': self.capital_growth['compound_interest_earned']
                },
                'current_state': {
                    'trading_mode': self.trading_state['current_mode'],
                    'daily_profit_target': self.trading_state['daily_profit_target'],
                    'daily_profit_achieved': self.trading_state['daily_profit_achieved'],
                    'risk_adjustment_factor': self.trading_state['risk_adjustment_factor'],
                    'leverage_multiplier': self.trading_state['leverage_multiplier']
                },
                'capital_allocation': {
                    'active_trading_capital': self.capital_allocation['active_trading_capital'],
                    'reserved_capital': self.capital_allocation['reserved_capital'],
                    'emergency_fund': self.capital_allocation['emergency_fund'],
                    'strategy_allocations': dict(self.capital_allocation['strategy_allocations']),
                    'symbol_allocations': dict(self.capital_allocation['symbol_allocations'])
                },
                'performance_metrics': {
                    'current_sharpe': self.performance_metrics['rolling_sharpe'][-1] if self.performance_metrics['rolling_sharpe'] else 0,
                    'current_max_drawdown': self.performance_metrics['rolling_max_drawdown'][-1] if self.performance_metrics['rolling_max_drawdown'] else 0,
                    'current_win_rate': self.performance_metrics['rolling_win_rate'][-1] if self.performance_metrics['rolling_win_rate'] else 0,
                    'current_profit_factor': self.performance_metrics['rolling_profit_factor'][-1] if self.performance_metrics['rolling_profit_factor'] else 0
                }
            }
            
            return analytics
            
        except Exception as e:
            self.logger.error(f"Compounding analytics generation failed: {e}")
            return {}
            
    def get_optimal_trade_parameters(self, symbol, strategy, signal_confidence):
        """Get optimal trade parameters for maximum compounding efficiency"""
        try:
            # Calculate optimal position size
            position_info = self.calculate_optimal_position_size(symbol, strategy, signal_confidence)
            
            # Get current capital allocation for this symbol/strategy
            symbol_allocation = self.capital_allocation['symbol_allocations'].get(symbol, 0)
            strategy_allocation = self.capital_allocation['strategy_allocations'].get(strategy, 0)
            
            # Use the minimum of calculated position size and allocated capital
            max_position_from_allocation = min(symbol_allocation, strategy_allocation)
            final_position_size = min(position_info['position_size_usdt'], max_position_from_allocation)
            
            # Apply risk adjustment factor
            final_position_size *= self.trading_state['risk_adjustment_factor']
            
            # Apply leverage multiplier
            final_leverage = position_info['leverage'] * self.trading_state['leverage_multiplier']
            final_leverage = max(1, min(125, round(final_leverage)))  # Keep within Binance limits
            
            # Calculate stop loss and take profit for optimal risk/reward
            optimal_risk_reward = self.calculate_optimal_risk_reward(signal_confidence)
            
            trade_parameters = {
                'position_size_usdt': final_position_size,
                'leverage': final_leverage,
                'risk_percentage': position_info['risk_percentage'],
                'stop_loss_percentage': 0.02 / final_leverage,  # 2% of margin
                'take_profit_percentage': optimal_risk_reward * (0.02 / final_leverage),
                'trailing_stop_enabled': True,
                'scale_out_levels': self.get_scale_out_levels(signal_confidence),
                'max_position_duration_hours': self.get_max_position_duration(),
                'compound_on_profit': True,
                'reinvest_percentage': 0.80
            }
            
            return trade_parameters
            
        except Exception as e:
            self.logger.error(f"Optimal trade parameters calculation failed: {e}")
            return self.get_default_trade_parameters()
            
    def calculate_optimal_risk_reward(self, signal_confidence):
        """Calculate optimal risk/reward ratio based on confidence and mode"""
        try:
            current_mode = self.trading_state['current_mode']
            
            # Base risk/reward ratios by mode
            base_ratios = {
                'psycho_mode': 2.0,      # 1:2 risk/reward
                'hunter_mode': 2.5,      # 1:2.5 risk/reward
                'accumulator_mode': 3.0, # 1:3 risk/reward
                'whale_mode': 3.5        # 1:3.5 risk/reward
            }
            
            base_ratio = base_ratios.get(current_mode, 2.5)
            
            # Adjust based on signal confidence
            confidence_multiplier = 0.5 + (signal_confidence * 1.5)  # 0.5x to 2.0x
            optimal_ratio = base_ratio * confidence_multiplier
            
            # Adjust based on winning streak (more aggressive on wins)
            if self.trading_state['winning_streak'] >= 5:
                optimal_ratio *= 1.2
            elif self.trading_state['losing_streak'] >= 3:
                optimal_ratio *= 0.8
                
            return max(1.5, min(5.0, optimal_ratio))  # Keep between 1.5:1 and 5:1
            
        except Exception as e:
            self.logger.error(f"Optimal risk/reward calculation failed: {e}")
            return 2.5
            
    def get_scale_out_levels(self, signal_confidence):
        """Get scale-out levels for profit taking"""
        try:
            # More confident signals get more aggressive scale-out
            if signal_confidence >= 0.8:
                return [0.25, 0.50, 0.75, 1.0, 1.5]  # Scale out at 25%, 50%, 75%, 100%, 150% of target
            elif signal_confidence >= 0.6:
                return [0.33, 0.66, 1.0, 1.25]       # Scale out at 33%, 66%, 100%, 125% of target
            else:
                return [0.50, 1.0]                   # Scale out at 50%, 100% of target
                
        except Exception as e:
            self.logger.error(f"Scale-out levels calculation failed: {e}")
            return [0.50, 1.0]
            
    def get_max_position_duration(self):
        """Get maximum position duration based on trading mode"""
        try:
            current_mode = self.trading_state['current_mode']
            
            # Duration limits by mode (in hours)
            durations = {
                'psycho_mode': 4,        # 4 hours max
                'hunter_mode': 8,        # 8 hours max
                'accumulator_mode': 24,  # 24 hours max
                'whale_mode': 72         # 72 hours max
            }
            
            return durations.get(current_mode, 12)
            
        except Exception as e:
            self.logger.error(f"Max position duration calculation failed: {e}")
            return 12
            
    def get_default_trade_parameters(self):
        """Get default trade parameters as fallback"""
        return {
            'position_size_usdt': 100,
            'leverage': 10,
            'risk_percentage': 0.02,
            'stop_loss_percentage': 0.02,
            'take_profit_percentage': 0.05,
            'trailing_stop_enabled': True,
            'scale_out_levels': [0.50, 1.0],
            'max_position_duration_hours': 12,
            'compound_on_profit': True,
            'reinvest_percentage': 0.80
        }
        
    def save_compounding_state(self):
        """Save current compounding state to memory"""
        try:
            state_data = {
                'capital_growth': dict(self.capital_growth),
                'trading_state': dict(self.trading_state),
                'capital_allocation': dict(self.capital_allocation),
                'performance_metrics': {
                    'daily_returns': list(self.performance_metrics['daily_returns']),
                    'weekly_returns': list(self.performance_metrics['weekly_returns']),
                    'monthly_returns': list(self.performance_metrics['monthly_returns']),
                    'rolling_sharpe': list(self.performance_metrics['rolling_sharpe']),
                    'rolling_max_drawdown': list(self.performance_metrics['rolling_max_drawdown']),
                    'rolling_win_rate': list(self.performance_metrics['rolling_win_rate']),
                    'rolling_profit_factor': list(self.performance_metrics['rolling_profit_factor']),
                    'compound_growth_rate': self.performance_metrics['compound_growth_rate'],
                    'total_return': self.performance_metrics['total_return'],
                    'annualized_return': self.performance_metrics['annualized_return']
                },
                'timestamp': time.time()
            }
            
            self.memory_manager.save_compounding_state(state_data)
            self.logger.info("üíæ Auto-compounding state saved successfully")
            
        except Exception as e:
            self.logger.error(f"Compounding state save failed: {e}")
            
    def load_compounding_state(self):
        """Load previous compounding state from memory"""
        try:
            state_data = self.memory_manager.load_compounding_state()
            
            if state_data:
                # Restore capital growth data
                self.capital_growth.update(state_data.get('capital_growth', {}))
                
                # Restore trading state
                self.trading_state.update(state_data.get('trading_state', {}))
                
                # Restore capital allocation
                self.capital_allocation.update(state_data.get('capital_allocation', {}))
                
                # Restore performance metrics
                perf_data = state_data.get('performance_metrics', {})
                if perf_data:
                    self.performance_metrics['daily_returns'] = deque(perf_data.get('daily_returns', []), maxlen=365)
                    self.performance_metrics['weekly_returns'] = deque(perf_data.get('weekly_returns', []), maxlen=52)
                    self.performance_metrics['monthly_returns'] = deque(perf_data.get('monthly_returns', []), maxlen=12)
                    self.performance_metrics['rolling_sharpe'] = deque(perf_data.get('rolling_sharpe', []), maxlen=30)
                    self.performance_metrics['rolling_max_drawdown'] = deque(perf_data.get('rolling_max_drawdown', []), maxlen=30)
                    self.performance_metrics['rolling_win_rate'] = deque(perf_data.get('rolling_win_rate', []), maxlen=30)
                    self.performance_metrics['rolling_profit_factor'] = deque(perf_data.get('rolling_profit_factor', []), maxlen=30)
                    self.performance_metrics['compound_growth_rate'] = perf_data.get('compound_growth_rate', 0)
                    self.performance_metrics['total_return'] = perf_data.get('total_return', 0)
                    self.performance_metrics['annualized_return'] = perf_data.get('annualized_return', 0)
                
                self.logger.info("üìö Auto-compounding state loaded successfully")
                return True
            else:
                self.logger.info("üì≠ No previous compounding state found")
                return False
                
        except Exception as e:
            self.logger.error(f"Compounding state load failed: {e}")
            return False

class LearningOptimizationEngine:
    """Advanced AI learning and optimization system for continuous strategy evolution"""
    
    def __init__(self, config, binance_client, memory_manager, performance_analytics, all_strategies):
        self.config = config
        self.binance_client = binance_client
        self.memory_manager = memory_manager
        self.performance_analytics = performance_analytics
        self.all_strategies = all_strategies
        self.logger = logging.getLogger('MonsterBot.LearningOptimization')
        
        # Learning configuration
        self.learning_config = {
            'neural_network': {
                'enabled': True,
                'architecture': 'lstm_attention',
                'input_features': 150,          # Number of input features
                'hidden_layers': [256, 128, 64],
                'output_neurons': 10,           # Strategy confidence scores
                'learning_rate': 0.001,
                'batch_size': 64,
                'sequence_length': 50,          # Lookback window
                'dropout_rate': 0.3,
                'l2_regularization': 0.01
            },
            'reinforcement_learning': {
                'enabled': True,
                'algorithm': 'ppo',             # Proximal Policy Optimization
                'environment': 'trading_env',
                'action_space': 21,             # 20 strategies + hold
                'state_space': 200,             # Market state dimensions
                'reward_function': 'profit_sharpe',
                'exploration_rate': 0.15,
                'learning_episodes': 1000,
                'memory_buffer_size': 10000
            },
            'genetic_algorithm': {
                'enabled': True,
                'population_size': 50,
                'generations': 100,
                'mutation_rate': 0.05,
                'crossover_rate': 0.8,
                'elite_percentage': 0.20,
                'parameter_ranges': {
                    'risk_percentage': (0.005, 0.10),
                    'leverage': (1, 125),
                    'stop_loss': (0.01, 0.05),
                    'take_profit': (0.02, 0.15),
                    'timeframe_weight': (0.1, 2.0)
                }
            },
            'bayesian_optimization': {
                'enabled': True,
                'acquisition_function': 'ei',   # Expected Improvement
                'kernel': 'matern',
                'optimization_rounds': 200,
                'initial_random_points': 20,
                'exploration_weight': 0.1
            }
        }
        
        # Learning algorithms
        self.learning_algorithms = {
            'pattern_recognition': {
                'lookback_periods': [5, 15, 30, 60, 240],  # Minutes
                'pattern_types': ['breakout', 'reversal', 'continuation', 'exhaustion'],
                'confidence_threshold': 0.70,
                'pattern_memory': deque(maxlen=10000),
                'success_patterns': defaultdict(list),
                'failure_patterns': defaultdict(list)
            },
            'market_regime_detection': {
                'regimes': ['trending_up', 'trending_down', 'ranging', 'high_volatility', 'low_volatility'],
                'regime_indicators': ['rsi', 'macd', 'bollinger_bands', 'atr', 'volume'],
                'regime_memory': deque(maxlen=1000),
                'regime_strategy_performance': defaultdict(lambda: defaultdict(list)),
                'current_regime': 'ranging',
                'confidence': 0.5
            },
            'strategy_evolution': {
                'parameter_evolution': True,
                'weight_evolution': True,
                'logic_evolution': True,
                'performance_memory': defaultdict(lambda: deque(maxlen=500)),
                'evolution_generations': 0,
                'mutation_history': [],
                'fitness_scores': defaultdict(float)
            },
            'adaptive_parameters': {
                'parameter_bounds': {},
                'parameter_sensitivity': {},
                'parameter_correlation': {},
                'optimization_history': deque(maxlen=1000),
                'current_parameters': {},
                'best_parameters': {}
            }
        }
        
        # AI models
        self.ai_models = {
            'prediction_model': None,
            'regime_classifier': None,
            'pattern_detector': None,
            'strategy_selector': None,
            'parameter_optimizer': None,
            'risk_predictor': None
        }
        
        # Learning memory
        self.learning_memory = {
            'trade_outcomes': deque(maxlen=5000),
            'market_states': deque(maxlen=5000),
            'strategy_decisions': deque(maxlen=5000),
            'parameter_settings': deque(maxlen=5000),
            'error_cases': deque(maxlen=1000),
            'success_cases': deque(maxlen=1000),
            'edge_cases': deque(maxlen=500)
        }
        
        # Optimization targets
        self.optimization_targets = {
            'primary_metric': 'profit_factor',
            'secondary_metrics': ['sharpe_ratio', 'max_drawdown', 'win_rate'],
            'weight_primary': 0.50,
            'weight_secondary': 0.50,
            'target_values': {
                'profit_factor': 2.0,
                'sharpe_ratio': 2.5,
                'max_drawdown': 0.15,
                'win_rate': 0.65,
                'calmar_ratio': 3.0
            }
        }
        
        # Real-time learning state
        self.learning_state = {
            'learning_active': True,
            'models_trained': False,
            'optimization_cycles': 0,
            'last_optimization': 0,
            'performance_trend': 'stable',
            'learning_rate_adaptive': 0.001,
            'exploration_rate_current': 0.15,
            'model_confidence': 0.5
        }
        
        # Initialize learning system
        self.initialize_learning_system()
        
    def initialize_learning_system(self):
        """Initialize the AI learning and optimization system"""
        try:
            # Load previous learning state
            self.load_learning_state()
            
            # Initialize AI models
            self.initialize_ai_models()
            
            # Load training data
            self.load_training_data()
            
            # Start learning threads
            self.start_learning_threads()
            
            self.logger.info("üß† Learning & Optimization Engine initialized")
            self.logger.info(f"   ü§ñ Neural Network: {self.learning_config['neural_network']['enabled']}")
            self.logger.info(f"   üéØ Reinforcement Learning: {self.learning_config['reinforcement_learning']['enabled']}")
            self.logger.info(f"   üß¨ Genetic Algorithm: {self.learning_config['genetic_algorithm']['enabled']}")
            self.logger.info(f"   üìä Bayesian Optimization: {self.learning_config['bayesian_optimization']['enabled']}")
            
        except Exception as e:
            self.logger.error(f"Learning system initialization failed: {e}")
            
    def initialize_ai_models(self):
        """Initialize AI models for learning"""
        try:
            # Neural network for price prediction
            if self.learning_config['neural_network']['enabled']:
                self.ai_models['prediction_model'] = self.build_prediction_model()
                
            # Classification model for market regime detection
            self.ai_models['regime_classifier'] = self.build_regime_classifier()
            
            # Pattern detection model
            self.ai_models['pattern_detector'] = self.build_pattern_detector()
            
            # Strategy selection model
            self.ai_models['strategy_selector'] = self.build_strategy_selector()
            
            # Parameter optimization model
            self.ai_models['parameter_optimizer'] = self.build_parameter_optimizer()
            
            # Risk prediction model
            self.ai_models['risk_predictor'] = self.build_risk_predictor()
            
            self.logger.info("ü§ñ AI models initialized successfully")
            
        except Exception as e:
            self.logger.error(f"AI model initialization failed: {e}")
            
    def build_prediction_model(self):
        """Build LSTM neural network for price prediction"""
        try:
            # This would typically use TensorFlow/Keras
            # For now, we'll create a placeholder structure
            model_config = {
                'type': 'lstm_attention',
                'layers': [
                    {'type': 'lstm', 'units': 256, 'return_sequences': True},
                    {'type': 'attention', 'units': 128},
                    {'type': 'lstm', 'units': 128, 'return_sequences': False},
                    {'type': 'dense', 'units': 64, 'activation': 'relu'},
                    {'type': 'dropout', 'rate': 0.3},
                    {'type': 'dense', 'units': 32, 'activation': 'relu'},
                    {'type': 'dense', 'units': 1, 'activation': 'linear'}
                ],
                'optimizer': 'adam',
                'loss': 'mse',
                'metrics': ['mae'],
                'input_shape': (50, 150),  # 50 timesteps, 150 features
                'compiled': False,
                'trained': False
            }
            
            return model_config
            
        except Exception as e:
            self.logger.error(f"Prediction model building failed: {e}")
            return None
            
    def build_regime_classifier(self):
        """Build market regime classification model"""
        try:
            model_config = {
                'type': 'random_forest',
                'n_estimators': 100,
                'max_depth': 10,
                'min_samples_split': 5,
                'min_samples_leaf': 2,
                'features': [
                    'rsi_14', 'macd_signal', 'bb_position', 'atr_normalized',
                    'volume_ratio', 'price_momentum', 'volatility_regime'
                ],
                'classes': ['trending_up', 'trending_down', 'ranging', 'high_vol', 'low_vol'],
                'trained': False,
                'accuracy': 0.0
            }
            
            return model_config
            
        except Exception as e:
            self.logger.error(f"Regime classifier building failed: {e}")
            return None
            
    def build_pattern_detector(self):
        """Build pattern detection model"""
        try:
            model_config = {
                'type': 'cnn_lstm',
                'pattern_types': ['head_shoulders', 'double_top', 'double_bottom', 'triangle', 'flag'],
                'lookback_window': 60,
                'confidence_threshold': 0.70,
                'trained_patterns': 0,
                'detection_accuracy': 0.0
            }
            
            return model_config
            
        except Exception as e:
            self.logger.error(f"Pattern detector building failed: {e}")
            return None
            
    def build_strategy_selector(self):
        """Build strategy selection model"""
        try:
            model_config = {
                'type': 'ensemble',
                'base_models': ['xgboost', 'random_forest', 'neural_network'],
                'strategy_count': len(self.all_strategies),
                'selection_criteria': ['expected_return', 'risk_adjusted_return', 'market_fit'],
                'trained': False,
                'selection_accuracy': 0.0
            }
            
            return model_config
            
        except Exception as e:
            self.logger.error(f"Strategy selector building failed: {e}")
            return None
            
    def build_parameter_optimizer(self):
        """Build parameter optimization model"""
        try:
            model_config = {
                'type': 'gaussian_process',
                'kernel': 'matern',
                'acquisition_function': 'expected_improvement',
                'parameter_space': self.learning_config['genetic_algorithm']['parameter_ranges'],
                'optimization_history': [],
                'best_parameters': {},
                'optimization_rounds': 0
            }
            
            return model_config
            
        except Exception as e:
            self.logger.error(f"Parameter optimizer building failed: {e}")
            return None
            
    def build_risk_predictor(self):
        """Build risk prediction model"""
        try:
            model_config = {
                'type': 'gradient_boosting',
                'risk_metrics': ['var_95', 'cvar_95', 'max_drawdown', 'volatility'],
                'prediction_horizon': [1, 5, 15, 60],  # Minutes ahead
                'features': ['market_vol', 'position_size', 'leverage', 'correlation'],
                'trained': False,
                'prediction_accuracy': 0.0
            }
            
            return model_config
            
        except Exception as e:
            self.logger.error(f"Risk predictor building failed: {e}")
            return None
            
    def load_training_data(self):
        """Load historical data for model training"""
        try:
            # Load trade history
            trade_history = self.memory_manager.get_trade_history(limit=5000)
            if trade_history:
                for trade in trade_history:
                    self.learning_memory['trade_outcomes'].append(trade)
                    
            # Load market data history
            market_history = self.memory_manager.get_market_data_history(limit=5000)
            if market_history:
                for market_state in market_history:
                    self.learning_memory['market_states'].append(market_state)
                    
            # Load strategy decision history
            strategy_history = self.memory_manager.get_strategy_decisions(limit=5000)
            if strategy_history:
                for decision in strategy_history:
                    self.learning_memory['strategy_decisions'].append(decision)
                    
            self.logger.info(f"üìö Training data loaded: {len(self.learning_memory['trade_outcomes'])} trades")
            
        except Exception as e:
            self.logger.error(f"Training data loading failed: {e}")
            
    def start_learning_threads(self):
        """Start background learning threads"""
        def continuous_learning():
            while True:
                try:
                    if self.learning_state['learning_active']:
                        self.perform_continuous_learning()
                    time.sleep(300)  # Learn every 5 minutes
                except Exception as e:
                    self.logger.error(f"Continuous learning error: {e}")
                    time.sleep(600)
                    
        def pattern_recognition():
            while True:
                try:
                    self.update_pattern_recognition()
                    time.sleep(180)  # Update every 3 minutes
                except Exception as e:
                    self.logger.error(f"Pattern recognition error: {e}")
                    time.sleep(300)
                    
        def regime_detection():
            while True:
                try:
                    self.detect_market_regime()
                    time.sleep(60)  # Update every minute
                except Exception as e:
                    self.logger.error(f"Regime detection error: {e}")
                    time.sleep(120)
                    
        def strategy_optimization():
            while True:
                try:
                    self.optimize_strategies()
                    time.sleep(1800)  # Optimize every 30 minutes
                except Exception as e:
                    self.logger.error(f"Strategy optimization error: {e}")
                    time.sleep(3600)
                    
        threading.Thread(target=continuous_learning, daemon=True).start()
        threading.Thread(target=pattern_recognition, daemon=True).start()
        threading.Thread(target=regime_detection, daemon=True).start()
        threading.Thread(target=strategy_optimization, daemon=True).start()
        
    def perform_continuous_learning(self):
        """Perform continuous learning from recent data"""
        try:
            # Check if we have enough new data
            recent_trades = self.get_recent_trade_data()
            if len(recent_trades) < 10:
                return
                
            # Update models with new data
            self.update_prediction_model(recent_trades)
            self.update_regime_classifier(recent_trades)
            self.update_strategy_selector(recent_trades)
            
            # Analyze recent performance
            self.analyze_recent_performance(recent_trades)
            
            # Adjust learning parameters
            self.adjust_learning_parameters()
            
            self.learning_state['optimization_cycles'] += 1
            self.learning_state['last_optimization'] = time.time()
            
        except Exception as e:
            self.logger.error(f"Continuous learning failed: {e}")
            
    def get_recent_trade_data(self):
        """Get recent trade data for learning"""
        try:
            # Get trades from last hour
            cutoff_time = time.time() - 3600
            recent_trades = [
                trade for trade in self.learning_memory['trade_outcomes']
                if trade.get('timestamp', 0) > cutoff_time
            ]
            
            return recent_trades
            
        except Exception as e:
            self.logger.error(f"Recent trade data retrieval failed: {e}")
            return []
            
    def update_prediction_model(self, recent_trades):
        """Update price prediction model with recent data"""
        try:
            if not self.ai_models['prediction_model']:
                return
                
            # Prepare training data
            features = []
            targets = []
            
            for trade in recent_trades:
                if 'market_features' in trade and 'outcome' in trade:
                    features.append(trade['market_features'])
                    targets.append(trade['outcome']['profit_percentage'])
                    
            if len(features) < 5:
                return
                
            # Simulate model training (in real implementation, would use actual ML library)
            training_score = np.random.uniform(0.6, 0.9)
            
            # Update model metadata
            model = self.ai_models['prediction_model']
            model['last_training'] = time.time()
            model['training_samples'] = len(features)
            model['validation_score'] = training_score
            model['trained'] = True
            
            self.logger.info(f"ü§ñ Prediction model updated: {len(features)} samples, score: {training_score:.3f}")
            
        except Exception as e:
            self.logger.error(f"Prediction model update failed: {e}")
            
    def update_regime_classifier(self, recent_trades):
        """Update market regime classifier"""
        try:
            if not self.ai_models['regime_classifier']:
                return
                
            # Analyze market conditions from recent trades
            regimes = []
            for trade in recent_trades:
                if 'market_regime' in trade:
                    regimes.append(trade['market_regime'])
                    
            if not regimes:
                return
                
            # Update regime detection accuracy
            regime_consistency = len(set(regimes)) / len(regimes) if regimes else 0
            self.ai_models['regime_classifier']['accuracy'] = 1 - regime_consistency
            
            # Update current regime
            most_common_regime = max(set(regimes), key=regimes.count) if regimes else 'ranging'
            self.learning_algorithms['market_regime_detection']['current_regime'] = most_common_regime
            
            self.logger.info(f"üìä Market regime updated: {most_common_regime}")
            
        except Exception as e:
            self.logger.error(f"Regime classifier update failed: {e}")
            
    def update_strategy_selector(self, recent_trades):
        """Update strategy selection model"""
        try:
            if not self.ai_models['strategy_selector']:
                return
                
            # Analyze strategy performance
            strategy_performance = defaultdict(list)
            
            for trade in recent_trades:
                strategy = trade.get('strategy')
                profit = trade.get('outcome', {}).get('profit_percentage', 0)
                
                if strategy:
                    strategy_performance[strategy].append(profit)
                    
            # Calculate strategy scores
            strategy_scores = {}
            for strategy, profits in strategy_performance.items():
                if profits:
                    avg_profit = np.mean(profits)
                    win_rate = sum(1 for p in profits if p > 0) / len(profits)
                    score = avg_profit * win_rate
                    strategy_scores[strategy] = score
                    
            # Update strategy selection accuracy
            if strategy_scores:
                best_strategy = max(strategy_scores, key=strategy_scores.get)
                self.ai_models['strategy_selector']['best_strategy'] = best_strategy
                self.ai_models['strategy_selector']['strategy_scores'] = strategy_scores
                
                self.logger.info(f"üéØ Best strategy updated: {best_strategy} (score: {strategy_scores[best_strategy]:.4f})")
                
        except Exception as e:
            self.logger.error(f"Strategy selector update failed: {e}")
            
    def analyze_recent_performance(self, recent_trades):
        """Analyze recent performance and identify improvement areas"""
        try:
            if not recent_trades:
                return
                
            # Calculate performance metrics
            profits = [trade.get('outcome', {}).get('profit_percentage', 0) for trade in recent_trades]
            total_profit = sum(profits)
            win_rate = sum(1 for p in profits if p > 0) / len(profits)
            avg_profit = np.mean(profits)
            
            # Identify performance trend
            if avg_profit > 0.02:  # 2%+ average profit
                trend = 'improving'
            elif avg_profit < -0.01:  # -1% average loss
                trend = 'declining'
            else:
                trend = 'stable'
                
            self.learning_state['performance_trend'] = trend
            
            # Identify common failure patterns
            losing_trades = [trade for trade in recent_trades if trade.get('outcome', {}).get('profit_percentage', 0) < 0]
            
            if losing_trades:
                self.analyze_failure_patterns(losing_trades)
                
            # Identify success patterns
            winning_trades = [trade for trade in recent_trades if trade.get('outcome', {}).get('profit_percentage', 0) > 0]
            
            if winning_trades:
                self.analyze_success_patterns(winning_trades)
                
            self.logger.info(f"üìà Performance analysis: {trend} trend, {win_rate:.1%} win rate, {avg_profit:.2%} avg profit")
            
        except Exception as e:
            self.logger.error(f"Performance analysis failed: {e}")
            
    def analyze_failure_patterns(self, losing_trades):
        """Analyze patterns in losing trades"""
        try:
            failure_patterns = {
                'common_strategies': defaultdict(int),
                'common_timeframes': defaultdict(int),
                'common_market_conditions': defaultdict(int),
                'common_errors': defaultdict(int)
            }
            
            for trade in losing_trades:
                strategy = trade.get('strategy')
                timeframe = trade.get('timeframe')
                market_condition = trade.get('market_regime')
                error_type = trade.get('error_type', 'execution_loss')
                
                if strategy:
                    failure_patterns['common_strategies'][strategy] += 1
                if timeframe:
                    failure_patterns['common_timeframes'][timeframe] += 1
                if market_condition:
                    failure_patterns['common_market_conditions'][market_condition] += 1
                    
                failure_patterns['common_errors'][error_type] += 1
                
            # Store failure patterns for learning
            self.learning_algorithms['pattern_recognition']['failure_patterns'].update(failure_patterns)
            
            # Identify most problematic elements
            worst_strategy = max(failure_patterns['common_strategies'], key=failure_patterns['common_strategies'].get) if failure_patterns['common_strategies'] else None
            worst_condition = max(failure_patterns['common_market_conditions'], key=failure_patterns['common_market_conditions'].get) if failure_patterns['common_market_conditions'] else None
            
            if worst_strategy:
                self.logger.warning(f"‚ö†Ô∏è Problematic strategy identified: {worst_strategy}")
            if worst_condition:
                self.logger.warning(f"‚ö†Ô∏è Problematic market condition: {worst_condition}")
                
        except Exception as e:
            self.logger.error(f"Failure pattern analysis failed: {e}")
            
    def analyze_success_patterns(self, winning_trades):
        """Analyze patterns in winning trades"""
        try:
            success_patterns = {
                'best_strategies': defaultdict(list),
                'best_timeframes': defaultdict(list),
                'best_market_conditions': defaultdict(list),
                'best_parameters': defaultdict(list)
            }
            
            for trade in winning_trades:
                strategy = trade.get('strategy')
                timeframe = trade.get('timeframe')
                market_condition = trade.get('market_regime')
                profit = trade.get('outcome', {}).get('profit_percentage', 0)
                
                if strategy:
                    success_patterns['best_strategies'][strategy].append(profit)
                if timeframe:
                    success_patterns['best_timeframes'][timeframe].append(profit)
                if market_condition:
                    success_patterns['best_market_conditions'][market_condition].append(profit)
                    
            # Store success patterns for learning
            self.learning_algorithms['pattern_recognition']['success_patterns'].update(success_patterns)
            
            # Identify best performing elements
            best_strategy = None
            best_strategy_profit = 0
            
            for strategy, profits in success_patterns['best_strategies'].items():
                avg_profit = np.mean(profits)
                if avg_profit > best_strategy_profit:
                    best_strategy_profit = avg_profit
                    best_strategy = strategy
                    
            if best_strategy:
                self.logger.info(f"‚úÖ Best performing strategy: {best_strategy} ({best_strategy_profit:.2%} avg)")
                
        except Exception as e:
            self.logger.error(f"Success pattern analysis failed: {e}")
            
    def adjust_learning_parameters(self):
        """Adjust learning parameters based on performance"""
        try:
            performance_trend = self.learning_state['performance_trend']
            
            if performance_trend == 'improving':
                # Performance is good, reduce exploration
                self.learning_state['exploration_rate_current'] *= 0.95
                self.learning_state['learning_rate_adaptive'] *= 0.98
                
            elif performance_trend == 'declining':
                # Performance is bad, increase exploration
                self.learning_state['exploration_rate_current'] *= 1.05
                self.learning_state['learning_rate_adaptive'] *= 1.02
                
            # Keep within bounds
            self.learning_state['exploration_rate_current'] = max(0.05, min(0.30, self.learning_state['exploration_rate_current']))
            self.learning_state['learning_rate_adaptive'] = max(0.0001, min(0.01, self.learning_state['learning_rate_adaptive']))
            
        except Exception as e:
            self.logger.error(f"Learning parameter adjustment failed: {e}")
            
    def update_pattern_recognition(self):
        """Update pattern recognition system"""
        try:
            # Get recent market data for pattern analysis
            recent_market_data = self.get_recent_market_data()
            
            if not recent_market_data:
                return
                
            # Detect patterns in recent data
            detected_patterns = self.detect_market_patterns(recent_market_data)
            
            # Update pattern memory
            for pattern in detected_patterns:
                self.learning_algorithms['pattern_recognition']['pattern_memory'].append(pattern)
                
            # Validate pattern effectiveness
            self.validate_pattern_effectiveness()
            
            if detected_patterns:
                self.logger.info(f"üîç Detected {len(detected_patterns)} market patterns")
                
        except Exception as e:
            self.logger.error(f"Pattern recognition update failed: {e}")
            
    def get_recent_market_data(self):
        """Get recent market data for analysis"""
        try:
            # This would typically fetch from binance client
            # For now, simulate recent market data
            symbols = ['BTCUSDT', 'ETHUSDT', 'BNBUSDT']
            market_data = {}
            
            for symbol in symbols:
                # Simulate getting recent price data
                market_data[symbol] = {
                    'prices': np.random.normal(50000, 1000, 100).tolist(),  # Simulated prices
                    'volumes': np.random.normal(1000000, 100000, 100).tolist(),  # Simulated volumes
                    'timestamp': time.time()
                }
                
            return market_data
            
        except Exception as e:
            self.logger.error(f"Recent market data retrieval failed: {e}")
            return {}
            
    def detect_market_patterns(self, market_data):
        """Detect patterns in market data"""
        try:
            detected_patterns = []
            
            for symbol, data in market_data.items():
                prices = data['prices']
                volumes = data['volumes']
                
                if len(prices) < 20:
                    continue
                    
                # Detect breakout patterns
                breakout_pattern = self.detect_breakout_pattern(prices, volumes)
                if breakout_pattern:
                    breakout_pattern['symbol'] = symbol
                    breakout_pattern['type'] = 'breakout'
                    detected_patterns.append(breakout_pattern)
                    
                # Detect reversal patterns
                reversal_pattern = self.detect_reversal_pattern(prices, volumes)
                if reversal_pattern:
                    reversal_pattern['symbol'] = symbol
                    reversal_pattern['type'] = 'reversal'
                    detected_patterns.append(reversal_pattern)
                    
                # Detect continuation patterns
                continuation_pattern = self.detect_continuation_pattern(prices, volumes)
                if continuation_pattern:
                    continuation_pattern['symbol'] = symbol
                    continuation_pattern['type'] = 'continuation'
                    detected_patterns.append(continuation_pattern)
                    
            return detected_patterns
            
        except Exception as e:
            self.logger.error(f"Pattern detection failed: {e}")
            return []
            
    def detect_breakout_pattern(self, prices, volumes):
        """Detect breakout patterns"""
        try:
            if len(prices) < 20:
                return None
                
            recent_prices = prices[-20:]
            recent_volumes = volumes[-20:]
            
            # Calculate resistance/support levels
            resistance = max(recent_prices[:-5])
            support = min(recent_prices[:-5])
            range_size = resistance - support
            
            current_price = recent_prices[-1]
            current_volume = recent_volumes[-1]
            avg_volume = np.mean(recent_volumes[:-1])
            
	    # Check for breakout conditions
            if (current_price > resistance + (range_size * 0.02) and  # 2% above resistance
                current_volume > avg_volume * 1.5):  # 50% higher volume
                
                return {
                    'confidence': 0.75,
                    'direction': 'bullish',
                    'resistance_level': resistance,
                    'breakout_price': current_price,
                    'volume_confirmation': True,
                    'timestamp': time.time()
                }
                
            elif (current_price < support - (range_size * 0.02) and  # 2% below support
                  current_volume > avg_volume * 1.5):  # 50% higher volume
                  
                return {
                    'confidence': 0.75,
                    'direction': 'bearish',
                    'support_level': support,
                    'breakout_price': current_price,
                    'volume_confirmation': True,
                    'timestamp': time.time()
                }
                
            return None
            
        except Exception as e:
            self.logger.error(f"Breakout pattern detection failed: {e}")
            return None
            
    def detect_reversal_pattern(self, prices, volumes):
        """Detect reversal patterns"""
        try:
            if len(prices) < 30:
                return None
                
            recent_prices = prices[-30:]
            recent_volumes = volumes[-30:]
            
            # Look for double top/bottom patterns
            highs = []
            lows = []
            
            for i in range(2, len(recent_prices) - 2):
                if (recent_prices[i] > recent_prices[i-1] and 
                    recent_prices[i] > recent_prices[i+1] and
                    recent_prices[i] > recent_prices[i-2] and
                    recent_prices[i] > recent_prices[i+2]):
                    highs.append((i, recent_prices[i]))
                    
                elif (recent_prices[i] < recent_prices[i-1] and 
                      recent_prices[i] < recent_prices[i+1] and
                      recent_prices[i] < recent_prices[i-2] and
                      recent_prices[i] < recent_prices[i+2]):
                    lows.append((i, recent_prices[i]))
                    
            # Check for double top
            if len(highs) >= 2:
                last_two_highs = highs[-2:]
                high1_price = last_two_highs[0][1]
                high2_price = last_two_highs[1][1]
                
                if abs(high1_price - high2_price) / high1_price < 0.02:  # Within 2%
                    return {
                        'confidence': 0.70,
                        'pattern': 'double_top',
                        'direction': 'bearish',
                        'high1': high1_price,
                        'high2': high2_price,
                        'timestamp': time.time()
                    }
                    
            # Check for double bottom
            if len(lows) >= 2:
                last_two_lows = lows[-2:]
                low1_price = last_two_lows[0][1]
                low2_price = last_two_lows[1][1]
                
                if abs(low1_price - low2_price) / low1_price < 0.02:  # Within 2%
                    return {
                        'confidence': 0.70,
                        'pattern': 'double_bottom',
                        'direction': 'bullish',
                        'low1': low1_price,
                        'low2': low2_price,
                        'timestamp': time.time()
                    }
                    
            return None
            
        except Exception as e:
            self.logger.error(f"Reversal pattern detection failed: {e}")
            return None
            
    def detect_continuation_pattern(self, prices, volumes):
        """Detect continuation patterns (flags, pennants)"""
        try:
            if len(prices) < 25:
                return None
                
            recent_prices = prices[-25:]
            
            # Detect flag pattern
            # Look for strong move followed by consolidation
            
            # Check for strong initial move (first 10 candles)
            initial_move = recent_prices[9] - recent_prices[0]
            move_percentage = abs(initial_move) / recent_prices[0]
            
            if move_percentage < 0.03:  # Need at least 3% move
                return None
                
            # Check for consolidation (next 10-15 candles)
            consolidation_prices = recent_prices[10:20]
            consolidation_range = max(consolidation_prices) - min(consolidation_prices)
            consolidation_percentage = consolidation_range / np.mean(consolidation_prices)
            
            if consolidation_percentage < 0.015:  # Tight consolidation (1.5% range)
                direction = 'bullish' if initial_move > 0 else 'bearish'
                
                return {
                    'confidence': 0.65,
                    'pattern': 'flag',
                    'direction': direction,
                    'initial_move': move_percentage,
                    'consolidation_range': consolidation_percentage,
                    'timestamp': time.time()
                }
                
            return None
            
        except Exception as e:
            self.logger.error(f"Continuation pattern detection failed: {e}")
            return None
            
    def validate_pattern_effectiveness(self):
        """Validate effectiveness of detected patterns"""
        try:
            pattern_memory = self.learning_algorithms['pattern_recognition']['pattern_memory']
            
            if len(pattern_memory) < 50:
                return
                
            # Group patterns by type
            pattern_groups = defaultdict(list)
            for pattern in pattern_memory:
                pattern_type = pattern.get('type', 'unknown')
                pattern_groups[pattern_type].append(pattern)
                
            # Calculate success rates for each pattern type
            pattern_success_rates = {}
            
            for pattern_type, patterns in pattern_groups.items():
                if len(patterns) < 10:  # Need at least 10 samples
                    continue
                    
                # Simulate success rate calculation
                # In real implementation, would track actual trade outcomes
                success_count = 0
                total_count = len(patterns)
                
                for pattern in patterns:
                    confidence = pattern.get('confidence', 0.5)
                    # Higher confidence patterns are more likely to succeed
                    success_probability = confidence * 0.8 + 0.2  # 20% base + 80% * confidence
                    if np.random.random() < success_probability:
                        success_count += 1
                        
                success_rate = success_count / total_count
                pattern_success_rates[pattern_type] = success_rate
                
            # Update pattern confidence thresholds
            for pattern_type, success_rate in pattern_success_rates.items():
                if success_rate > 0.70:
                    # Good pattern - lower threshold to catch more
                    self.learning_algorithms['pattern_recognition']['confidence_threshold'] *= 0.95
                elif success_rate < 0.50:
                    # Poor pattern - raise threshold to be more selective
                    self.learning_algorithms['pattern_recognition']['confidence_threshold'] *= 1.05
                    
            # Keep threshold within reasonable bounds
            threshold = self.learning_algorithms['pattern_recognition']['confidence_threshold']
            self.learning_algorithms['pattern_recognition']['confidence_threshold'] = max(0.50, min(0.90, threshold))
            
            self.logger.info(f"üîç Pattern validation: {len(pattern_success_rates)} types, threshold: {threshold:.2f}")
            
        except Exception as e:
            self.logger.error(f"Pattern effectiveness validation failed: {e}")
            
    def detect_market_regime(self):
        """Detect current market regime"""
        try:
            # Get recent market indicators
            market_indicators = self.get_market_regime_indicators()
            
            if not market_indicators:
                return
                
            # Analyze indicators to determine regime
            regime_scores = {
                'trending_up': 0,
                'trending_down': 0,
                'ranging': 0,
                'high_volatility': 0,
                'low_volatility': 0
            }
            
            # RSI analysis
            rsi = market_indicators.get('rsi', 50)
            if rsi > 70:
                regime_scores['trending_up'] += 2
            elif rsi < 30:
                regime_scores['trending_down'] += 2
            else:
                regime_scores['ranging'] += 1
                
            # MACD analysis
            macd_signal = market_indicators.get('macd_signal', 0)
            if macd_signal > 0:
                regime_scores['trending_up'] += 1
            elif macd_signal < 0:
                regime_scores['trending_down'] += 1
                
            # Bollinger Bands analysis
            bb_position = market_indicators.get('bb_position', 0.5)
            if bb_position > 0.8:
                regime_scores['trending_up'] += 1
                regime_scores['high_volatility'] += 1
            elif bb_position < 0.2:
                regime_scores['trending_down'] += 1
                regime_scores['high_volatility'] += 1
            else:
                regime_scores['ranging'] += 1
                
            # ATR analysis for volatility
            atr_normalized = market_indicators.get('atr_normalized', 0.02)
            if atr_normalized > 0.04:
                regime_scores['high_volatility'] += 2
            elif atr_normalized < 0.015:
                regime_scores['low_volatility'] += 2
                
            # Volume analysis
            volume_ratio = market_indicators.get('volume_ratio', 1.0)
            if volume_ratio > 1.5:
                regime_scores['high_volatility'] += 1
            elif volume_ratio < 0.7:
                regime_scores['low_volatility'] += 1
                
            # Determine dominant regime
            dominant_regime = max(regime_scores, key=regime_scores.get)
            confidence = regime_scores[dominant_regime] / sum(regime_scores.values())
            
            # Update regime detection
            regime_detection = self.learning_algorithms['market_regime_detection']
            regime_detection['current_regime'] = dominant_regime
            regime_detection['confidence'] = confidence
            
            # Store regime in memory
            regime_data = {
                'regime': dominant_regime,
                'confidence': confidence,
                'scores': regime_scores,
                'indicators': market_indicators,
                'timestamp': time.time()
            }
            
            regime_detection['regime_memory'].append(regime_data)
            
            self.logger.info(f"üìä Market regime: {dominant_regime} (confidence: {confidence:.2f})")
            
        except Exception as e:
            self.logger.error(f"Market regime detection failed: {e}")
            
    def get_market_regime_indicators(self):
        """Get market indicators for regime detection"""
        try:
            # This would typically calculate from real market data
            # For now, simulate market indicators
            indicators = {
                'rsi': np.random.uniform(20, 80),
                'macd_signal': np.random.uniform(-0.1, 0.1),
                'bb_position': np.random.uniform(0, 1),
                'atr_normalized': np.random.uniform(0.01, 0.06),
                'volume_ratio': np.random.uniform(0.5, 2.0),
                'price_momentum': np.random.uniform(-0.05, 0.05)
            }
            
            return indicators
            
        except Exception as e:
            self.logger.error(f"Market regime indicators calculation failed: {e}")
            return {}
            
    def optimize_strategies(self):
        """Optimize trading strategies using multiple algorithms"""
        try:
            # Get strategy performance data
            strategy_performance = self.get_strategy_performance_data()
            
            if not strategy_performance:
                return
                
            # Apply different optimization algorithms
            if self.learning_config['genetic_algorithm']['enabled']:
                self.genetic_algorithm_optimization(strategy_performance)
                
            if self.learning_config['bayesian_optimization']['enabled']:
                self.bayesian_optimization(strategy_performance)
                
            # Update strategy parameters
            self.update_strategy_parameters()
            
            # Evolve strategy weights
            self.evolve_strategy_weights(strategy_performance)
            
            self.logger.info("üß¨ Strategy optimization completed")
            
        except Exception as e:
            self.logger.error(f"Strategy optimization failed: {e}")
            
    def get_strategy_performance_data(self):
        """Get performance data for all strategies"""
        try:
            strategy_data = {}
            
            for strategy_name in self.all_strategies.keys():
                stats = self.performance_analytics.get_strategy_stats(strategy_name)
                
                if stats and stats['total_trades'] >= 10:
                    strategy_data[strategy_name] = {
                        'profit_factor': stats.get('profit_factor', 1.0),
                        'win_rate': stats.get('win_rate', 0.5),
                        'avg_profit': stats.get('avg_win_percentage', 0.02),
                        'avg_loss': stats.get('avg_loss_percentage', -0.01),
                        'max_drawdown': stats.get('max_drawdown', 0.1),
                        'sharpe_ratio': stats.get('sharpe_ratio', 0.0),
                        'total_trades': stats['total_trades']
                    }
                    
            return strategy_data
            
        except Exception as e:
            self.logger.error(f"Strategy performance data retrieval failed: {e}")
            return {}
            
    def genetic_algorithm_optimization(self, strategy_performance):
        """Apply genetic algorithm optimization"""
        try:
            ga_config = self.learning_config['genetic_algorithm']
            
            # Create initial population
            population = self.create_initial_population(ga_config['population_size'])
            
            # Run genetic algorithm
            for generation in range(min(10, ga_config['generations'])):  # Limit generations for real-time
                # Evaluate fitness
                fitness_scores = self.evaluate_population_fitness(population, strategy_performance)
                
                # Select elite
                elite_count = int(ga_config['population_size'] * ga_config['elite_percentage'])
                elite_indices = np.argsort(fitness_scores)[-elite_count:]
                elite_population = [population[i] for i in elite_indices]
                
                # Create new population
                new_population = elite_population.copy()
                
                while len(new_population) < ga_config['population_size']:
                    # Selection
                    parent1 = self.tournament_selection(population, fitness_scores)
                    parent2 = self.tournament_selection(population, fitness_scores)
                    
                    # Crossover
                    if np.random.random() < ga_config['crossover_rate']:
                        child1, child2 = self.crossover(parent1, parent2)
                    else:
                        child1, child2 = parent1.copy(), parent2.copy()
                        
                    # Mutation
                    if np.random.random() < ga_config['mutation_rate']:
                        child1 = self.mutate(child1, ga_config['parameter_ranges'])
                    if np.random.random() < ga_config['mutation_rate']:
                        child2 = self.mutate(child2, ga_config['parameter_ranges'])
                        
                    new_population.extend([child1, child2])
                    
                population = new_population[:ga_config['population_size']]
                
            # Store best parameters
            final_fitness = self.evaluate_population_fitness(population, strategy_performance)
            best_individual = population[np.argmax(final_fitness)]
            
            self.learning_algorithms['adaptive_parameters']['best_parameters'] = best_individual
            
            self.logger.info(f"üß¨ Genetic algorithm: {generation + 1} generations, best fitness: {max(final_fitness):.4f}")
            
        except Exception as e:
            self.logger.error(f"Genetic algorithm optimization failed: {e}")
            
    def create_initial_population(self, population_size):
        """Create initial population for genetic algorithm"""
        try:
            population = []
            param_ranges = self.learning_config['genetic_algorithm']['parameter_ranges']
            
            for _ in range(population_size):
                individual = {}
                for param, (min_val, max_val) in param_ranges.items():
                    individual[param] = np.random.uniform(min_val, max_val)
                population.append(individual)
                
            return population
            
        except Exception as e:
            self.logger.error(f"Initial population creation failed: {e}")
            return []
            
    def evaluate_population_fitness(self, population, strategy_performance):
        """Evaluate fitness of population"""
        try:
            fitness_scores = []
            
            for individual in population:
                # Calculate fitness based on strategy performance with these parameters
                fitness = 0
                
                for strategy_name, performance in strategy_performance.items():
                    # Simulate how these parameters would affect strategy performance
                    risk_adj = individual.get('risk_percentage', 0.02) / 0.02
                    leverage_adj = individual.get('leverage', 10) / 10
                    
                    # Adjust performance metrics
                    adjusted_profit_factor = performance['profit_factor'] * (1 + (risk_adj - 1) * 0.5)
                    adjusted_sharpe = performance['sharpe_ratio'] * (1 + (leverage_adj - 1) * 0.3)
                    
                    strategy_fitness = (adjusted_profit_factor * 0.4 + 
                                      performance['win_rate'] * 0.3 + 
                                      max(0, adjusted_sharpe) * 0.3)
                    
                    fitness += strategy_fitness
                    
                fitness_scores.append(fitness / len(strategy_performance))
                
            return fitness_scores
            
        except Exception as e:
            self.logger.error(f"Population fitness evaluation failed: {e}")
            return [0] * len(population)
            
    def tournament_selection(self, population, fitness_scores, tournament_size=3):
        """Tournament selection for genetic algorithm"""
        try:
            tournament_indices = np.random.choice(len(population), tournament_size, replace=False)
            tournament_fitness = [fitness_scores[i] for i in tournament_indices]
            winner_index = tournament_indices[np.argmax(tournament_fitness)]
            
            return population[winner_index].copy()
            
        except Exception as e:
            self.logger.error(f"Tournament selection failed: {e}")
            return population[0].copy() if population else {}
            
    def crossover(self, parent1, parent2):
        """Crossover operation for genetic algorithm"""
        try:
            child1 = {}
            child2 = {}
            
            for param in parent1.keys():
                if np.random.random() < 0.5:
                    child1[param] = parent1[param]
                    child2[param] = parent2[param]
                else:
                    child1[param] = parent2[param]
                    child2[param] = parent1[param]
                    
            return child1, child2
            
        except Exception as e:
            self.logger.error(f"Crossover operation failed: {e}")
            return parent1.copy(), parent2.copy()
            
    def mutate(self, individual, parameter_ranges):
        """Mutation operation for genetic algorithm"""
        try:
            mutated = individual.copy()
            
            for param, value in individual.items():
                if param in parameter_ranges:
                    min_val, max_val = parameter_ranges[param]
                    
                    # Gaussian mutation
                    mutation_strength = (max_val - min_val) * 0.1
                    mutation = np.random.normal(0, mutation_strength)
                    
                    new_value = value + mutation
                    new_value = max(min_val, min(max_val, new_value))
                    
                    mutated[param] = new_value
                    
            return mutated
            
        except Exception as e:
            self.logger.error(f"Mutation operation failed: {e}")
            return individual.copy()
            
    def bayesian_optimization(self, strategy_performance):
        """Apply Bayesian optimization"""
        try:
            bo_config = self.learning_config['bayesian_optimization']
            
            # Initialize optimization history if empty
            optimization_history = self.learning_algorithms['adaptive_parameters']['optimization_history']
            
            if len(optimization_history) < bo_config['initial_random_points']:
                # Generate random points for initial exploration
                param_ranges = self.learning_config['genetic_algorithm']['parameter_ranges']
                
                for _ in range(min(5, bo_config['initial_random_points'] - len(optimization_history))):
                    random_params = {}
                    for param, (min_val, max_val) in param_ranges.items():
                        random_params[param] = np.random.uniform(min_val, max_val)
                        
                    # Evaluate performance with these parameters
                    performance_score = self.evaluate_parameter_performance(random_params, strategy_performance)
                    
                    optimization_history.append({
                        'parameters': random_params,
                        'performance': performance_score,
                        'timestamp': time.time()
                    })
                    
            # Apply Bayesian optimization (simplified)
            if len(optimization_history) >= bo_config['initial_random_points']:
                # Find best parameters so far
                best_result = max(optimization_history, key=lambda x: x['performance'])
                best_params = best_result['parameters']
                
                # Update current parameters with best found
                self.learning_algorithms['adaptive_parameters']['current_parameters'] = best_params
                
                self.logger.info(f"üìä Bayesian optimization: best performance {best_result['performance']:.4f}")
                
        except Exception as e:
            self.logger.error(f"Bayesian optimization failed: {e}")
            
    def evaluate_parameter_performance(self, parameters, strategy_performance):
        """Evaluate performance of parameter set"""
        try:
            total_score = 0
            
            for strategy_name, performance in strategy_performance.items():
                # Calculate how these parameters would affect this strategy
                risk_factor = parameters.get('risk_percentage', 0.02) / 0.02
                leverage_factor = parameters.get('leverage', 10) / 10
                
                # Adjust performance based on parameters
                adjusted_profit_factor = performance['profit_factor'] * risk_factor
                adjusted_win_rate = performance['win_rate'] * (1 + (leverage_factor - 1) * 0.1)
                
                strategy_score = (adjusted_profit_factor * 0.6 + adjusted_win_rate * 0.4)
                total_score += strategy_score
                
            return total_score / len(strategy_performance)
            
        except Exception as e:
            self.logger.error(f"Parameter performance evaluation failed: {e}")
            return 0
            
    def update_strategy_parameters(self):
        """Update strategy parameters based on optimization results"""
        try:
            best_params = self.learning_algorithms['adaptive_parameters'].get('best_parameters')
            current_params = self.learning_algorithms['adaptive_parameters'].get('current_parameters')
            
            # Use best parameters if available, otherwise current
            optimal_params = best_params or current_params
            
            if not optimal_params:
                return
                
            # Update each strategy with optimized parameters
            for strategy_name, strategy_obj in self.all_strategies.items():
                try:
                    # Update common parameters
                    if hasattr(strategy_obj, 'update_parameters'):
                        strategy_obj.update_parameters(optimal_params)
                    elif hasattr(strategy_obj, 'config'):
                        # Update strategy config
                        for param, value in optimal_params.items():
                            if param in strategy_obj.config:
                                strategy_obj.config[param] = value
                                
                except Exception as e:
                    self.logger.error(f"Parameter update failed for {strategy_name}: {e}")
                    
            self.logger.info(f"‚öôÔ∏è Strategy parameters updated with optimized values")
            
        except Exception as e:
            self.logger.error(f"Strategy parameter update failed: {e}")
            
    def evolve_strategy_weights(self, strategy_performance):
        """Evolve strategy weights based on performance"""
        try:
            # Calculate new weights based on recent performance
            strategy_weights = {}
            total_score = 0
            
            for strategy_name, performance in strategy_performance.items():
                # Calculate composite score
                profit_score = performance['profit_factor']
                win_rate_score = performance['win_rate'] * 2  # Scale to match profit factor
                sharpe_score = max(0, performance['sharpe_ratio'])
                
                composite_score = (profit_score * 0.5 + win_rate_score * 0.3 + sharpe_score * 0.2)
                strategy_weights[strategy_name] = composite_score
                total_score += composite_score
                
            # Normalize weights
            if total_score > 0:
                for strategy_name in strategy_weights:
                    strategy_weights[strategy_name] /= total_score
                    
            # Apply weight evolution
            evolution_data = self.learning_algorithms['strategy_evolution']
            
            for strategy_name, new_weight in strategy_weights.items():
                # Store performance memory
                evolution_data['performance_memory'][strategy_name].append(new_weight)
                
                # Update fitness score
                evolution_data['fitness_scores'][strategy_name] = new_weight
                
            evolution_data['evolution_generations'] += 1
            
            self.logger.info(f"üß¨ Strategy weights evolved: generation {evolution_data['evolution_generations']}")
            
        except Exception as e:
            self.logger.error(f"Strategy weight evolution failed: {e}")
            
    def get_optimized_strategy_selection(self, market_conditions):
        """Get optimized strategy selection based on current conditions"""
        try:
            # Get current market regime
            current_regime = self.learning_algorithms['market_regime_detection']['current_regime']
            regime_confidence = self.learning_algorithms['market_regime_detection']['confidence']
            
            # Get strategy fitness scores
            fitness_scores = self.learning_algorithms['strategy_evolution']['fitness_scores']
            
            # Get regime-specific strategy performance
            regime_performance = self.learning_algorithms['market_regime_detection']['regime_strategy_performance']
            
            # Calculate strategy scores for current conditions
            strategy_scores = {}
            
            for strategy_name, fitness in fitness_scores.items():
                base_score = fitness
                
                # Adjust based on regime performance
                if current_regime in regime_performance and strategy_name in regime_performance[current_regime]:
                    regime_performance_data = regime_performance[current_regime][strategy_name]
                    if regime_performance_data:
                        regime_score = np.mean(regime_performance_data)
                        base_score = (base_score * 0.7) + (regime_score * 0.3)
                        
                # Adjust based on market conditions
                if 'volatility' in market_conditions:
                    volatility = market_conditions['volatility']
                    if strategy_name in ['breakout', 'momentum'] and volatility > 0.04:
                        base_score *= 1.2  # Boost volatility strategies in high vol
                    elif strategy_name in ['mean_reversion'] and volatility < 0.02:
                        base_score *= 1.2  # Boost mean reversion in low vol
                        
                strategy_scores[strategy_name] = base_score
                
            # Select top strategies
            sorted_strategies = sorted(strategy_scores.items(), key=lambda x: x[1], reverse=True)
            
            # Return top 5 strategies with their scores
            selected_strategies = {}
            total_score = sum(score for _, score in sorted_strategies[:5])
            
            for strategy_name, score in sorted_strategies[:5]:
                normalized_score = score / total_score if total_score > 0 else 0.2
                selected_strategies[strategy_name] = {
                    'weight': normalized_score,
                    'confidence': min(1.0, score),
                    'regime_fit': regime_confidence
                }
                
            return selected_strategies
            
        except Exception as e:
            self.logger.error(f"Optimized strategy selection failed: {e}")
            return {}
            
    def get_learning_analytics(self):
        """Get comprehensive learning analytics"""
        try:
            analytics = {
                'learning_state': {
                    'learning_active': self.learning_state['learning_active'],
                    'models_trained': self.learning_state['models_trained'],
                    'optimization_cycles': self.learning_state['optimization_cycles'],
                    'performance_trend': self.learning_state['performance_trend'],
                    'model_confidence': self.learning_state['model_confidence']
                },
                'pattern_recognition': {
                    'patterns_detected': len(self.learning_algorithms['pattern_recognition']['pattern_memory']),
                    'confidence_threshold': self.learning_algorithms['pattern_recognition']['confidence_threshold'],
                    'success_patterns': len(self.learning_algorithms['pattern_recognition']['success_patterns']),
                    'failure_patterns': len(self.learning_algorithms['pattern_recognition']['failure_patterns'])
                },
                'market_regime': {
                    'current_regime': self.learning_algorithms['market_regime_detection']['current_regime'],
                    'confidence': self.learning_algorithms['market_regime_detection']['confidence'],
                    'regime_history': len(self.learning_algorithms['market_regime_detection']['regime_memory'])
                },
                'strategy_evolution': {
                    'evolution_generations': self.learning_algorithms['strategy_evolution']['evolution_generations'],
                    'fitness_scores': dict(self.learning_algorithms['strategy_evolution']['fitness_scores']),
                    'best_strategy': max(self.learning_algorithms['strategy_evolution']['fitness_scores'], 
                                       key=self.learning_algorithms['strategy_evolution']['fitness_scores'].get) if self.learning_algorithms['strategy_evolution']['fitness_scores'] else None
                },
                'optimization': {
                    'optimization_history': len(self.learning_algorithms['adaptive_parameters']['optimization_history']),
                    'best_parameters': self.learning_algorithms['adaptive_parameters'].get('best_parameters', {}),
                    'current_parameters': self.learning_algorithms['adaptive_parameters'].get('current_parameters', {})
                },
                'ai_models': {
                    'prediction_model_trained': self.ai_models['prediction_model']['trained'] if self.ai_models['prediction_model'] else False,
                    'regime_classifier_accuracy': self.ai_models['regime_classifier']['accuracy'] if self.ai_models['regime_classifier'] else 0,
                    'strategy_selector_accuracy': self.ai_models['strategy_selector']['selection_accuracy'] if self.ai_models['strategy_selector'] else 0
                }
            }

	    return analytics
            
        except Exception as e:
            self.logger.error(f"Learning analytics generation failed: {e}")
            return {}
            
    def save_learning_state(self):
        """Save current learning state to memory"""
        try:
            state_data = {
                'learning_config': self.learning_config,
                'learning_algorithms': {
                    'pattern_recognition': {
                        'confidence_threshold': self.learning_algorithms['pattern_recognition']['confidence_threshold'],
                        'pattern_memory': list(self.learning_algorithms['pattern_recognition']['pattern_memory']),
                        'success_patterns': dict(self.learning_algorithms['pattern_recognition']['success_patterns']),
                        'failure_patterns': dict(self.learning_algorithms['pattern_recognition']['failure_patterns'])
                    },
                    'market_regime_detection': {
                        'current_regime': self.learning_algorithms['market_regime_detection']['current_regime'],
                        'confidence': self.learning_algorithms['market_regime_detection']['confidence'],
                        'regime_memory': list(self.learning_algorithms['market_regime_detection']['regime_memory']),
                        'regime_strategy_performance': dict(self.learning_algorithms['market_regime_detection']['regime_strategy_performance'])
                    },
                    'strategy_evolution': {
                        'evolution_generations': self.learning_algorithms['strategy_evolution']['evolution_generations'],
                        'fitness_scores': dict(self.learning_algorithms['strategy_evolution']['fitness_scores']),
                        'performance_memory': {k: list(v) for k, v in self.learning_algorithms['strategy_evolution']['performance_memory'].items()},
                        'mutation_history': self.learning_algorithms['strategy_evolution']['mutation_history']
                    },
                    'adaptive_parameters': {
                        'optimization_history': list(self.learning_algorithms['adaptive_parameters']['optimization_history']),
                        'current_parameters': self.learning_algorithms['adaptive_parameters']['current_parameters'],
                        'best_parameters': self.learning_algorithms['adaptive_parameters']['best_parameters']
                    }
                },
                'ai_models': {
                    'prediction_model': self.ai_models['prediction_model'],
                    'regime_classifier': self.ai_models['regime_classifier'],
                    'pattern_detector': self.ai_models['pattern_detector'],
                    'strategy_selector': self.ai_models['strategy_selector'],
                    'parameter_optimizer': self.ai_models['parameter_optimizer'],
                    'risk_predictor': self.ai_models['risk_predictor']
                },
                'learning_memory': {
                    'trade_outcomes': list(self.learning_memory['trade_outcomes']),
                    'market_states': list(self.learning_memory['market_states']),
                    'strategy_decisions': list(self.learning_memory['strategy_decisions']),
                    'parameter_settings': list(self.learning_memory['parameter_settings']),
                    'error_cases': list(self.learning_memory['error_cases']),
                    'success_cases': list(self.learning_memory['success_cases']),
                    'edge_cases': list(self.learning_memory['edge_cases'])
                },
                'learning_state': self.learning_state,
                'timestamp': time.time()
            }
            
            self.memory_manager.save_learning_state(state_data)
            self.logger.info("üíæ Learning state saved successfully")
            
        except Exception as e:
            self.logger.error(f"Learning state save failed: {e}")
            
    def load_learning_state(self):
        """Load previous learning state from memory"""
        try:
            state_data = self.memory_manager.load_learning_state()
            
            if state_data:
                # Restore learning algorithms
                algo_data = state_data.get('learning_algorithms', {})
                
                if 'pattern_recognition' in algo_data:
                    pr_data = algo_data['pattern_recognition']
                    self.learning_algorithms['pattern_recognition']['confidence_threshold'] = pr_data.get('confidence_threshold', 0.70)
                    self.learning_algorithms['pattern_recognition']['pattern_memory'] = deque(pr_data.get('pattern_memory', []), maxlen=10000)
                    self.learning_algorithms['pattern_recognition']['success_patterns'].update(pr_data.get('success_patterns', {}))
                    self.learning_algorithms['pattern_recognition']['failure_patterns'].update(pr_data.get('failure_patterns', {}))
                    
                if 'market_regime_detection' in algo_data:
                    mrd_data = algo_data['market_regime_detection']
                    self.learning_algorithms['market_regime_detection']['current_regime'] = mrd_data.get('current_regime', 'ranging')
                    self.learning_algorithms['market_regime_detection']['confidence'] = mrd_data.get('confidence', 0.5)
                    self.learning_algorithms['market_regime_detection']['regime_memory'] = deque(mrd_data.get('regime_memory', []), maxlen=1000)
                    self.learning_algorithms['market_regime_detection']['regime_strategy_performance'].update(mrd_data.get('regime_strategy_performance', {}))
                    
                if 'strategy_evolution' in algo_data:
                    se_data = algo_data['strategy_evolution']
                    self.learning_algorithms['strategy_evolution']['evolution_generations'] = se_data.get('evolution_generations', 0)
                    self.learning_algorithms['strategy_evolution']['fitness_scores'].update(se_data.get('fitness_scores', {}))
                    self.learning_algorithms['strategy_evolution']['mutation_history'] = se_data.get('mutation_history', [])
                    
                    perf_memory = se_data.get('performance_memory', {})
                    for strategy, history in perf_memory.items():
                        self.learning_algorithms['strategy_evolution']['performance_memory'][strategy] = deque(history, maxlen=500)
                        
                if 'adaptive_parameters' in algo_data:
                    ap_data = algo_data['adaptive_parameters']
                    self.learning_algorithms['adaptive_parameters']['optimization_history'] = deque(ap_data.get('optimization_history', []), maxlen=1000)
                    self.learning_algorithms['adaptive_parameters']['current_parameters'] = ap_data.get('current_parameters', {})
                    self.learning_algorithms['adaptive_parameters']['best_parameters'] = ap_data.get('best_parameters', {})
                    
                # Restore AI models
                model_data = state_data.get('ai_models', {})
                for model_name, model_config in model_data.items():
                    if model_name in self.ai_models:
                        self.ai_models[model_name] = model_config
                        
                # Restore learning memory
                memory_data = state_data.get('learning_memory', {})
                for memory_type, memory_list in memory_data.items():
                    if memory_type in self.learning_memory:
                        maxlen = getattr(self.learning_memory[memory_type], 'maxlen', None)
                        self.learning_memory[memory_type] = deque(memory_list, maxlen=maxlen)
                        
                # Restore learning state
                self.learning_state.update(state_data.get('learning_state', {}))
                
                self.logger.info("üìö Learning state loaded successfully")
                return True
            else:
                self.logger.info("üì≠ No previous learning state found")
                return False
                
        except Exception as e:
            self.logger.error(f"Learning state load failed: {e}")
            return False
            
    def process_trade_feedback(self, trade_result):
        """Process trade result for learning feedback"""
        try:
            # Extract relevant data
            symbol = trade_result.get('symbol')
            strategy = trade_result.get('strategy')
            profit_percentage = trade_result.get('profit_percentage', 0)
            market_conditions = trade_result.get('market_conditions', {})
            parameters_used = trade_result.get('parameters', {})
            
            # Store in learning memory
            trade_data = {
                'symbol': symbol,
                'strategy': strategy,
                'outcome': {
                    'profit_percentage': profit_percentage,
                    'success': profit_percentage > 0
                },
                'market_conditions': market_conditions,
                'parameters': parameters_used,
                'timestamp': time.time()
            }
            
            self.learning_memory['trade_outcomes'].append(trade_data)
            
            # Update strategy performance memory
            current_regime = self.learning_algorithms['market_regime_detection']['current_regime']
            regime_perf = self.learning_algorithms['market_regime_detection']['regime_strategy_performance']
            
            if current_regime not in regime_perf:
                regime_perf[current_regime] = defaultdict(list)
                
            regime_perf[current_regime][strategy].append(profit_percentage)
            
            # Update pattern recognition
            if profit_percentage > 0:
                # Success case
                self.learning_memory['success_cases'].append(trade_data)
                
                # Update success patterns
                success_patterns = self.learning_algorithms['pattern_recognition']['success_patterns']
                pattern_key = f"{strategy}_{current_regime}"
                success_patterns[pattern_key].append(trade_data)
                
            else:
                # Failure case
                self.learning_memory['error_cases'].append(trade_data)
                
                # Update failure patterns
                failure_patterns = self.learning_algorithms['pattern_recognition']['failure_patterns']
                pattern_key = f"{strategy}_{current_regime}"
                failure_patterns[pattern_key].append(trade_data)
                
            # Update strategy evolution fitness
            evolution_data = self.learning_algorithms['strategy_evolution']
            current_fitness = evolution_data['fitness_scores'].get(strategy, 0.5)
            
            # Adjust fitness based on trade result
            fitness_adjustment = profit_percentage * 0.1  # Scale adjustment
            new_fitness = current_fitness + fitness_adjustment
            new_fitness = max(0.1, min(1.0, new_fitness))  # Keep within bounds
            
            evolution_data['fitness_scores'][strategy] = new_fitness
            evolution_data['performance_memory'][strategy].append(new_fitness)
            
            # Check for edge cases
            if abs(profit_percentage) > 0.10:  # >10% profit or loss
                self.learning_memory['edge_cases'].append(trade_data)
                
            self.logger.info(f"üß† Trade feedback processed: {strategy} -> {profit_percentage:.2%}")
            
        except Exception as e:
            self.logger.error(f"Trade feedback processing failed: {e}")
            
    def get_market_prediction(self, symbol, timeframe='5m'):
        """Get AI market prediction for symbol"""
        try:
            prediction_model = self.ai_models.get('prediction_model')
            
            if not prediction_model or not prediction_model.get('trained'):
                return {
                    'direction': 'neutral',
                    'confidence': 0.5,
                    'predicted_move': 0.0,
                    'model_status': 'not_trained'
                }
                
            # Get recent market data for prediction
            market_features = self.extract_market_features(symbol, timeframe)
            
            if not market_features:
                return {
                    'direction': 'neutral',
                    'confidence': 0.5,
                    'predicted_move': 0.0,
                    'model_status': 'no_data'
                }
                
            # Simulate prediction (in real implementation, would use actual ML model)
            predicted_move = np.random.uniform(-0.05, 0.05)  # -5% to +5%
            confidence = np.random.uniform(0.4, 0.9)
            
            direction = 'bullish' if predicted_move > 0.01 else 'bearish' if predicted_move < -0.01 else 'neutral'
            
            prediction = {
                'direction': direction,
                'confidence': confidence,
                'predicted_move': predicted_move,
                'prediction_horizon': '1h',
                'model_status': 'active',
                'features_used': len(market_features),
                'timestamp': time.time()
            }
            
            # Store prediction for validation
            self.learning_memory['market_states'].append({
                'symbol': symbol,
                'prediction': prediction,
                'actual_features': market_features,
                'timestamp': time.time()
            })
            
            return prediction
            
        except Exception as e:
            self.logger.error(f"Market prediction failed for {symbol}: {e}")
            return {
                'direction': 'neutral',
                'confidence': 0.5,
                'predicted_move': 0.0,
                'model_status': 'error'
            }
            
    def extract_market_features(self, symbol, timeframe):
        """Extract market features for prediction"""
        try:
            # This would typically extract real technical indicators
            # For now, simulate feature extraction
            features = {
                'price_features': np.random.uniform(-1, 1, 20).tolist(),
                'volume_features': np.random.uniform(-1, 1, 15).tolist(),
                'technical_indicators': np.random.uniform(-1, 1, 25).tolist(),
                'market_regime_features': np.random.uniform(-1, 1, 10).tolist(),
                'sentiment_features': np.random.uniform(-1, 1, 8).tolist(),
                'correlation_features': np.random.uniform(-1, 1, 12).tolist()
            }
            
            # Flatten all features
            all_features = []
            for feature_type, feature_list in features.items():
                all_features.extend(feature_list)
                
            return all_features[:150]  # Limit to model input size
            
        except Exception as e:
            self.logger.error(f"Feature extraction failed for {symbol}: {e}")
            return []
            
    def get_strategy_recommendations(self, symbol, market_conditions):
        """Get AI-powered strategy recommendations"""
        try:
            # Get current market regime
            current_regime = self.learning_algorithms['market_regime_detection']['current_regime']
            regime_confidence = self.learning_algorithms['market_regime_detection']['confidence']
            
            # Get optimized strategy selection
            selected_strategies = self.get_optimized_strategy_selection(market_conditions)
            
            # Get market prediction
            market_prediction = self.get_market_prediction(symbol)
            
            # Combine all factors for recommendations
            recommendations = []
            
            for strategy_name, strategy_data in selected_strategies.items():
                # Calculate recommendation score
                base_score = strategy_data['weight']
                regime_bonus = strategy_data['regime_fit'] * 0.2
                prediction_bonus = 0
                
                # Adjust based on market prediction
                if market_prediction['direction'] == 'bullish' and strategy_name in ['breakout', 'momentum']:
                    prediction_bonus = market_prediction['confidence'] * 0.3
                elif market_prediction['direction'] == 'bearish' and strategy_name in ['short_breakout', 'short_momentum']:
                    prediction_bonus = market_prediction['confidence'] * 0.3
                elif market_prediction['direction'] == 'neutral' and strategy_name in ['mean_reversion', 'range_trading']:
                    prediction_bonus = market_prediction['confidence'] * 0.2
                    
                total_score = base_score + regime_bonus + prediction_bonus
                
                recommendation = {
                    'strategy': strategy_name,
                    'score': min(1.0, total_score),
                    'confidence': strategy_data['confidence'],
                    'regime_fit': strategy_data['regime_fit'],
                    'market_prediction_alignment': prediction_bonus > 0,
                    'recommended_parameters': self.get_recommended_parameters(strategy_name, market_conditions),
                    'risk_level': self.assess_strategy_risk(strategy_name, market_conditions)
                }
                
                recommendations.append(recommendation)
                
            # Sort by score
            recommendations.sort(key=lambda x: x['score'], reverse=True)
            
            # Return top 5 recommendations
            return {
                'recommendations': recommendations[:5],
                'market_regime': current_regime,
                'regime_confidence': regime_confidence,
                'market_prediction': market_prediction,
                'timestamp': time.time()
            }
            
        except Exception as e:
            self.logger.error(f"Strategy recommendations failed for {symbol}: {e}")
            return {'recommendations': [], 'market_regime': 'unknown', 'regime_confidence': 0}
            
    def get_recommended_parameters(self, strategy_name, market_conditions):
        """Get recommended parameters for strategy"""
        try:
            # Get optimized parameters
            best_params = self.learning_algorithms['adaptive_parameters'].get('best_parameters', {})
            current_params = self.learning_algorithms['adaptive_parameters'].get('current_parameters', {})
            
            # Use best parameters if available
            base_params = best_params or current_params
            
            # Adjust parameters based on market conditions
            recommended_params = base_params.copy() if base_params else {}
            
            # Default parameters if none available
            if not recommended_params:
                recommended_params = {
                    'risk_percentage': 0.02,
                    'leverage': 10,
                    'stop_loss': 0.02,
                    'take_profit': 0.05,
                    'timeframe_weight': 1.0
                }
                
            # Adjust for market volatility
            volatility = market_conditions.get('volatility', 0.02)
            
            if volatility > 0.04:  # High volatility
                recommended_params['risk_percentage'] *= 0.8
                recommended_params['stop_loss'] *= 1.2
                recommended_params['leverage'] *= 0.8
            elif volatility < 0.015:  # Low volatility
                recommended_params['risk_percentage'] *= 1.2
                recommended_params['leverage'] *= 1.2
                
            return recommended_params
            
        except Exception as e:
            self.logger.error(f"Parameter recommendation failed for {strategy_name}: {e}")
            return {}
            
    def assess_strategy_risk(self, strategy_name, market_conditions):
        """Assess risk level for strategy in current conditions"""
        try:
            # Base risk levels by strategy type
            base_risk_levels = {
                'breakout': 'medium',
                'momentum': 'medium',
                'mean_reversion': 'low',
                'liquidation_cascade': 'high',
                'whale_tracking': 'medium',
                'arbitrage': 'low',
                'funding_rate': 'low'
            }
            
            base_risk = base_risk_levels.get(strategy_name, 'medium')
            
            # Adjust based on market conditions
            volatility = market_conditions.get('volatility', 0.02)
            volume = market_conditions.get('volume', 1.0)
            
            risk_adjustments = 0
            
            if volatility > 0.05:  # Very high volatility
                risk_adjustments += 1
            elif volatility < 0.015:  # Very low volatility
                risk_adjustments -= 1
                
            if volume < 0.5:  # Low volume
                risk_adjustments += 1
                
            # Calculate final risk level
            risk_levels = ['very_low', 'low', 'medium', 'high', 'very_high']
            current_index = risk_levels.index(base_risk)
            final_index = max(0, min(len(risk_levels) - 1, current_index + risk_adjustments))
            
            return risk_levels[final_index]
            
        except Exception as e:
            self.logger.error(f"Risk assessment failed for {strategy_name}: {e}")
            return 'medium'
            
    def validate_predictions(self):
        """Validate previous predictions against actual outcomes"""
        try:
            # Get recent market state predictions
            recent_predictions = [
                state for state in self.learning_memory['market_states']
                if time.time() - state.get('timestamp', 0) < 3600  # Last hour
            ]
            
            if len(recent_predictions) < 5:
                return
                
            # Validate predictions (simplified)
            correct_predictions = 0
            total_predictions = len(recent_predictions)
            
            for prediction_data in recent_predictions:
                prediction = prediction_data.get('prediction', {})
                predicted_direction = prediction.get('direction', 'neutral')
                confidence = prediction.get('confidence', 0.5)
                
                # Simulate actual outcome (in real implementation, would use actual price data)
                actual_move = np.random.uniform(-0.05, 0.05)
                actual_direction = 'bullish' if actual_move > 0.01 else 'bearish' if actual_move < -0.01 else 'neutral'
                
                if predicted_direction == actual_direction and confidence > 0.6:
                    correct_predictions += 1
                    
            # Update model confidence
            prediction_accuracy = correct_predictions / total_predictions
            self.learning_state['model_confidence'] = prediction_accuracy
            
            # Update prediction model accuracy
            if self.ai_models['prediction_model']:
                self.ai_models['prediction_model']['validation_score'] = prediction_accuracy
                
            self.logger.info(f"üéØ Prediction validation: {prediction_accuracy:.1%} accuracy ({correct_predictions}/{total_predictions})")
            
        except Exception as e:
            self.logger.error(f"Prediction validation failed: {e}")
            
    def emergency_learning_reset(self):
        """Emergency reset of learning system if performance degrades severely"""
        try:
            # Check if emergency reset is needed
            recent_performance = self.get_recent_performance_score()
            
            if recent_performance < 0.3:  # Very poor performance
                self.logger.warning("üö® Emergency learning reset triggered due to poor performance")
                
                # Reset learning parameters to defaults
                self.learning_state['exploration_rate_current'] = 0.15
                self.learning_state['learning_rate_adaptive'] = 0.001
                self.learning_state['model_confidence'] = 0.5
                
                # Clear recent poor performance memory
                recent_cutoff = time.time() - 7200  # Last 2 hours
                
                # Filter out recent poor trades
                filtered_outcomes = deque(maxlen=5000)
                for outcome in self.learning_memory['trade_outcomes']:
                    if (outcome.get('timestamp', 0) < recent_cutoff or 
                        outcome.get('outcome', {}).get('profit_percentage', 0) > 0):
                        filtered_outcomes.append(outcome)
                        
                self.learning_memory['trade_outcomes'] = filtered_outcomes
                
                # Reset strategy fitness scores to neutral
                for strategy in self.learning_algorithms['strategy_evolution']['fitness_scores']:
                    self.learning_algorithms['strategy_evolution']['fitness_scores'][strategy] = 0.5
                    
                # Increase exploration temporarily
                self.learning_state['exploration_rate_current'] = 0.25
                
                self.logger.info("üîÑ Learning system reset completed")
                
        except Exception as e:
            self.logger.error(f"Emergency learning reset failed: {e}")
            
    def get_recent_performance_score(self):
        """Get recent performance score for emergency reset check"""
        try:
            recent_cutoff = time.time() - 3600  # Last hour
            recent_trades = [
                trade for trade in self.learning_memory['trade_outcomes']
                if trade.get('timestamp', 0) > recent_cutoff
            ]
            
            if len(recent_trades) < 5:
                return 0.5  # Neutral if not enough data
                
            profits = [trade.get('outcome', {}).get('profit_percentage', 0) for trade in recent_trades]
            avg_profit = np.mean(profits)
            win_rate = sum(1 for p in profits if p > 0) / len(profits)
            
            # Combine average profit and win rate
            performance_score = (avg_profit * 10 + win_rate) / 2  # Scale avg_profit
            return max(0, min(1, performance_score + 0.5))  # Normalize to 0-1
            
        except Exception as e:
            self.logger.error(f"Recent performance score calculation failed: {e}")
            return 0.5

class MonsterTradingBot:
    """
    THE ULTIMATE CRYPTO TRADING MONSTER
    
    This is the main orchestrator that controls all 24 subsystems and coordinates
    the entire trading operation. It's the brain that makes this bot a true
    profit-hunting monster that evolves, learns, and dominates the markets.
    
    DAILY ROI TARGETS:
    * $100-$1k: 30% daily (PSYCHO MODE)
    * $1k-$10k: 25% daily (HUNTER MODE) 
    * $10k-$100k: 20% daily (ACCUMULATOR MODE)
    * $100k+: 15% daily (WHALE MODE)
    """
    
    def __init__(self, config_file='monster_config.json'):
        # Initialize logging first
        self.setup_logging()
        self.logger = logging.getLogger('MonsterBot.Main')
        
        self.logger.info("üöÄ INITIALIZING THE TRADING MONSTER...")
        self.logger.info("   üíÄ Preparing to hunt profits 24/7")
        self.logger.info("   üéØ Target: MAXIMUM AGGRESSIVE GROWTH")
        self.logger.info("   ‚ö° Mode: NO LIMITS, NO EXCUSES")
        
        # Load configuration
        self.config = self.load_configuration(config_file)
        
        # Monster state
        self.monster_state = {
            'active': False,
            'mode': 'initializing',
            'total_profit': 0,
            'daily_profit': 0,
            'trades_today': 0,
            'winning_streak': 0,
            'capital': 0,
            'target_hit': False,
            'emergency_stop': False,
            'last_heartbeat': 0,
            'startup_time': time.time()
        }
        
        # Performance tracking
        self.performance = {
            'total_trades': 0,
            'winning_trades': 0,
            'total_profit_usd': 0,
            'best_day': 0,
            'worst_day': 0,
            'max_drawdown': 0,
            'roi_achieved': [],
            'milestones_hit': [],
            'current_streak': 0
        }
        
        # Initialize all subsystems
        self.initialize_subsystems()
        
        # Trading coordination
        self.coordination = {
            'active_strategies': {},
            'position_queue': deque(maxlen=100),
            'signal_queue': deque(maxlen=1000),
            'execution_queue': deque(maxlen=500),
            'market_data_cache': {},
            'strategy_weights': {},
            'risk_override': False
        }
        
        # Emergency systems
        self.emergency = {
            'circuit_breakers': {
                'max_daily_loss': 0.50,      # 50% max daily loss
                'max_drawdown': 0.30,        # 30% max drawdown
                'max_concurrent_positions': 15, # Max positions
                'network_timeout': 30,       # 30 sec timeout
                'api_error_limit': 10        # Max API errors
            },
            'protection_active': False,
            'last_emergency': 0,
            'emergency_count': 0
        }
        
    def setup_logging(self):
        """Setup comprehensive logging system"""
        try:
            logging.basicConfig(
                level=logging.INFO,
                format='%(asctime)s | %(name)-20s | %(levelname)-8s | %(message)s',
                handlers=[
                    logging.FileHandler('monster_bot.log'),
                    logging.StreamHandler()
                ]
            )
            
            # Create monster-specific logger
            monster_logger = logging.getLogger('MonsterBot')
            monster_logger.setLevel(logging.INFO)
            
        except Exception as e:
            print(f"Logging setup failed: {e}")
            
    def load_configuration(self, config_file):
        """Load bot configuration"""
        try:
            # Default monster configuration
            default_config = {
                'api': {
                    'binance_key': '',
                    'binance_secret': '',
                    'testnet': True
                },
                'trading': {
                    'enabled_symbols': ['BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'ADAUSDT', 'SOLUSDT'],
                    'max_positions': 10,
                    'base_currency': 'USDT',
                    'min_trade_amount': 10
                },
                'risk': {
                    'max_risk_per_trade': 0.05,
                    'max_portfolio_risk': 0.20,
                    'emergency_stop_loss': 0.50
                },
                'monster_mode': {
                    'psycho_mode_enabled': True,
                    'aggressive_scaling': True,
                    'yolo_trades_allowed': True,
                    'max_leverage': 125
                }
            }
            
            try:
                with open(config_file, 'r') as f:
                    user_config = json.load(f)
                default_config.update(user_config)
            except FileNotFoundError:
                self.logger.warning(f"Config file {config_file} not found, using defaults")
                
            return default_config
            
        except Exception as e:
            self.logger.error(f"Configuration loading failed: {e}")
            return {}
            
    def initialize_subsystems(self):
        """Initialize all 24 subsystems of the trading monster"""
        try:
            self.logger.info("üîß Initializing Monster Subsystems...")
            
            # PART 1: Core Configuration & Setup
            from config_manager import ConfigManager
            from memory_manager import MemoryManager
            self.config_manager = ConfigManager(self.config)
            self.memory_manager = MemoryManager(self.config)
            
            # PART 2: Binance Client & Market Data
            from binance_client import AdvancedBinanceClient
            self.binance_client = AdvancedBinanceClient(self.config, self.memory_manager)
            
            # PART 3: Neural Network Prediction Engine
            from neural_network import NeuralNetworkEngine
            self.neural_network = NeuralNetworkEngine(self.config, self.binance_client, self.memory_manager)
            
            # PART 4: Multi-Strategy System
            from multi_strategy import MultiStrategySystem
            self.strategy_system = MultiStrategySystem(self.config, self.binance_client, self.memory_manager)
            
            # PART 5: Sentiment Analysis Engine
            from sentiment_analysis import SentimentAnalysisEngine
            self.sentiment_engine = SentimentAnalysisEngine(self.config, self.memory_manager)
            
            # PART 6: Whale Tracking System
            from whale_tracking import WhaleTrackingSystem
            self.whale_tracker = WhaleTrackingSystem(self.config, self.binance_client, self.memory_manager)
            
            # PART 7: News Integration & Reaction
            from news_integration import NewsIntegrationEngine
            self.news_engine = NewsIntegrationEngine(self.config, self.memory_manager)
            
            # PART 8: Order Book Analysis
            from order_book import OrderBookAnalysisEngine
            self.order_book_analyzer = OrderBookAnalysisEngine(self.config, self.binance_client, self.memory_manager)
            
            # PART 9: Position Management System
            from position_management import PositionManagementSystem
            self.position_manager = PositionManagementSystem(self.config, self.binance_client, self.memory_manager)
            
            # PART 10: Risk Management Engine
            from risk_management import RiskManagementEngine
            self.risk_manager = RiskManagementEngine(self.config, self.position_manager, self.memory_manager)
            
            # PART 11: Dynamic Exit Strategies
            from exit_strategies import DynamicExitEngine
            self.exit_engine = DynamicExitEngine(self.config, self.binance_client, self.position_manager, self.memory_manager)
            
            # PART 12: Correlation Matrix
            from correlation_matrix import CorrelationMatrixEngine
            self.correlation_engine = CorrelationMatrixEngine(self.config, self.binance_client, self.memory_manager)
            
            # PART 13: Arbitrage Scanner
            from arbitrage_scanner import ArbitrageScannerEngine
            self.arbitrage_scanner = ArbitrageScannerEngine(self.config, self.binance_client, self.memory_manager)
            
            # PART 14: Performance Analytics
            from performance_analytics import PerformanceAnalyticsEngine
            self.performance_analytics = PerformanceAnalyticsEngine(self.config, self.memory_manager)
            
            # PART 15: Dashboard & UI
            from dashboard import DashboardEngine
            self.dashboard = DashboardEngine(self.config, self.memory_manager)
            
            # PART 16: Notification Systems
            from notifications import NotificationEngine
            self.notifications = NotificationEngine(self.config)
            
            # PART 17: Backtesting Engine
            from backtesting import BacktestingEngine
            self.backtesting = BacktestingEngine(self.config, self.memory_manager)
            
            # PART 18: On-chain Analysis
            from onchain_analysis import OnChainAnalysisEngine
            self.onchain_analyzer = OnChainAnalysisEngine(self.config, self.memory_manager)
            
            # PART 19: Custom Indicators
            from custom_indicators import CustomIndicatorEngine
            self.indicator_engine = CustomIndicatorEngine(self.config, self.binance_client, self.memory_manager)
            
            # PART 20: Market Maker Detection
            from market_maker_detection import MarketMakerDetectionEngine
            self.market_maker_detector = MarketMakerDetectionEngine(self.config, self.binance_client, self.memory_manager, self.order_book_analyzer)
            
            # PART 21: Funding Rate Arbitrage
            from funding_arbitrage import FundingRateArbitrageEngine
            self.funding_arbitrage = FundingRateArbitrageEngine(self.config, self.binance_client, self.memory_manager, self.performance_analytics)
            
            # PART 22: Liquidation Heat Maps
            from liquidation_heatmaps import LiquidationHeatMapsEngine
            self.liquidation_engine = LiquidationHeatMapsEngine(self.config, self.binance_client, self.memory_manager, self.performance_analytics)
            
            # PART 23: Auto-Compounding System
            from auto_compounding import AutoCompoundingEngine
            self.compounding_engine = AutoCompoundingEngine(self.config, self.binance_client, self.memory_manager, self.performance_analytics, self.risk_manager)
            
            # PART 24: Learning & Optimization
            from learning_optimization import LearningOptimizationEngine
            all_strategies = self.strategy_system.get_all_strategies()
            self.learning_engine = LearningOptimizationEngine(self.config, self.binance_client, self.memory_manager, self.performance_analytics, all_strategies)
            
            # Start all subsystems
            self.start_all_subsystems()
            
            self.logger.info("‚úÖ ALL 24 MONSTER SUBSYSTEMS INITIALIZED!")
            self.logger.info("   üß† Neural networks: ONLINE")
            self.logger.info("   üìä Market analysis: ACTIVE") 
            self.logger.info("   üéØ Strategy system: LOADED")
            self.logger.info("   ‚ö° Risk management: ARMED")
            self.logger.info("   üí∞ Profit hunting: READY")
            
        except Exception as e:
            self.logger.error(f"üí• SUBSYSTEM INITIALIZATION FAILED: {e}")
            raise
            
    def start_all_subsystems(self):
        """Start all subsystems in proper order"""
        try:
            self.logger.info("üöÄ Starting Monster subsystems...")
            
            # Start core systems first
            subsystem_order = [
                'binance_client', 'memory_manager', 'performance_analytics',
                'risk_manager', 'position_manager', 'correlation_engine',
                'sentiment_engine', 'news_engine', 'whale_tracker',
                'order_book_analyzer', 'indicator_engine', 'neural_network',
                'strategy_system', 'exit_engine', 'arbitrage_scanner',
                'funding_arbitrage', 'liquidation_engine', 'onchain_analyzer',
                'market_maker_detector', 'compounding_engine', 'learning_engine',
                'notifications', 'dashboard'
            ]
            
            for subsystem_name in subsystem_order:
                if hasattr(self, subsystem_name):
                    subsystem = getattr(self, subsystem_name)
                    if hasattr(subsystem, 'start'):
                        subsystem.start()
                        self.logger.info(f"   ‚úÖ {subsystem_name} started")
                        
            self.logger.info("üî• ALL SUBSYSTEMS ONLINE - MONSTER IS ALIVE!")
            
        except Exception as e:
            self.logger.error(f"Subsystem startup failed: {e}")
            
    def start_trading(self):
        """Start the trading monster"""
        try:
            self.logger.info("üî• STARTING THE TRADING MONSTER...")
            self.logger.info("   üíÄ HUNT MODE: ACTIVATED")
            self.logger.info("   üéØ TARGET: MAXIMUM PROFITS")
            self.logger.info("   ‚ö° MERCY: DISABLED")
            
            # Update monster state
            self.monster_state['active'] = True
            self.monster_state['mode'] = 'hunting'
            self.monster_state['last_heartbeat'] = time.time()
            
            # Get initial capital
            self.update_capital()
            
            # Determine trading mode
            self.determine_trading_mode()
            
            # Start main trading loop
            self.start_main_loop()
            
            # Start monitoring threads
            self.start_monitoring_threads()
            
            # Send startup notification
            self.notifications.send_startup_notification(self.monster_state)
            
            self.logger.info("üöÄ TRADING MONSTER IS NOW LIVE!")
            self.logger.info(f"   üí∞ Starting Capital: ${self.monster_state['capital']:,.2f}")
            self.logger.info(f"   üéÆ Trading Mode: {self.monster_state['mode'].upper()}")
            self.logger.info("   üî• LET THE PROFIT HUNT BEGIN!")
            
        except Exception as e:
            self.logger.error(f"üí• MONSTER STARTUP FAILED: {e}")
            self.emergency_shutdown()
            
    def update_capital(self):
        """Update current capital from account"""
        try:
            self.compounding_engine.update_current_capital()
            current_capital = self.compounding_engine.capital_growth['current_capital']
            self.monster_state['capital'] = current_capital
            
        except Exception as e:
            self.logger.error(f"Capital update failed: {e}")
            
    def determine_trading_mode(self):
        """Determine trading mode based on capital"""
        try:
            capital = self.monster_state['capital']
            
            if capital < 1000:
                mode = 'psycho'
                daily_target = 0.30  # 30%
                max_risk = 0.10      # 10%
                max_leverage = 100
                self.logger.info("üíÄ PSYCHO MODE ACTIVATED - NO MERCY!")
                
            elif capital < 10000:
                mode = 'hunter'
                daily_target = 0.25  # 25%
                max_risk = 0.08      # 8%
                max_leverage = 50
                self.logger.info("üéØ HUNTER MODE ACTIVATED - SELECTIVE KILLS!")
                
            elif capital < 100000:
                mode = 'accumulator'
                daily_target = 0.20  # 20%
                max_risk = 0.05      # 5%
                max_leverage = 25
                self.logger.info("üìà ACCUMULATOR MODE ACTIVATED - STEADY GROWTH!")
                
            else:
                mode = 'whale'
                daily_target = 0.15  # 15%
                max_risk = 0.03      # 3%
                max_leverage = 15
                self.logger.info("üêã WHALE MODE ACTIVATED - INSTITUTIONAL DOMINANCE!")
                
            # Update monster state
            self.monster_state['mode'] = mode
            self.monster_state['daily_target'] = daily_target
            self.monster_state['max_risk'] = max_risk
            self.monster_state['max_leverage'] = max_leverage
            
            # Update subsystems with new mode
            self.update_subsystem_modes(mode)
            
        except Exception as e:
            self.logger.error(f"Trading mode determination failed: {e}")
            
    def update_subsystem_modes(self, mode):
        """Update all subsystems with new trading mode"""
        try:
            # Update compounding engine
            self.compounding_engine.trading_state['current_mode'] = f"{mode}_mode"
            
            # Update risk manager
            self.risk_manager.update_risk_parameters(mode)
            
            # Update strategy system
            self.strategy_system.update_trading_mode(mode)
            
            # Update position manager
            self.position_manager.update_position_limits(mode)
            
            self.logger.info(f"üîß All subsystems updated for {mode} mode")
            
        except Exception as e:
            self.logger.error(f"Subsystem mode update failed: {e}")
            
    def start_main_loop(self):
        """Start the main trading loop"""
        def main_trading_loop():
            while self.monster_state['active']:
                try:
                    # Update heartbeat
                    self.monster_state['last_heartbeat'] = time.time()
                    
                    # Execute trading cycle
                    self.execute_trading_cycle()
                    
                    # Brief pause to prevent overwhelming
                    time.sleep(1)
                    
                except Exception as e:
                    self.logger.error(f"Main loop error: {e}")
                    time.sleep(5)
                    
        threading.Thread(target=main_trading_loop, daemon=True).start()
        
    def execute_trading_cycle(self):
        """Execute one complete trading cycle"""
        try:
            # 1. Market Analysis
            market_analysis = self.perform_market_analysis()
            
            # 2. Signal Generation
            signals = self.generate_trading_signals(market_analysis)
            
            # 3. Signal Processing
            processed_signals = self.process_signals(signals)
            
            # 4. Risk Assessment
            risk_approved_signals = self.assess_signal_risk(processed_signals)
            
            # 5. Position Execution
            executed_trades = self.execute_signals(risk_approved_signals)
            
            # 6. Position Management
            self.manage_existing_positions()
            
            # 7. Performance Update
            self.update_performance_metrics()
            
            # 8. Learning Update
            self.update_learning_systems(executed_trades)
            
            # 9. Compounding Update
            self.update_compounding()
            
            # 10. Emergency Checks
            self.check_emergency_conditions()
            
        except Exception as e:
            self.logger.error(f"Trading cycle execution failed: {e}")
            
    def perform_market_analysis(self):
        """Perform comprehensive market analysis"""
        try:
            analysis = {
                'timestamp': time.time(),
                'market_regime': {},
                'sentiment': {},
                'whale_activity': {},
                'liquidation_risk': {},
                'arbitrage_opportunities': {},
                'onchain_signals': {},
                'news_sentiment': {},
                'technical_indicators': {}
            }
            
            # Get market regime from learning engine
            regime_data = self.learning_engine.learning_algorithms['market_regime_detection']
            analysis['market_regime'] = {
                'current_regime': regime_data['current_regime'],
                'confidence': regime_data['confidence']
            }
            
            # Get sentiment analysis
            analysis['sentiment'] = self.sentiment_engine.get_current_sentiment()
            
            # Get whale activity
            analysis['whale_activity'] = self.whale_tracker.get_whale_activity_summary()
            
            # Get liquidation analysis
            analysis['liquidation_risk'] = self.liquidation_engine.get_liquidation_analytics()
            
            # Get arbitrage opportunities
            analysis['arbitrage_opportunities'] = self.arbitrage_scanner.get_arbitrage_opportunities()
            
            # Get on-chain signals
            analysis['onchain_signals'] = self.onchain_analyzer.get_onchain_summary()
            
            # Get news sentiment
            analysis['news_sentiment'] = self.news_engine.get_news_sentiment_summary()
            
            # Cache analysis
            self.coordination['market_data_cache'] = analysis
            
            return analysis
            
        except Exception as e:
            self.logger.error(f"Market analysis failed: {e}")
            return {}
            
    def generate_trading_signals(self, market_analysis):
        """Generate trading signals from all strategies"""
        try:
            all_signals = []
            
            # Get signals from strategy system
            strategy_signals = self.strategy_system.get_all_strategy_signals(market_analysis)
            all_signals.extend(strategy_signals)
            
            # Get neural network predictions
            nn_signals = self.neural_network.get_prediction_signals(market_analysis)
            all_signals.extend(nn_signals)
            
            # Get liquidation cascade signals
            liquidation_signals = self.liquidation_engine.get_liquidation_trade_signals()
            all_signals.extend(liquidation_signals)
            
            # Get arbitrage signals
            arbitrage_signals = self.arbitrage_scanner.get_arbitrage_signals()
            all_signals.extend(arbitrage_signals)
            
            # Get funding rate arbitrage signals
            funding_signals = self.funding_arbitrage.get_funding_signals()
            all_signals.extend(funding_signals)
            
            # Get whale tracking signals
            whale_signals = self.whale_tracker.get_whale_signals()
            all_signals.extend(whale_signals)
            
            # Filter and rank signals
            filtered_signals = self.filter_signals(all_signals)
            
            return filtered_signals
            
        except Exception as e:
            self.logger.error(f"Signal generation failed: {e}")
            return []
            
    def filter_signals(self, signals):
        """Filter and rank trading signals"""
        try:
            # Remove duplicate signals
            unique_signals = {}
            for signal in signals:
                key = f"{signal.get('symbol')}_{signal.get('strategy')}_{signal.get('type')}"
                if key not in unique_signals or signal.get('confidence', 0) > unique_signals[key].get('confidence', 0):
                    unique_signals[key] = signal
                    
            # Convert back to list
            filtered_signals = list(unique_signals.values())
            
            # Score signals
            for signal in filtered_signals:
                signal['composite_score'] = self.calculate_signal_score(signal)
                
            # Sort by composite score
            filtered_signals.sort(key=lambda x: x.get('composite_score', 0), reverse=True)
            
            # Return top signals based on mode
            max_signals = {
                'psycho': 15,
                'hunter': 12,
                'accumulator': 8,
                'whale': 5
            }
            
            mode = self.monster_state['mode']
            limit = max_signals.get(mode, 10)
            
            return filtered_signals[:limit]
            
        except Exception as e:
            self.logger.error(f"Signal filtering failed: {e}")
            return signals
            
    def calculate_signal_score(self, signal):
        """Calculate composite score for signal"""
        try:
            base_confidence = signal.get('confidence', 0.5)
            strategy_weight = self.get_strategy_weight(signal.get('strategy', ''))
            market_fit = self.assess_market_fit(signal)
            risk_reward = signal.get('risk_reward_ratio', 2.0)
            
            # Combine factors
            composite_score = (
                base_confidence * 0.40 +
                strategy_weight * 0.25 +
                market_fit * 0.20 +
                min(1.0, risk_reward / 5.0) * 0.15
            )
            
            return composite_score
            
        except Exception as e:
            self.logger.error(f"Signal scoring failed: {e}")
            return 0.5
            
    def get_strategy_weight(self, strategy_name):
        """Get current weight for strategy"""
        try:
            fitness_scores = self.learning_engine.learning_algorithms['strategy_evolution']['fitness_scores']
            return fitness_scores.get(strategy_name, 0.5)
        except:
            return 0.5
            
    def assess_market_fit(self, signal):
        """Assess how well signal fits current market conditions"""
        try:
            # Get current market regime
            regime = self.coordination['market_data_cache'].get('market_regime', {})
            current_regime = regime.get('current_regime', 'ranging')
            
            # Strategy-regime fit mapping
            regime_fit = {
                'trending_up': {
                    'breakout': 0.9, 'momentum': 0.8, 'whale_tracking': 0.7,
                    'mean_reversion': 0.3, 'liquidation_cascade': 0.6
                },
                'trending_down': {
                    'short_breakout': 0.9, 'short_momentum': 0.8, 'liquidation_cascade': 0.8,
                    'mean_reversion': 0.4, 'whale_tracking': 0.6
                },
                'ranging': {
                    'mean_reversion': 0.9, 'arbitrage': 0.8, 'funding_rate': 0.7,
                    'breakout': 0.3, 'momentum': 0.3
                },
                'high_volatility': {
                    'liquidation_cascade': 0.9, 'whale_tracking': 0.8, 'breakout': 0.7,
                    'arbitrage': 0.4, 'funding_rate': 0.3
                }
            }
            
            strategy = signal.get('strategy', '')
            return regime_fit.get(current_regime, {}).get(strategy, 0.5)
            
        except Exception as e:
            self.logger.error(f"Market fit assessment failed: {e}")
            return 0.5
            
    def process_signals(self, signals):
        """Process and enhance signals"""
        try:
            processed_signals = []
            
            for signal in signals:
                # Get AI-enhanced parameters
                enhanced_signal = self.enhance_signal_with_ai(signal)
                
                # Add compounding parameters
                compounding_params = self.compounding_engine.get_optimal_trade_parameters(
                    enhanced_signal['symbol'],
                    enhanced_signal['strategy'],
                    enhanced_signal['confidence']
                )
                
                enhanced_signal.update(compounding_params)
                processed_signals.append(enhanced_signal)
                
            return processed_signals
            
        except Exception as e:
            self.logger.error(f"Signal processing failed: {e}")
            return signals
            
    def enhance_signal_with_ai(self, signal):
        """Enhance signal with AI predictions and analysis"""
        try:
            symbol = signal['symbol']
            
            # Get AI market prediction
            market_prediction = self.learning_engine.get_market_prediction(symbol)
            
            # Get strategy recommendations
            market_conditions = self.get_current_market_conditions(symbol)
            ai_recommendations = self.learning_engine.get_strategy_recommendations(symbol, market_conditions)
            
            # Enhance signal with AI insights
            enhanced_signal = signal.copy()
            enhanced_signal['ai_prediction'] = market_prediction
            enhanced_signal['ai_recommendations'] = ai_recommendations
            enhanced_signal['market_conditions'] = market_conditions
            
            # Adjust confidence based on AI alignment
            ai_confidence_boost = 0
            if market_prediction['direction'] == signal.get('direction', 'neutral'):
                ai_confidence_boost = market_prediction['confidence'] * 0.2
                
            enhanced_signal['confidence'] = min(1.0, signal.get('confidence', 0.5) + ai_confidence_boost)
            
            return enhanced_signal
            
        except Exception as e:
            self.logger.error(f"AI signal enhancement failed: {e}")
            return signal
            
    def get_current_market_conditions(self, symbol):
        """Get current market conditions for symbol"""
        try:
            # Get volatility
            volatility = self.performance_analytics.get_symbol_volatility(symbol)
            
            # Get volume
            ticker = self.binance_client.get_ticker(symbol)
            volume = float(ticker['volume']) if ticker else 1000000
            
            # Get trend strength
            trend_strength = self.indicator_engine.get_trend_strength(symbol)
            
            return {
                'volatility': volatility or 0.02,
                'volume': volume,
                'trend_strength': trend_strength or 0.5
            }
            
        except Exception as e:
            self.logger.error(f"Market conditions retrieval failed for {symbol}: {e}")
            return {'volatility': 0.02, 'volume': 1000000, 'trend_strength': 0.5}
            
    def assess_signal_risk(self, signals):
        """Assess risk for each signal and approve/reject"""
        try:
            approved_signals = []
            
            for signal in signals:
                # Check with risk manager
                risk_assessment = self.risk_manager.assess_trade_risk(signal)
                
                if risk_assessment['approved']:
                    signal['risk_assessment'] = risk_assessment
                    approved_signals.append(signal)
                else:
                    self.logger.info(f"‚ùå Signal rejected: {signal['symbol']} - {risk_assessment['reason']}")
                    
            return approved_signals
            
        except Exception as e:
            self.logger.error(f"Risk assessment failed: {e}")
            return signals
            
    def execute_signals(self, signals):
        """Execute approved trading signals"""
        try:
            executed_trades = []
            
            for signal in signals:
                try:
                    # Execute trade
                    trade_result = self.position_manager.execute_trade(signal)
                    
                    if trade_result['success']:
                        executed_trades.append(trade_result)
                        self.monster_state['trades_today'] += 1
                        
                        self.logger.info(
                            f"‚úÖ TRADE EXECUTED: {signal['symbol']} "
                            f"{signal['type']} - {signal['strategy']} "
                            f"Size: ${signal['position_size_usdt']:,.0f} "
                            f"Leverage: {signal['leverage']}x "
                            f"Confidence: {signal['confidence']:.1%}"
                        )
                        
                        # Send trade notification
                        self.notifications.send_trade_notification(trade_result)
                        
                    else:
                        self.logger.warning(f"‚ùå Trade execution failed: {signal['symbol']} - {trade_result['reason']}")
                        
                except Exception as e:
                    self.logger.error(f"Trade execution error for {signal['symbol']}: {e}")
                    
            return executed_trades
            
        except Exception as e:
            self.logger.error(f"Signal execution failed: {e}")
            return []
            
    def manage_existing_positions(self):
        """Manage all existing positions"""
        try:
            # Get all open positions
            open_positions = self.position_manager.get_open_positions()
            
            for position in open_positions:
                # Update position with current market data
                self.position_manager.update_position(position)
                
                # Check exit conditions
                exit_signal = self.exit_engine.check_exit_conditions(position)
                
                if exit_signal:
                    # Execute exit
                    exit_result = self.position_manager.close_position(position, exit_signal)
                    
                    if exit_result['success']:
                        profit = exit_result['profit_usd']
                        self.monster_state['total_profit'] += profit
                        self.monster_state['daily_profit'] += profit
                        
                        if profit > 0:
                            self.monster_state['winning_streak'] += 1
                            self.performance['winning_trades'] += 1
                            
                            self.logger.info(
                                f"üí∞ PROFIT LOCKED: {position['symbol']} "
                                f"+${profit:,.2f} ({exit_result['profit_percentage']:+.2%}) "
                                f"Streak: {self.monster_state['winning_streak']}"
                            )
                        else:
                            self.monster_state['winning_streak'] = 0
                            
                            self.logger.info(
                                f"üîª LOSS CUT: {position['symbol']} "
                                f"${profit:,.2f} ({exit_result['profit_percentage']:+.2%})"
                            )
                            
                        # Process trade for learning
                        self.learning_engine.process_trade_feedback(exit_result)
                        
                        # Process for compounding
                        self.compounding_engine.process_trade_result(exit_result)
                        
                        # Update performance
                        self.performance['total_trades'] += 1
                        self.performance['total_profit_usd'] += profit
                        
                        # Send exit notification
                        self.notifications.send_exit_notification(exit_result)
                        
        except Exception as e:
            self.logger.error(f"Position management failed: {e}")
            
    def update_performance_metrics(self):
        """Update comprehensive performance metrics"""
        try:
            # Update capital
            self.update_capital()
            
            # Calculate daily performance
            daily_return = self.monster_state['daily_profit'] / max(1, self.monster_state['capital'] - self.monster_state['daily_profit'])
            
            # Check if daily target hit
            daily_target = self.monster_state.get('daily_target', 0.30)
            if daily_return >= daily_target and not self.monster_state.get('target_hit'):
                self.monster_state['target_hit'] = True
                
                self.logger.info(
                    f"üéØ DAILY TARGET SMASHED! "
                    f"{daily_return:.1%} / {daily_target:.1%} "
                    f"(${self.monster_state['daily_profit']:,.2f})"
                )
                
                # Send target hit notification
                self.notifications.send_target_hit_notification({
                    'daily_return': daily_return,
                    'daily_target': daily_target,
                    'profit_usd': self.monster_state['daily_profit'],
                    'trades_count': self.monster_state['trades_today']
                })
                
            # Update performance analytics
            self.performance_analytics.update_daily_performance({
                'date': datetime.now().strftime('%Y-%m-%d'),
                'capital': self.monster_state['capital'],
                'daily_return': daily_return,
                'daily_profit_usd': self.monster_state['daily_profit'],
                'trades_count': self.monster_state['trades_today'],
                'winning_streak': self.monster_state['winning_streak']
            })
            
            # Check for milestones
            self.check_milestones()
            
        except Exception as e:
            self.logger.error(f"Performance update failed: {e}")
            
    def check_milestones(self):
        """Check and celebrate milestones"""
        try:
            capital = self.monster_state['capital']
            milestones = [1000, 5000, 10000, 25000, 50000, 100000, 250000, 500000, 1000000, 10000000]
            
            for milestone in milestones:
                if capital >= milestone and milestone not in self.performance['milestones_hit']:
                    self.performance['milestones_hit'].append(milestone)
                    
                    # Calculate growth multiple
                    initial_capital = self.compounding_engine.capital_growth.get('initial_capital', 1000)
                    growth_multiple = capital / initial_capital if initial_capital > 0 else 1
                    
                    self.logger.info(
                        f"üéâ MILESTONE CRUSHED! ${milestone:,} ACHIEVED! "
                        f"({growth_multiple:.1f}x growth)"
                    )
                    
                    # Send milestone notification
                    self.notifications.send_milestone_notification({
                        'milestone': milestone,
                        'current_capital': capital,
                        'growth_multiple': growth_multiple,
                        'days_to_achieve': self.calculate_days_trading()
                    })
                    
        except Exception as e:
            self.logger.error(f"Milestone checking failed: {e}")
            
    def calculate_days_trading(self):
        """Calculate days since bot started"""
        try:
            start_time = self.monster_state.get('startup_time', time.time())
            return (time.time() - start_time) / 86400  # Convert to days
        except:
            return 1
            
    def update_learning_systems(self, executed_trades):
        """Update learning systems with new data"""
        try:
            # Update learning engine with trade data
            for trade in executed_trades:
                # Store trade decision for learning
                trade_decision = {
                    'symbol': trade['symbol'],
                    'strategy': trade['strategy'],
                    'market_conditions': trade.get('market_conditions', {}),
                    'parameters': trade.get('parameters', {}),
                    'ai_prediction': trade.get('ai_prediction', {}),
                    'timestamp': time.time()
                }
                
                self.learning_engine.learning_memory['strategy_decisions'].append(trade_decision)
                
            # Update strategy weights based on recent performance
            if len(executed_trades) > 0:
                self.update_strategy_weights()
                
        except Exception as e:
            self.logger.error(f"Learning systems update failed: {e}")
            
    def update_strategy_weights(self):
        """Update strategy weights based on performance"""
        try:
            # Get recent strategy performance
            strategy_performance = {}
            recent_trades = list(self.learning_engine.learning_memory['trade_outcomes'])[-50:]  # Last 50 trades
            
            for trade in recent_trades:
                strategy = trade.get('strategy')
                profit = trade.get('outcome', {}).get('profit_percentage', 0)
                
                if strategy:
                    if strategy not in strategy_performance:
                        strategy_performance[strategy] = []
                    strategy_performance[strategy].append(profit)
                    
            # Update coordination weights
            total_score = 0
            for strategy, profits in strategy_performance.items():
                if len(profits) >= 3:  # Need at least 3 trades
                    avg_profit = np.mean(profits)
                    win_rate = sum(1 for p in profits if p > 0) / len(profits)
                    score = avg_profit * win_rate
                    self.coordination['strategy_weights'][strategy] = max(0.1, score + 0.5)
                    total_score += self.coordination['strategy_weights'][strategy]
                    
            # Normalize weights
            if total_score > 0:
                for strategy in self.coordination['strategy_weights']:
                    self.coordination['strategy_weights'][strategy] /= total_score
                    
        except Exception as e:
            self.logger.error(f"Strategy weight update failed: {e}")
            
    def update_compounding(self):
        """Update compounding calculations"""
        try:
            # Update compounding engine
            compounding_analytics = self.compounding_engine.get_compounding_analytics()
            
            # Check if mode should change based on capital growth
            current_mode = self.monster_state['mode']
            capital = self.monster_state['capital']
            
            new_mode = None
            if current_mode == 'psycho' and capital >= 1000:
                new_mode = 'hunter'
            elif current_mode == 'hunter' and capital >= 10000:
                new_mode = 'accumulator'
            elif current_mode == 'accumulator' and capital >= 100000:
                new_mode = 'whale'
                
            if new_mode and new_mode != current_mode:
                self.logger.info(f"üéÆ MODE EVOLUTION: {current_mode.upper()} ‚Üí {new_mode.upper()}")
                self.determine_trading_mode()
                
                # Send mode change notification
                self.notifications.send_mode_change_notification({
                    'old_mode': current_mode,
                    'new_mode': new_mode,
                    'capital': capital
                })
                
        except Exception as e:
            self.logger.error(f"Compounding update failed: {e}")
            
    def check_emergency_conditions(self):
        """Check for emergency conditions"""
        try:
            # Check circuit breakers
            daily_loss_pct = abs(min(0, self.monster_state['daily_profit'])) / max(1, self.monster_state['capital'])
            
            if daily_loss_pct > self.emergency['circuit_breakers']['max_daily_loss']:
                self.trigger_emergency_stop('daily_loss_limit')
                return
                
            # Check max drawdown
            max_drawdown = self.performance_analytics.get_current_drawdown()
            if max_drawdown > self.emergency['circuit_breakers']['max_drawdown']:
                self.trigger_emergency_stop('max_drawdown')
                return
                
            # Check position count
            open_positions = len(self.position_manager.get_open_positions())
            if open_positions > self.emergency['circuit_breakers']['max_concurrent_positions']:
                self.emergency['protection_active'] = True
                self.logger.warning(f"‚ö†Ô∏è Position limit reached: {open_positions}")
                
            # Check API errors
            api_error_count = self.binance_client.get_error_count()
            if api_error_count > self.emergency['circuit_breakers']['api_error_limit']:
                self.trigger_emergency_stop('api_errors')
                return
                
            # Check heartbeat
            if time.time() - self.monster_state['last_heartbeat'] > 300:  # 5 minutes
                self.logger.warning("üíì Heartbeat missed - system may be struggling")
                
        except Exception as e:
            self.logger.error(f"Emergency condition check failed: {e}")
            
    def trigger_emergency_stop(self, reason):
        """Trigger emergency stop"""
        try:
            self.logger.error(f"üö® EMERGENCY STOP TRIGGERED: {reason}")
            
            self.monster_state['emergency_stop'] = True
            self.emergency['protection_active'] = True
            self.emergency['last_emergency'] = time.time()
            self.emergency['emergency_count'] += 1
            
            # Close all positions immediately
            self.emergency_close_all_positions()
            
            # Stop trading
            self.monster_state['active'] = False
            
            # Send emergency notification
            self.notifications.send_emergency_notification({
                'reason': reason,
                'capital': self.monster_state['capital'],
                'daily_profit': self.monster_state['daily_profit'],
                'open_positions': len(self.position_manager.get_open_positions())
            })
            
            self.logger.error("üõë TRADING STOPPED - EMERGENCY PROTECTION ACTIVE")
            
        except Exception as e:
            self.logger.error(f"Emergency stop failed: {e}")
            
    def emergency_close_all_positions(self):
        """Emergency close all open positions"""
        try:
            open_positions = self.position_manager.get_open_positions()
            
            for position in open_positions:
                try:
                    result = self.position_manager.emergency_close_position(position)
                    if result['success']:
                        self.logger.info(f"üö® Emergency closed: {position['symbol']}")
                    else:
                        self.logger.error(f"‚ùå Failed to emergency close: {position['symbol']}")
                except Exception as e:
                    self.logger.error(f"Emergency close failed for {position['symbol']}: {e}")
                    
        except Exception as e:
            self.logger.error(f"Emergency close all failed: {e}")
            
    def start_monitoring_threads(self):
        """Start monitoring and maintenance threads"""
        def heartbeat_monitor():
            while self.monster_state['active']:
                try:
                    self.send_heartbeat()
                    time.sleep(60)  # Every minute
                except Exception as e:
                    self.logger.error(f"Heartbeat error: {e}")
                    time.sleep(30)
                    
        def daily_reset():
            while self.monster_state['active']:
                try:
                    # Check if new day
                    current_day = datetime.now().strftime('%Y-%m-%d')
                    if not hasattr(self, '_current_day') or self._current_day != current_day:
                        self.perform_daily_reset()
                        self._current_day = current_day
                        
                    time.sleep(300)  # Check every 5 minutes
                except Exception as e:
                    self.logger.error(f"Daily reset error: {e}")
                    time.sleep(600)
                    
        def performance_monitor():
            while self.monster_state['active']:
                try:
                    self.monitor_performance()
                    time.sleep(600)  # Every 10 minutes
                except Exception as e:
                    self.logger.error(f"Performance monitor error: {e}")
                    time.sleep(300)
                    
        def system_maintenance():
            while self.monster_state['active']:
                try:
                    self.perform_system_maintenance()
                    time.sleep(3600)  # Every hour
                except Exception as e:
                    self.logger.error(f"System maintenance error: {e}")
                    time.sleep(1800)
                    
        # Start monitoring threads
        threading.Thread(target=heartbeat_monitor, daemon=True).start()
        threading.Thread(target=daily_reset, daemon=True).start()
        threading.Thread(target=performance_monitor, daemon=True).start()
        threading.Thread(target=system_maintenance, daemon=True).start()
        
    def send_heartbeat(self):
        """Send system heartbeat"""
        try:
            heartbeat_data = {
                'timestamp': time.time(),
                'status': 'active' if self.monster_state['active'] else 'inactive',
                'capital': self.monster_state['capital'],
                'daily_profit': self.monster_state['daily_profit'],
                'trades_today': self.monster_state['trades_today'],
                'open_positions': len(self.position_manager.get_open_positions()),
                'mode': self.monster_state['mode']
            }
            
            # Store heartbeat
            self.memory_manager.store_heartbeat(heartbeat_data)
            
            # Update dashboard
            if hasattr(self, 'dashboard'):
                self.dashboard.update_heartbeat(heartbeat_data)
                
        except Exception as e:
            self.logger.error(f"Heartbeat failed: {e}")
            
    def perform_daily_reset(self):
        """Perform daily reset operations"""
        try:
            # Log daily summary
            self.log_daily_summary()
            
            # Reset daily counters
            self.monster_state['daily_profit'] = 0
            self.monster_state['trades_today'] = 0
            self.monster_state['target_hit'] = False
            
            # Update daily targets
            self.compounding_engine.calculate_daily_targets()
            
            # Save state
            self.save_all_states()
            
            # Send daily summary
            self.notifications.send_daily_summary(self.get_daily_summary())
            
            self.logger.info("üåÖ DAILY RESET COMPLETED - NEW HUNT BEGINS!")
            
        except Exception as e:
            self.logger.error(f"Daily reset failed: {e}")
            
    def log_daily_summary(self):
        """Log comprehensive daily summary"""
        try:
            capital = self.monster_state['capital']
            daily_profit = self.monster_state['daily_profit']
            daily_return = daily_profit / max(1, capital - daily_profit)
            trades_count = self.monster_state['trades_today']
            
            self.logger.info("üìä DAILY SUMMARY:")
            self.logger.info(f"   üí∞ Capital: ${capital:,.2f}")
            self.logger.info(f"   üìà Daily P&L: ${daily_profit:,.2f} ({daily_return:+.1%})")
            self.logger.info(f"   üéØ Trades: {trades_count}")
            self.logger.info(f"   üî• Win Streak: {self.monster_state['winning_streak']}")
            self.logger.info(f"   üéÆ Mode: {self.monster_state['mode'].upper()}")
            
        except Exception as e:
            self.logger.error(f"Daily summary logging failed: {e}")
            
    def get_daily_summary(self):
        """Get daily summary data"""
        try:
            return {
                'date': datetime.now().strftime('%Y-%m-%d'),
                'capital': self.monster_state['capital'],
                'daily_profit': self.monster_state['daily_profit'],
                'daily_return': self.monster_state['daily_profit'] / max(1, self.monster_state['capital'] - self.monster_state['daily_profit']),
                'trades_count': self.monster_state['trades_today'],
                'winning_streak': self.monster_state['winning_streak'],
                'mode': self.monster_state['mode'],
                'target_hit': self.monster_state['target_hit']
            }
        except Exception as e:
            self.logger.error(f"Daily summary generation failed: {e}")
            return {}
            
    def monitor_performance(self):
        """Monitor system performance"""
        try:
            # Check system health
            health_metrics = {
                'memory_usage': self.get_memory_usage(),
                'cpu_usage': self.get_cpu_usage(),
                'api_latency': self.binance_client.get_average_latency(),
                'error_rate': self.get_error_rate(),
                'uptime': time.time() - self.monster_state['startup_time']
            }
            
            # Log if any issues
            if health_metrics['memory_usage'] > 80:
                self.logger.warning(f"‚ö†Ô∏è High memory usage: {health_metrics['memory_usage']:.1f}%")
                
            if health_metrics['api_latency'] > 1000:  # 1 second
                self.logger.warning(f"‚ö†Ô∏è High API latency: {health_metrics['api_latency']:.0f}ms")
                
            if health_metrics['error_rate'] > 0.05:  # 5%
                self.logger.warning(f"‚ö†Ô∏è High error rate: {health_metrics['error_rate']:.1%}")
                
            # Update dashboard
            if hasattr(self, 'dashboard'):
                self.dashboard.update_health_metrics(health_metrics)
                
        except Exception as e:
            self.logger.error(f"Performance monitoring failed: {e}")
            
    def get_memory_usage(self):
        """Get memory usage percentage"""
        try:
            import psutil
            return psutil.virtual_memory().percent
        except:
            return 0
            
    def get_cpu_usage(self):
        """Get CPU usage percentage"""
        try:
            import psutil
            return psutil.cpu_percent()
        except:
            return 0
            
    def get_error_rate(self):
        """Get current error rate"""
        try:
            # Calculate from recent operations
            return 0.01  # Placeholder
        except:
            return 0
            
    def perform_system_maintenance(self):
        """Perform regular system maintenance"""
        try:
            # Clean up old data
            self.cleanup_old_data()
            
            # Optimize memory usage
            self.optimize_memory()
            
            # Validate all subsystems
            self.validate_subsystems()
            
            # Save states
            self.save_all_states()
            
            self.logger.info("üîß System maintenance completed")
            
        except Exception as e:
            self.logger.error(f"System maintenance failed: {e}")
            
    def cleanup_old_data(self):
        """Clean up old data to free memory"""
        try:
            # Clean old signals
            if len(self.coordination['signal_queue']) > 500:
                for _ in range(200):
                    if self.coordination['signal_queue']:
                        self.coordination['signal_queue'].popleft()
                        
            # Clean old positions
            if len(self.coordination['position_queue']) > 50:
                for _ in range(20):
                    if self.coordination['position_queue']:
                        self.coordination['position_queue'].popleft()
                        
            # Clean old executions
            if len(self.coordination['execution_queue']) > 200:
                for _ in range(100):
                    if self.coordination['execution_queue']:
                        self.coordination['execution_queue'].popleft()
                        
        except Exception as e:
            self.logger.error(f"Data cleanup failed: {e}")
            
    def optimize_memory(self):
        """Optimize memory usage"""
        try:
            import gc
            gc.collect()
        except Exception as e:
            self.logger.error(f"Memory optimization failed: {e}")
            
    def validate_subsystems(self):
        """Validate all subsystems are healthy"""
        try:
            subsystem_health = {}
            
            # Check critical subsystems
            critical_systems = [
                'binance_client', 'position_manager', 'risk_manager',
                'compounding_engine', 'learning_engine'
            ]
            
            for system_name in critical_systems:
                if hasattr(self, system_name):
                    system = getattr(self, system_name)
                    health = self.check_subsystem_health(system)
                    subsystem_health[system_name] = health
                    
                    if not health:
                        self.logger.warning(f"‚ö†Ô∏è Subsystem unhealthy: {system_name}")
                        
            return subsystem_health
            
        except Exception as e:
            self.logger.error(f"Subsystem validation failed: {e}")
            return {}
            
    def check_subsystem_health(self, subsystem):
        """Check health of individual subsystem"""
        try:
            # Basic health checks
            if hasattr(subsystem, 'is_healthy'):
                return subsystem.is_healthy()
            elif hasattr(subsystem, 'get_health'):
                return subsystem.get_health()
            else:
                return True  # Assume healthy if no health check
        except:
            return False
            
    def save_all_states(self):
        """Save states of all subsystems"""
        try:
            # Save monster state
            self.memory_manager.save_monster_state(self.monster_state)
            
            # Save performance data
            self.memory_manager.save_performance_data(self.performance)
            
            # Save subsystem states
            if hasattr(self.compounding_engine, 'save_compounding_state'):
                self.compounding_engine.save_compounding_state()
                
            if hasattr(self.learning_engine, 'save_learning_state'):
                self.learning_engine.save_learning_state()
                
            if hasattr(self.liquidation_engine, 'save_liquidation_state'):
                self.liquidation_engine.save_liquidation_state()
                
            self.logger.info("üíæ All states saved successfully")
            
        except Exception as e:
            self.logger.error(f"State saving failed: {e}")
            
    def get_monster_status(self):
        """Get comprehensive monster status"""
        try:
            status = {
                'monster_state': self.monster_state.copy(),
                'performance': self.performance.copy(),
                'coordination': {
                    'active_strategies': len(self.coordination['active_strategies']),
                    'signals_pending': len(self.coordination['signal_queue']),
                    'positions_pending': len(self.coordination['position_queue'])
                },
                'emergency': self.emergency.copy(),
                'subsystem_health': self.validate_subsystems(),
                'uptime_hours': (time.time() - self.monster_state['startup_time']) / 3600
            }
            
            return status
            
        except Exception as e:
            self.logger.error(f"Status generation failed: {e}")
            return {}
            
    def stop_trading(self):
        """Stop the trading monster gracefully"""
        try:
            self.logger.info("üõë STOPPING TRADING MONSTER...")
            
            # Set inactive
            self.monster_state['active'] = False
            
            # Close all positions
            self.logger.info("üì§ Closing all open positions...")
            open_positions = self.position_manager.get_open_positions()
            
            for position in open_positions:
                try:
                    result = self.position_manager.close_position(position, {'reason': 'shutdown'})
                    if result['success']:
                        self.logger.info(f"‚úÖ Closed position: {position['symbol']}")
                except Exception as e:
                    self.logger.error(f"Failed to close position {position['symbol']}: {e}")
                    
            # Save final states
            self.save_all_states()
            
            # Send shutdown notification
            self.notifications.send_shutdown_notification(self.get_monster_status())
            
            self.logger.info("üî• TRADING MONSTER STOPPED")
            self.logger.info(f"   üí∞ Final Capital: ${self.monster_state['capital']:,.2f}")
            self.logger.info(f"   üìà Total Profit: ${self.monster_state['total_profit']:,.2f}")
            self.logger.info(f"   üéØ Total Trades: {self.performance['total_trades']}")
            self.logger.info("   üëë HUNT COMPLETE - PROFITS SECURED!")
            
        except Exception as e:
            self.logger.error(f"üí• SHUTDOWN FAILED: {e}")
            
    def emergency_shutdown(self):
        """Emergency shutdown procedure"""
        try:
            self.logger.error("üö® EMERGENCY SHUTDOWN INITIATED")
            
            # Immediate stop
            self.monster_state['active'] = False
            self.monster_state['emergency_stop'] = True
            
            # Emergency close all positions
            self.emergency_close_all_positions()
            
            # Send emergency notification
            self.notifications.send_emergency_shutdown_notification()
            
            self.logger.error("üõë EMERGENCY SHUTDOWN COMPLETE")
            
        except Exception as e:
            self.logger.error(f"üí• EMERGENCY SHUTDOWN FAILED: {e}")

# Example usage and main entry point
if __name__ == "__main__":
    try:
        # Create the monster
        monster_bot = MonsterTradingBot('config.json')
        
        # Start trading
        monster_bot.start_trading()
        
        # Keep running
        while monster_bot.monster_state['active']:
            time.sleep(10)
            
            # Check for user commands (could add keyboard controls)
            # For now, just keep running
            
    except KeyboardInterrupt:
        print("\nüõë Shutdown requested by user")
        if 'monster_bot' in locals():
            monster_bot.stop_trading()
            
    except Exception as e:
        print(f"üí• MONSTER BOT CRASHED: {e}")
        if 'monster_bot' in locals():
            monster_bot.emergency_shutdown()
